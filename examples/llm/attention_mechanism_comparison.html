<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>注意力机制详解</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script>
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: true
            }
        });
    </script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        }
        
        h1 {
            text-align: center;
            color: #764ba2;
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.1);
        }
        
        .subtitle {
            text-align: center;
            color: #666;
            margin-bottom: 40px;
            font-size: 1.1em;
        }
        
        .attention-type {
            margin-bottom: 60px;
            padding: 30px;
            background: white;
            border-radius: 15px;
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s, box-shadow 0.3s;
        }
        
        .attention-type:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.15);
        }
        
        h2 {
            color: #667eea;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .icon {
            width: 30px;
            height: 30px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            border-radius: 50%;
            display: inline-flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
        }
        
        h3 {
            color: #444;
            margin-top: 25px;
            margin-bottom: 15px;
            font-size: 1.3em;
        }
        
        .math-section {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 4px solid #667eea;
        }
        
        .visualization {
            margin: 30px 0;
            padding: 20px;
            background: #fff;
            border-radius: 10px;
            border: 2px solid #e0e0e0;
        }
        
        svg {
            width: 100%;
            height: auto;
            max-width: 1200px;
            display: block;
            margin: 0 auto;
        }
        
        .code-ref {
            background: #f4f4f4;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #764ba2;
            font-family: 'Courier New', monospace;
        }
        
        .code-ref h4 {
            color: #764ba2;
            margin-bottom: 10px;
        }
        
        .code-comment {
            color: #008000;
            font-style: italic;
        }
        
        .complexity-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
        
        .complexity-table th, .complexity-table td {
            padding: 12px;
            text-align: left;
            border: 1px solid #ddd;
        }
        
        .complexity-table th {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            font-weight: bold;
        }
        
        .complexity-table tr:nth-child(even) {
            background: #f8f9fa;
        }
        
        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .feature-card {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            transition: transform 0.3s;
        }
        
        .feature-card:hover {
            transform: scale(1.05);
        }
        
        .feature-card h4 {
            color: #667eea;
            margin-bottom: 10px;
        }
        
        .nav-menu {
            position: fixed;
            right: 20px;
            top: 50%;
            transform: translateY(-50%);
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.2);
            z-index: 1000;
        }
        
        .nav-menu ul {
            list-style: none;
        }
        
        .nav-menu li {
            margin: 10px 0;
        }
        
        .nav-menu a {
            color: #667eea;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        
        .nav-menu a:hover {
            color: #764ba2;
        }
        
        @media (max-width: 1200px) {
            .nav-menu {
                display: none;
            }
        }
        
        .highlight {
            background: linear-gradient(120deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: bold;
        }
        
        .step-box {
            background: #fff3cd;
            border: 2px solid #ffc107;
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
        }
        
        .matrix-label {
            font-family: 'Courier New', monospace;
            font-weight: bold;
            fill: #333;
        }
    </style>
</head>
<body>
    <div class="nav-menu">
        <ul>
            <li><a href="#full-attention">全注意力</a></li>
            <li><a href="#sparse-attention">稀疏注意力</a></li>
            <li><a href="#linear-attention">线性注意力</a></li>
            <li><a href="#gqa">GQA</a></li>
            <li><a href="#mqa">MQA</a></li>
            <li><a href="#flash-attention">Flash</a></li>
        </ul>
    </div>
    
    <div class="container">
        <h1>🚀 大语言模型注意力机制详解</h1>
        <p class="subtitle">深入理解Transformer的核心：从数学原理到代码实现</p>
        
        <!-- 1. 全注意力 -->
        <div class="attention-type" id="full-attention">
            <h2><span class="icon">1</span> 全注意力 (Full/Dense Attention)</h2>
            
            <h3>📐 数学原理</h3>
            <div class="math-section">
                <p><strong>缩放点积注意力 (Scaled Dot-Product Attention):</strong></p>
                <p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$</p>
                
                <div class="step-box">
                    <strong>计算步骤分解：</strong>
                    <ol style="margin-left: 20px; line-height: 2;">
                        <li><strong>步骤1：</strong> 计算注意力分数 $S = QK^T$ （Query与Key的相似度）</li>
                        <li><strong>步骤2：</strong> 缩放 $S_{scaled} = \frac{S}{\sqrt{d_k}}$ （防止梯度消失）</li>
                        <li><strong>步骤3：</strong> 归一化 $A = \text{softmax}(S_{scaled})$ （得到注意力权重）</li>
                        <li><strong>步骤4：</strong> 加权求和 $O = AV$ （根据权重聚合Value）</li>
                    </ol>
                </div>
                
                <p style="margin-top: 20px;">其中：</p>
                <ul style="margin-left: 20px;">
                    <li>$Q \in \mathbb{R}^{n \times d_k}$：查询矩阵 (Queries)</li>
                    <li>$K \in \mathbb{R}^{n \times d_k}$：键矩阵 (Keys)</li>
                    <li>$V \in \mathbb{R}^{n \times d_v}$：值矩阵 (Values)</li>
                    <li>$n$：序列长度，$d_k$：键/查询维度，$d_v$：值维度</li>
                </ul>
            </div>
            
            <h3>🎨 图形化解释 - 分步骤展示</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 600">
                    <!-- 标题 -->
                    <text x="600" y="30" text-anchor="middle" font-weight="bold" font-size="18">注意力计算详细步骤</text>
                    
                    <!-- 步骤1: Q × K^T -->
                    <g transform="translate(50, 60)">
                        <text x="100" y="0" text-anchor="middle" font-weight="bold" fill="#667eea">步骤1: 计算相似度</text>
                        
                        <!-- Q矩阵 -->
                        <rect x="0" y="20" width="60" height="100" fill="#667eea" opacity="0.7" rx="5"/>
                        <text x="30" y="75" text-anchor="middle" class="matrix-label" fill="white">Q</text>
                        <text x="30" y="135" text-anchor="middle" font-size="12">[n×d_k]</text>
                        
                        <!-- × 符号 -->
                        <text x="85" y="75" text-anchor="middle" font-size="20">×</text>
                        
                        <!-- K^T矩阵 (注意维度是转置后的) -->
                        <rect x="110" y="20" width="100" height="60" fill="#764ba2" opacity="0.7" rx="5"/>
                        <text x="160" y="55" text-anchor="middle" class="matrix-label" fill="white">K^T</text>
                        <text x="160" y="95" text-anchor="middle" font-size="12">[d_k×n]</text>
                        
                        <!-- = 符号 -->
                        <text x="235" y="75" text-anchor="middle" font-size="20">=</text>
                        
                        <!-- S矩阵 -->
                        <rect x="260" y="20" width="100" height="100" fill="#f093fb" opacity="0.7" rx="5"/>
                        <text x="310" y="75" text-anchor="middle" class="matrix-label" fill="white">S</text>
                        <text x="310" y="135" text-anchor="middle" font-size="12">[n×n]</text>
                        
                        <!-- 说明文字 -->
                        <text x="180" y="160" text-anchor="middle" font-size="11" fill="#666">
                            每个Query与所有Key的点积
                        </text>
                    </g>
                    
                    <!-- 步骤2: 缩放 -->
                    <g transform="translate(450, 60)">
                        <text x="100" y="0" text-anchor="middle" font-weight="bold" fill="#667eea">步骤2: 缩放</text>
                        
                        <!-- S矩阵 -->
                        <rect x="0" y="20" width="100" height="100" fill="#f093fb" opacity="0.7" rx="5"/>
                        <text x="50" y="75" text-anchor="middle" class="matrix-label" fill="white">S</text>
                        
                        <!-- ÷ 符号 -->
                        <text x="125" y="75" text-anchor="middle" font-size="20">÷</text>
                        
                        <!-- sqrt(d_k) -->
                        <rect x="150" y="55" width="50" height="30" fill="#ffc107" opacity="0.7" rx="5"/>
                        <text x="175" y="75" text-anchor="middle" font-size="12">√d_k</text>
                        
                        <!-- = 符号 -->
                        <text x="225" y="75" text-anchor="middle" font-size="20">=</text>
                        
                        <!-- S_scaled矩阵 -->
                        <rect x="250" y="20" width="100" height="100" fill="#ff6b6b" opacity="0.7" rx="5"/>
                        <text x="300" y="75" text-anchor="middle" class="matrix-label" fill="white">S'</text>
                        <text x="300" y="135" text-anchor="middle" font-size="12">[n×n]</text>
                        
                        <!-- 说明文字 -->
                        <text x="175" y="160" text-anchor="middle" font-size="11" fill="#666">
                            防止softmax梯度消失
                        </text>
                    </g>
                    
                    <!-- 步骤3: Softmax -->
                    <g transform="translate(850, 60)">
                        <text x="100" y="0" text-anchor="middle" font-weight="bold" fill="#667eea">步骤3: 归一化</text>
                        
                        <!-- S_scaled矩阵 -->
                        <rect x="0" y="20" width="100" height="100" fill="#ff6b6b" opacity="0.7" rx="5"/>
                        <text x="50" y="75" text-anchor="middle" class="matrix-label" fill="white">S'</text>
                        
                        <!-- softmax箭头 -->
                        <path d="M 110 70 L 140 70" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
                        <text x="125" y="60" text-anchor="middle" font-size="11">softmax</text>
                        <text x="125" y="90" text-anchor="middle" font-size="10" fill="#666">(按行)</text>
                        
                        <!-- A矩阵 -->
                        <rect x="150" y="20" width="100" height="100" fill="#f5576c" opacity="0.7" rx="5"/>
                        <text x="200" y="75" text-anchor="middle" class="matrix-label" fill="white">A</text>
                        <text x="200" y="135" text-anchor="middle" font-size="12">[n×n]</text>
                        
                        <!-- 说明文字 -->
                        <text x="125" y="160" text-anchor="middle" font-size="11" fill="#666">
                            每行和为1的权重矩阵
                        </text>
                    </g>
                    
                    <!-- 步骤4: A × V -->
                    <g transform="translate(250, 250)">
                        <text x="250" y="0" text-anchor="middle" font-weight="bold" fill="#667eea">步骤4: 加权聚合</text>
                        
                        <!-- A矩阵 -->
                        <rect x="0" y="20" width="100" height="100" fill="#f5576c" opacity="0.7" rx="5"/>
                        <text x="50" y="75" text-anchor="middle" class="matrix-label" fill="white">A</text>
                        <text x="50" y="135" text-anchor="middle" font-size="12">[n×n]</text>
                        
                        <!-- × 符号 -->
                        <text x="125" y="75" text-anchor="middle" font-size="20">×</text>
                        
                        <!-- V矩阵 -->
                        <rect x="150" y="20" width="60" height="100" fill="#4facfe" opacity="0.7" rx="5"/>
                        <text x="180" y="75" text-anchor="middle" class="matrix-label" fill="white">V</text>
                        <text x="180" y="135" text-anchor="middle" font-size="12">[n×d_v]</text>
                        
                        <!-- = 符号 -->
                        <text x="235" y="75" text-anchor="middle" font-size="20">=</text>
                        
                        <!-- Output矩阵 -->
                        <rect x="260" y="20" width="60" height="100" fill="#00f2fe" opacity="0.7" rx="5"/>
                        <text x="290" y="75" text-anchor="middle" class="matrix-label" fill="white">O</text>
                        <text x="290" y="135" text-anchor="middle" font-size="12">[n×d_v]</text>
                        
                        <!-- 说明文字 -->
                        <text x="160" y="160" text-anchor="middle" font-size="11" fill="#666">
                            根据注意力权重组合Value
                        </text>
                    </g>
                    
                    <!-- 连接线展示流程 -->
                    <g stroke="#667eea" stroke-width="2" fill="none" stroke-dasharray="5,5">
                        <path d="M 360 130 Q 400 180, 350 250"/>
                        <path d="M 800 130 Q 850 180, 350 270"/>
                    </g>
                    
                    <!-- 维度示例框 -->
                    <g transform="translate(650, 350)">
                        <rect x="0" y="0" width="400" height="180" fill="#f8f9fa" stroke="#667eea" stroke-width="2" rx="10"/>
                        <text x="200" y="25" text-anchor="middle" font-weight="bold" fill="#667eea">维度示例（n=4, d_k=64, d_v=64）</text>
                        
                        <text x="20" y="55" font-size="13">Q: [4×64] × K^T: [64×4] = S: [4×4]</text>
                        <text x="20" y="80" font-size="13">S: [4×4] ÷ √64 = S': [4×4]</text>
                        <text x="20" y="105" font-size="13">softmax(S'): [4×4] = A: [4×4]</text>
                        <text x="20" y="130" font-size="13">A: [4×4] × V: [4×64] = O: [4×64]</text>
                        
                        <text x="20" y="160" font-size="11" fill="#666">💡 输出维度与输入Q相同，但内容是V的加权组合</text>
                    </g>
                    
                    <!-- 箭头定义 -->
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" fill="#333"/>
                        </marker>
                    </defs>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 代码实现与步骤对应</h4>
                <pre>
<span class="code-comment"># 步骤1: 计算注意力分数（Query与Key的相似度）</span>
scores = torch.matmul(Q, K.transpose(-2, -1))  <span class="code-comment"># [batch, heads, n, d_k] × [batch, heads, d_k, n] = [batch, heads, n, n]</span>

<span class="code-comment"># 步骤2: 缩放（防止softmax的梯度消失）</span>
scores = scores / math.sqrt(d_k)  <span class="code-comment"># 除以√d_k，使方差稳定在1附近</span>

<span class="code-comment"># 步骤3: 应用softmax得到注意力权重（每行归一化）</span>
attn_weights = F.softmax(scores, dim=-1)  <span class="code-comment"># [batch, heads, n, n]，每行和为1</span>

<span class="code-comment"># 步骤4: 使用注意力权重对Value进行加权求和</span>
output = torch.matmul(attn_weights, V)  <span class="code-comment"># [batch, heads, n, n] × [batch, heads, n, d_v] = [batch, heads, n, d_v]</span>

<span class="code-comment"># 关键点解释：</span>
<span class="code-comment"># - Q的每一行代表一个位置的查询向量</span>
<span class="code-comment"># - K的每一行代表一个位置的键向量</span>
<span class="code-comment"># - scores[i,j]表示位置i对位置j的注意力分数</span>
<span class="code-comment"># - softmax使得每个位置的注意力权重和为1</span>
<span class="code-comment"># - 最终输出是V的加权组合，权重由Q和K的相似度决定</span></pre>
            </div>
            
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>✅ 优势</h4>
                    <p>完整的上下文建模能力</p>
                </div>
                <div class="feature-card">
                    <h4>⚠️ 劣势</h4>
                    <p>O(n²)复杂度，长序列计算昂贵</p>
                </div>
                <div class="feature-card">
                    <h4>💡 应用</h4>
                    <p>GPT-3, BERT, 标准Transformer</p>
                </div>
            </div>
        </div>
        
        <!-- 2. 稀疏注意力 -->
        <div class="attention-type" id="sparse-attention">
            <h2><span class="icon">2</span> 稀疏注意力 (Sparse Attention)</h2>
            
            <h3>📐 数学原理</h3>
            <div class="math-section">
                <p><strong>局部窗口注意力 (Local/Sliding Window):</strong></p>
                <p>$$\text{LocalAttn}(Q, K, V)_i = \text{softmax}\left(\frac{Q_i K_{[i-w:i+w]}^T}{\sqrt{d_k}}\right)V_{[i-w:i+w]}$$</p>
                <p>其中 $w$ 是窗口半径，每个位置只关注邻近的 $2w+1$ 个位置</p>
                
                <p style="margin-top: 20px;"><strong>块稀疏注意力 (Block Sparse):</strong></p>
                <p>将序列分成大小为 $b$ 的块：</p>
                <p>$$\text{BlockAttn}(Q^{(i)}, K^{(i)}, V^{(i)}) = \text{softmax}\left(\frac{Q^{(i)}(K^{(i)})^T}{\sqrt{d_k}}\right)V^{(i)}$$</p>
            </div>
            
            <h3>🎨 图形化解释</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 500">
                    <!-- 局部注意力模式 -->
                    <g transform="translate(100, 50)">
                        <text x="200" y="0" text-anchor="middle" font-weight="bold">局部窗口注意力</text>
                        
                        <!-- 注意力矩阵可视化 -->
                        <rect x="0" y="20" width="400" height="400" fill="none" stroke="#333" stroke-width="2"/>
                        
                        <!-- 窗口模式（对角线） -->
                        <g opacity="0.7">
                            <!-- 窗口块 -->
                            <rect x="0" y="20" width="100" height="100" fill="#667eea"/>
                            <rect x="50" y="70" width="100" height="100" fill="#667eea"/>
                            <rect x="100" y="120" width="100" height="100" fill="#667eea"/>
                            <rect x="150" y="170" width="100" height="100" fill="#667eea"/>
                            <rect x="200" y="220" width="100" height="100" fill="#667eea"/>
                            <rect x="250" y="270" width="100" height="100" fill="#667eea"/>
                            <rect x="300" y="320" width="100" height="100" fill="#667eea"/>
                        </g>
                        
                        <!-- 维度标注 -->
                        <text x="-30" y="220" text-anchor="middle" font-size="12">位置i</text>
                        <text x="200" y="450" text-anchor="middle" font-size="12">位置j</text>
                        
                        <!-- 示例说明 -->
                        <g transform="translate(0, 460)">
                            <rect x="0" y="0" width="15" height="15" fill="#667eea" opacity="0.7"/>
                            <text x="25" y="12" font-size="12">有注意力连接</text>
                            <rect x="150" y="0" width="15" height="15" fill="none" stroke="#333"/>
                            <text x="175" y="12" font-size="12">无注意力连接</text>
                        </g>
                    </g>
                    
                    <!-- 块稀疏注意力模式 -->
                    <g transform="translate(650, 50)">
                        <text x="200" y="0" text-anchor="middle" font-weight="bold">块稀疏注意力</text>
                        
                        <!-- 注意力矩阵 -->
                        <rect x="0" y="20" width="400" height="400" fill="none" stroke="#333" stroke-width="2"/>
                        
                        <!-- 块模式 -->
                        <g opacity="0.7">
                            <rect x="0" y="20" width="100" height="100" fill="#764ba2"/>
                            <rect x="100" y="120" width="100" height="100" fill="#764ba2"/>
                            <rect x="200" y="220" width="100" height="100" fill="#764ba2"/>
                            <rect x="300" y="320" width="100" height="100" fill="#764ba2"/>
                        </g>
                        
                        <!-- 块大小标注 -->
                        <path d="M 0 20 L 0 120" stroke="#333" stroke-width="1"/>
                        <text x="-30" y="75" text-anchor="middle" font-size="11">块1</text>
                        <path d="M 100 120 L 100 220" stroke="#333" stroke-width="1"/>
                        <text x="70" y="175" text-anchor="middle" font-size="11">块2</text>
                    </g>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 代码实现</h4>
                <pre>
<span class="code-comment"># 局部窗口注意力：每个位置只关注窗口内的位置</span>
def create_local_mask(seq_len, window_size):
    mask = torch.zeros(seq_len, seq_len)
    for i in range(seq_len):
        start = max(0, i - window_size // 2)
        end = min(seq_len, i + window_size // 2 + 1)
        mask[i, start:end] = 1  <span class="code-comment"># 位置i只关注[start:end]范围</span>
    return mask

<span class="code-comment"># 块稀疏注意力：将序列分块，块内全连接</span>
Q = Q.view(batch, heads, n_blocks, block_size, d_k)  <span class="code-comment"># 重塑为块</span>
K = K.view(batch, heads, n_blocks, block_size, d_k)
scores = torch.matmul(Q, K.transpose(-2, -1))  <span class="code-comment"># 块内计算注意力</span>

<span class="code-comment"># 复杂度分析：</span>
<span class="code-comment"># - 局部注意力: O(n × window_size × d) 而非 O(n² × d)</span>
<span class="code-comment"># - 块稀疏: O(n × block_size × d) 而非 O(n² × d)</span></pre>
            </div>
        </div>
        
        <!-- 3. 线性注意力 -->
        <div class="attention-type" id="linear-attention">
            <h2><span class="icon">3</span> 线性注意力 (Linear Attention)</h2>
            
            <h3>📐 数学原理</h3>
            <div class="math-section">
                <p><strong>核技巧 (Kernel Trick):</strong></p>
                <p>标准注意力：$$O = \text{softmax}(QK^T)V = \frac{\exp(QK^T)}{\text{rowsum}(\exp(QK^T))}V$$</p>
                <p>线性注意力（改变计算顺序）：$$O = \frac{\phi(Q)(\phi(K)^TV)}{\phi(Q)\text{sum}(\phi(K))}$$</p>
                
                <div class="step-box">
                    <strong>关键优化：改变矩阵乘法顺序</strong>
                    <ul style="margin-left: 20px;">
                        <li>标准方式：$(QK^T)V$ - 先算$QK^T$得到$n×n$矩阵，复杂度$O(n^2d)$</li>
                        <li>线性方式：$Q(K^TV)$ - 先算$K^TV$得到$d×d$矩阵，复杂度$O(nd^2)$</li>
                        <li>当$d \ll n$时，显著降低计算复杂度</li>
                    </ul>
                </div>
            </div>
            
            <h3>🎨 图形化解释</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 600">
                    <!-- 标准注意力计算顺序 -->
                    <g transform="translate(100, 50)">
                        <text x="200" y="0" text-anchor="middle" font-weight="bold" fill="#667eea">标准注意力 O(n²d)</text>
                        
                        <!-- 第一步：Q × K^T -->
                        <text x="50" y="40" font-size="12" fill="#666">第1步:</text>
                        <rect x="0" y="50" width="60" height="100" fill="#667eea" opacity="0.7"/>
                        <text x="30" y="105" text-anchor="middle" class="matrix-label" fill="white">Q</text>
                        <text x="30" y="165" text-anchor="middle" font-size="11">[n×d]</text>
                        
                        <text x="75" y="100" text-anchor="middle">×</text>
                        
                        <rect x="90" y="70" width="100" height="60" fill="#764ba2" opacity="0.7"/>
                        <text x="140" y="105" text-anchor="middle" class="matrix-label" fill="white">K^T</text>
                        <text x="140" y="145" text-anchor="middle" font-size="11">[d×n]</text>
                        
                        <text x="205" y="100" text-anchor="middle">=</text>
                        
                        <rect x="220" y="50" width="100" height="100" fill="#f093fb" opacity="0.7"/>
                        <text x="270" y="105" text-anchor="middle" class="matrix-label" fill="white">Attn</text>
                        <text x="270" y="165" text-anchor="middle" font-size="11">[n×n]</text>
                        <text x="270" y="180" text-anchor="middle" font-size="11" fill="red">大矩阵!</text>
                        
                        <!-- 第二步：Attn × V -->
                        <text x="50" y="220" font-size="12" fill="#666">第2步:</text>
                        <rect x="0" y="230" width="100" height="100" fill="#f093fb" opacity="0.7"/>
                        <text x="50" y="285" text-anchor="middle" class="matrix-label" fill="white">Attn</text>
                        
                        <text x="115" y="280" text-anchor="middle">×</text>
                        
                        <rect x="130" y="230" width="60" height="100" fill="#4facfe" opacity="0.7"/>
                        <text x="160" y="285" text-anchor="middle" class="matrix-label" fill="white">V</text>
                        <text x="160" y="345" text-anchor="middle" font-size="11">[n×d]</text>
                        
                        <text x="205" y="280" text-anchor="middle">=</text>
                        
                        <rect x="220" y="230" width="60" height="100" fill="#00f2fe" opacity="0.7"/>
                        <text x="250" y="285" text-anchor="middle" class="matrix-label" fill="white">O</text>
                        <text x="250" y="345" text-anchor="middle" font-size="11">[n×d]</text>
                        
                        <!-- 复杂度标注 -->
                        <rect x="0" y="380" width="320" height="40" fill="#ffe0e0" stroke="#ff0000" rx="5"/>
                        <text x="160" y="405" text-anchor="middle" font-size="12" fill="#ff0000">
                            总复杂度: O(n²d) - 需要存储n×n矩阵
                        </text>
                    </g>
                    
                    <!-- 线性注意力计算顺序 -->
                    <g transform="translate(650, 50)">
                        <text x="200" y="0" text-anchor="middle" font-weight="bold" fill="#667eea">线性注意力 O(nd²)</text>
                        
                        <!-- 第一步：K^T × V -->
                        <text x="50" y="40" font-size="12" fill="#666">第1步:</text>
                        <rect x="0" y="70" width="100" height="60" fill="#764ba2" opacity="0.7"/>
                        <text x="50" y="105" text-anchor="middle" class="matrix-label" fill="white">K^T</text>
                        <text x="50" y="145" text-anchor="middle" font-size="11">[d×n]</text>
                        
                        <text x="115" y="100" text-anchor="middle">×</text>
                        
                        <rect x="130" y="50" width="60" height="100" fill="#4facfe" opacity="0.7"/>
                        <text x="160" y="105" text-anchor="middle" class="matrix-label" fill="white">V</text>
                        <text x="160" y="165" text-anchor="middle" font-size="11">[n×d]</text>
                        
                        <text x="205" y="100" text-anchor="middle">=</text>
                        
                        <rect x="220" y="70" width="60" height="60" fill="#ffc107" opacity="0.7"/>
                        <text x="250" y="105" text-anchor="middle" class="matrix-label">KV</text>
                        <text x="250" y="145" text-anchor="middle" font-size="11">[d×d]</text>
                        <text x="250" y="160" text-anchor="middle" font-size="11" fill="green">小矩阵!</text>
                        
                        <!-- 第二步：Q × KV -->
                        <text x="50" y="220" font-size="12" fill="#666">第2步:</text>
                        <rect x="0" y="230" width="60" height="100" fill="#667eea" opacity="0.7"/>
                        <text x="30" y="285" text-anchor="middle" class="matrix-label" fill="white">Q</text>
                        <text x="30" y="345" text-anchor="middle" font-size="11">[n×d]</text>
                        
                        <text x="75" y="280" text-anchor="middle">×</text>
                        
                        <rect x="90" y="250" width="60" height="60" fill="#ffc107" opacity="0.7"/>
                        <text x="120" y="285" text-anchor="middle" class="matrix-label">KV</text>
                        <text x="120" y="325" text-anchor="middle" font-size="11">[d×d]</text>
                        
                        <text x="165" y="280" text-anchor="middle">=</text>
                        
                        <rect x="180" y="230" width="60" height="100" fill="#00f2fe" opacity="0.7"/>
                        <text x="210" y="285" text-anchor="middle" class="matrix-label" fill="white">O</text>
                        <text x="210" y="345" text-anchor="middle" font-size="11">[n×d]</text>
                        
                        <!-- 复杂度标注 -->
                        <rect x="0" y="380" width="280" height="40" fill="#e0ffe0" stroke="#00cc00" rx="5"/>
                        <text x="140" y="405" text-anchor="middle" font-size="12" fill="#00cc00">
                            总复杂度: O(nd²) - 只需d×d矩阵
                        </text>
                    </g>
                    
                    <!-- 对比说明 -->
                    <g transform="translate(200, 480)">
                        <rect x="0" y="0" width="800" height="80" fill="#f8f9fa" stroke="#667eea" stroke-width="2" rx="10"/>
                        <text x="400" y="25" text-anchor="middle" font-weight="bold" fill="#667eea">效率对比（假设n=2048, d=64）</text>
                        <text x="200" y="50" font-size="12">标准注意力: 2048² × 64 = 268M 操作</text>
                        <text x="200" y="70" font-size="12">线性注意力: 2048 × 64² = 8.4M 操作</text>
                        <text x="600" y="60" font-size="14" fill="#00cc00" font-weight="bold">速度提升 32倍！</text>
                    </g>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 代码实现</h4>
                <pre>
<span class="code-comment"># 线性注意力核心：改变计算顺序</span>

<span class="code-comment"># 标准注意力（低效）</span>
attn = torch.matmul(Q, K.transpose(-2, -1))  <span class="code-comment"># [n, d] × [d, n] = [n, n] 大矩阵！</span>
output = torch.matmul(attn, V)               <span class="code-comment"># [n, n] × [n, d] = [n, d]</span>

<span class="code-comment"># 线性注意力（高效）</span>
<span class="code-comment"># 步骤1: 先计算K^T V，得到小矩阵</span>
KV = torch.matmul(K.transpose(-2, -1), V)    <span class="code-comment"># [d, n] × [n, d] = [d, d] 小矩阵！</span>

<span class="code-comment"># 步骤2: 计算归一化因子</span>
Z = 1 / (torch.einsum('nd,d->n', Q, K.sum(dim=0)) + eps)  <span class="code-comment"># [n]</span>

<span class="code-comment"># 步骤3: Q与KV相乘并归一化</span>
output = torch.matmul(Q, KV) * Z.unsqueeze(-1)  <span class="code-comment"># [n, d] × [d, d] = [n, d]</span>

<span class="code-comment"># 关键点：</span>
<span class="code-comment"># - 避免了n×n的注意力矩阵</span>
<span class="code-comment"># - 当d << n时，效率提升巨大</span>
<span class="code-comment"># - 需要特征映射φ来保证非负性</span></pre>
            </div>
        </div>
        
        <!-- 4. GQA -->
        <div class="attention-type" id="gqa">
            <h2><span class="icon">4</span> 分组查询注意力 (GQA)</h2>
            
            <h3>📐 数学原理</h3>
            <div class="math-section">
                <p><strong>核心思想：</strong>将查询头分成G组，每组共享键值对</p>
                <p>$$\text{GQA}(Q^{(g)}, K^{(g/G)}, V^{(g/G)}) = \text{softmax}\left(\frac{Q^{(g)}(K^{(g/G)})^T}{\sqrt{d_k}}\right)V^{(g/G)}$$</p>
                
                <div class="step-box">
                    <strong>内存节省计算：</strong>
                    <ul style="margin-left: 20px;">
                        <li>MHA KV缓存: $2 × n_{heads} × seq\_len × d_k$</li>
                        <li>GQA KV缓存: $2 × n_{kv\_heads} × seq\_len × d_k$</li>
                        <li>节省比例: $1 - \frac{n_{kv\_heads}}{n_{heads}}$</li>
                    </ul>
                </div>
            </div>
            
            <h3>🎨 图形化解释</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 400">
                    <text x="600" y="30" text-anchor="middle" font-weight="bold" font-size="16">GQA: 8个Q头，2个KV头（4:1分组）</text>
                    
                    <!-- Query heads -->
                    <g transform="translate(100, 80)">
                        <text x="100" y="-10" text-anchor="middle" font-weight="bold">Query Heads (8个)</text>
                        
                        <!-- 第一组Q头 -->
                        <g fill="#667eea" opacity="0.8">
                            <rect x="0" y="0" width="40" height="50" stroke="#333"/>
                            <text x="20" y="30" text-anchor="middle" font-size="12" fill="white">Q₁</text>
                            <rect x="45" y="0" width="40" height="50" stroke="#333"/>
                            <text x="65" y="30" text-anchor="middle" font-size="12" fill="white">Q₂</text>
                            <rect x="90" y="0" width="40" height="50" stroke="#333"/>
                            <text x="110" y="30" text-anchor="middle" font-size="12" fill="white">Q₃</text>
                            <rect x="135" y="0" width="40" height="50" stroke="#333"/>
                            <text x="155" y="30" text-anchor="middle" font-size="12" fill="white">Q₄</text>
                        </g>
                        
                        <!-- 第二组Q头 -->
                        <g fill="#667eea" opacity="0.6">
                            <rect x="0" y="60" width="40" height="50" stroke="#333"/>
                            <text x="20" y="90" text-anchor="middle" font-size="12" fill="white">Q₅</text>
                            <rect x="45" y="60" width="40" height="50" stroke="#333"/>
                            <text x="65" y="90" text-anchor="middle" font-size="12" fill="white">Q₆</text>
                            <rect x="90" y="60" width="40" height="50" stroke="#333"/>
                            <text x="110" y="90" text-anchor="middle" font-size="12" fill="white">Q₇</text>
                            <rect x="135" y="60" width="40" height="50" stroke="#333"/>
                            <text x="155" y="90" text-anchor="middle" font-size="12" fill="white">Q₈</text>
                        </g>
                    </g>
                    
                    <!-- 连接线 -->
                    <g stroke="#333" stroke-width="1.5" fill="none">
                        <!-- 第一组连接 -->
                        <path d="M 275 105 Q 400 105, 500 130" stroke="#667eea"/>
                        <path d="M 275 105 Q 400 110, 500 130" stroke="#667eea"/>
                        <path d="M 275 105 Q 400 115, 500 130" stroke="#667eea"/>
                        <path d="M 275 105 Q 400 120, 500 130" stroke="#667eea"/>
                        
                        <!-- 第二组连接 -->
                        <path d="M 275 165 Q 400 165, 500 210" stroke="#667eea" opacity="0.6"/>
                        <path d="M 275 165 Q 400 170, 500 210" stroke="#667eea" opacity="0.6"/>
                        <path d="M 275 165 Q 400 175, 500 210" stroke="#667eea" opacity="0.6"/>
                        <path d="M 275 165 Q 400 180, 500 210" stroke="#667eea" opacity="0.6"/>
                    </g>
                    
                    <!-- KV heads -->
                    <g transform="translate(500, 100)">
                        <text x="50" y="-10" text-anchor="middle" font-weight="bold">KV Pairs (2个)</text>
                        <rect x="0" y="10" width="100" height="60" fill="#764ba2" opacity="0.8" stroke="#333"/>
                        <text x="50" y="45" text-anchor="middle" font-size="14" fill="white">K₁,V₁</text>
                        
                        <rect x="0" y="90" width="100" height="60" fill="#764ba2" opacity="0.6" stroke="#333"/>
                        <text x="50" y="125" text-anchor="middle" font-size="14" fill="white">K₂,V₂</text>
                    </g>
                    
                    <!-- 内存对比 -->
                    <g transform="translate(700, 100)">
                        <text x="200" y="-10" text-anchor="middle" font-weight="bold">KV缓存内存对比</text>
                        
                        <!-- MHA -->
                        <rect x="0" y="10" width="400" height="30" fill="#e0e0e0" stroke="#333"/>
                        <rect x="0" y="10" width="400" height="30" fill="#667eea" opacity="0.5"/>
                        <text x="200" y="30" text-anchor="middle" font-size="12">MHA: 8个KV头 (100%)</text>
                        
                        <!-- GQA -->
                        <rect x="0" y="60" width="400" height="30" fill="#e0e0e0" stroke="#333"/>
                        <rect x="0" y="60" width="100" height="30" fill="#764ba2" opacity="0.8"/>
                        <text x="200" y="80" text-anchor="middle" font-size="12">GQA: 2个KV头 (25%)</text>
                        
                        <text x="200" y="120" text-anchor="middle" font-size="14" fill="green" font-weight="bold">
                            节省75%内存！
                        </text>
                        
                        <!-- 计算示例 -->
                        <rect x="0" y="150" width="400" height="100" fill="#f8f9fa" stroke="#667eea" rx="5"/>
                        <text x="10" y="170" font-size="11">示例（seq_len=2048, d_k=64）：</text>
                        <text x="10" y="190" font-size="11">MHA: 2 × 8 × 2048 × 64 × 4B = 8MB</text>
                        <text x="10" y="210" font-size="11">GQA: 2 × 2 × 2048 × 64 × 4B = 2MB</text>
                        <text x="10" y="230" font-size="11" fill="green">每层节省6MB，12层模型节省72MB！</text>
                    </g>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 代码实现</h4>
                <pre>
<span class="code-comment"># GQA核心实现</span>
class GroupedQueryAttention(nn.Module):
    def __init__(self, d_model, n_heads=8, n_kv_heads=2):
        <span class="code-comment"># n_heads: Query头数量（如8）</span>
        <span class="code-comment"># n_kv_heads: KV头数量（如2）</span>
        <span class="code-comment"># n_groups: 每个KV头服务的Q头数（8/2=4）</span>
        self.n_groups = n_heads // n_kv_heads
        
        <span class="code-comment"># Q使用全部维度，KV使用更少维度</span>
        self.W_q = nn.Linear(d_model, d_model)              <span class="code-comment"># 8个头</span>
        self.W_k = nn.Linear(d_model, n_kv_heads * d_k)     <span class="code-comment"># 2个头</span>
        self.W_v = nn.Linear(d_model, n_kv_heads * d_k)     <span class="code-comment"># 2个头</span>
    
    def forward(self, x):
        <span class="code-comment"># 计算Q, K, V</span>
        Q = self.W_q(x).view(batch, seq, n_heads, d_k)      <span class="code-comment"># [B, N, 8, d]</span>
        K = self.W_k(x).view(batch, seq, n_kv_heads, d_k)   <span class="code-comment"># [B, N, 2, d]</span>
        V = self.W_v(x).view(batch, seq, n_kv_heads, d_k)   <span class="code-comment"># [B, N, 2, d]</span>
        
        <span class="code-comment"># 关键步骤：重复KV以匹配Q的头数</span>
        K = K.repeat_interleave(self.n_groups, dim=2)       <span class="code-comment"># [B, N, 8, d]</span>
        V = V.repeat_interleave(self.n_groups, dim=2)       <span class="code-comment"># [B, N, 8, d]</span>
        
        <span class="code-comment"># 标准注意力计算</span>
        attn = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(d_k)
        attn = F.softmax(attn, dim=-1)
        output = torch.matmul(attn, V)</pre>
            </div>
        </div>
        
        <!-- 5. MQA -->
        <div class="attention-type" id="mqa">
            <h2><span class="icon">5</span> 多查询注意力 (MQA)</h2>
            
            <h3>📐 数学原理</h3>
            <div class="math-section">
                <p><strong>核心思想：</strong>所有查询头共享单一的键值对</p>
                <p>$$\text{MQA}(Q^{(h)}, K, V) = \text{softmax}\left(\frac{Q^{(h)}K^T}{\sqrt{d_k}}\right)V$$</p>
                <p>KV缓存从 $O(n \cdot h \cdot d)$ 降到 $O(n \cdot d)$</p>
            </div>
            
            <h3>🎨 图形化解释</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 350">
                    <text x="600" y="30" text-anchor="middle" font-weight="bold" font-size="16">MQA: 所有查询头共享单个KV对</text>
                    
                    <!-- Query heads -->
                    <g transform="translate(150, 100)">
                        <text x="100" y="-10" text-anchor="middle" font-weight="bold">8个Query Heads</text>
                        <rect x="0" y="0" width="200" height="40" fill="#667eea" opacity="0.8" stroke="#333"/>
                        <text x="100" y="25" text-anchor="middle" fill="white">Q₁ Q₂ Q₃ Q₄ Q₅ Q₆ Q₇ Q₈</text>
                        
                        <!-- 表示8个独立的头 -->
                        <g transform="translate(0, 50)">
                            <circle cx="25" cy="10" r="5" fill="#667eea"/>
                            <circle cx="50" cy="10" r="5" fill="#667eea"/>
                            <circle cx="75" cy="10" r="5" fill="#667eea"/>
                            <circle cx="100" cy="10" r="5" fill="#667eea"/>
                            <circle cx="125" cy="10" r="5" fill="#667eea"/>
                            <circle cx="150" cy="10" r="5" fill="#667eea"/>
                            <circle cx="175" cy="10" r="5" fill="#667eea"/>
                            <circle cx="200" cy="10" r="5" fill="#667eea"/>
                        </g>
                    </g>
                    
                    <!-- 连接线（所有Q头连到同一个KV） -->
                    <g stroke="#333" stroke-width="1.5" fill="none">
                        <path d="M 350 120 Q 450 120, 550 150"/>
                        <path d="M 350 125 Q 450 130, 550 150"/>
                        <path d="M 350 130 Q 450 140, 550 150"/>
                        <path d="M 350 135 Q 450 150, 550 150"/>
                    </g>
                    
                    <!-- Single KV -->
                    <g transform="translate(550, 120)">
                        <text x="60" y="-10" text-anchor="middle" font-weight="bold">单个KV对</text>
                        <rect x="0" y="10" width="120" height="60" fill="#764ba2" opacity="0.8" stroke="#333" stroke-width="2"/>
                        <text x="60" y="45" text-anchor="middle" font-size="16" fill="white">K, V</text>
                        <text x="60" y="85" text-anchor="middle" font-size="11">共享给所有Q头</text>
                    </g>
                    
                    <!-- 内存节省可视化 -->
                    <g transform="translate(750, 100)">
                        <text x="150" y="-10" text-anchor="middle" font-weight="bold">KV缓存对比</text>
                        
                        <!-- MHA -->
                        <rect x="0" y="10" width="300" height="30" fill="#667eea" opacity="0.5" stroke="#333"/>
                        <text x="310" y="30" font-size="12">MHA: 8×n×d</text>
                        
                        <!-- MQA -->
                        <rect x="0" y="60" width="37.5" height="30" fill="#764ba2" opacity="0.8" stroke="#333"/>
                        <text x="310" y="80" font-size="12">MQA: 1×n×d</text>
                        
                        <text x="150" y="120" text-anchor="middle" font-size="14" fill="green" font-weight="bold">
                            节省87.5%内存！
                        </text>
                        
                        <!-- 极限优化说明 -->
                        <rect x="0" y="150" width="300" height="80" fill="#fff3cd" stroke="#ffc107" rx="5"/>
                        <text x="10" y="170" font-size="11">💡 极限内存优化：</text>
                        <text x="10" y="190" font-size="11">• 推理时KV缓存最小化</text>
                        <text x="10" y="210" font-size="11">• 适合边缘设备部署</text>
                    </g>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 代码实现</h4>
                <pre>
<span class="code-comment"># MQA核心实现</span>
class MultiQueryAttention(nn.Module):
    def __init__(self, d_model, n_heads=8):
        <span class="code-comment"># Q有多个头，K和V只有单头</span>
        self.W_q = nn.Linear(d_model, d_model)     <span class="code-comment"># 输出8个头的Q</span>
        self.W_k = nn.Linear(d_model, d_k)         <span class="code-comment"># 输出1个头的K</span>
        self.W_v = nn.Linear(d_model, d_k)         <span class="code-comment"># 输出1个头的V</span>
    
    def forward(self, x):
        <span class="code-comment"># 计算Q（多头）和KV（单头）</span>
        Q = self.W_q(x).view(batch, seq, n_heads, d_k)  <span class="code-comment"># [B, N, 8, d]</span>
        K = self.W_k(x).view(batch, seq, 1, d_k)        <span class="code-comment"># [B, N, 1, d]</span>
        V = self.W_v(x).view(batch, seq, 1, d_k)        <span class="code-comment"># [B, N, 1, d]</span>
        
        <span class="code-comment"># 关键：扩展KV到所有头（广播）</span>
        K = K.expand(-1, -1, n_heads, -1)               <span class="code-comment"># [B, N, 8, d]</span>
        V = V.expand(-1, -1, n_heads, -1)               <span class="code-comment"># [B, N, 8, d]</span>
        
        <span class="code-comment"># 标准注意力计算</span>
        attn = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(d_k)
        
<span class="code-comment"># 内存分析（seq_len=2048, d=64, 8头）：</span>
<span class="code-comment"># MHA KV缓存: 2 × 8 × 2048 × 64 × 4B = 8MB</span>
<span class="code-comment"># MQA KV缓存: 2 × 1 × 2048 × 64 × 4B = 1MB</span>
<span class="code-comment"># 节省: 87.5%！</span></pre>
            </div>
        </div>
        
        <!-- 6. Flash Attention -->
        <div class="attention-type" id="flash-attention">
            <h2><span class="icon">6</span> Flash Attention</h2>
            
            <h3>📐 数学原理</h3>
            <div class="math-section">
                <p><strong>在线Softmax (Online Softmax)：</strong></p>
                <p>数值稳定的增量计算：</p>
                <p>$$m^{new} = \max(m^{old}, \text{rowmax}(S))$$</p>
                <p>$$l^{new} = e^{m^{old}-m^{new}}l^{old} + \text{rowsum}(e^{S-m^{new}})$$</p>
                <p>$$O^{new} = \frac{e^{m^{old}-m^{new}}l^{old}O^{old} + e^{S-m^{new}}V}{l^{new}}$$</p>
                
                <div class="step-box">
                    <strong>分块策略：</strong>
                    <ul style="margin-left: 20px;">
                        <li>将Q分成大小为B_r的块</li>
                        <li>将K,V分成大小为B_c的块</li>
                        <li>逐块在SRAM中计算，减少HBM访问</li>
                    </ul>
                </div>
            </div>
            
            <h3>🎨 图形化解释</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 500">
                    <text x="600" y="30" text-anchor="middle" font-weight="bold" font-size="16">Flash Attention: 分块计算与内存优化</text>
                    
                    <!-- 内存层次 -->
                    <g transform="translate(100, 80)">
                        <text x="150" y="-10" text-anchor="middle" font-weight="bold">GPU内存层次</text>
                        
                        <!-- SRAM (快速小内存) -->
                        <rect x="0" y="0" width="300" height="50" fill="#00f2fe" opacity="0.7" stroke="#333"/>
                        <text x="150" y="30" text-anchor="middle">SRAM (20KB, 19TB/s)</text>
                        
                        <!-- HBM (慢速大内存) -->
                        <rect x="0" y="80" width="300" height="100" fill="#667eea" opacity="0.5" stroke="#333"/>
                        <text x="150" y="135" text-anchor="middle">HBM (40GB, 1.5TB/s)</text>
                        
                        <!-- 数据移动箭头 -->
                        <g stroke-width="2" marker-end="url(#arrowhead)">
                            <path d="M 100 50 L 100 80" stroke="#ff0000"/>
                            <text x="120" y="65" font-size="11" fill="#ff0000">写回</text>
                            <path d="M 200 80 L 200 50" stroke="#00cc00"/>
                            <text x="220" y="65" font-size="11" fill="#00cc00">加载</text>
                        </g>
                        
                        <!-- 瓶颈说明 -->
                        <rect x="0" y="200" width="300" height="60" fill="#ffe0e0" stroke="#ff0000" rx="5"/>
                        <text x="10" y="220" font-size="11">❌ 标准注意力问题：</text>
                        <text x="10" y="240" font-size="11">n×n矩阵太大，无法放入SRAM</text>
                    </g>
                    
                    <!-- 分块计算可视化 -->
                    <g transform="translate(500, 80)">
                        <text x="300" y="-10" text-anchor="middle" font-weight="bold">分块计算策略</text>
                        
                        <!-- Q的分块 -->
                        <g>
                            <text x="30" y="20" font-size="12">Q块</text>
                            <rect x="0" y="30" width="60" height="200" fill="#667eea" opacity="0.3" stroke="#333"/>
                            <!-- 当前处理的块 -->
                            <rect x="0" y="30" width="60" height="50" fill="#667eea" opacity="0.8" stroke="#333" stroke-width="2"/>
                            <text x="30" y="60" text-anchor="middle" font-size="11" fill="white">Q₁</text>
                            <rect x="0" y="80" width="60" height="50" fill="#667eea" opacity="0.4" stroke="#333"/>
                            <rect x="0" y="130" width="60" height="50" fill="#667eea" opacity="0.4" stroke="#333"/>
                            <rect x="0" y="180" width="60" height="50" fill="#667eea" opacity="0.4" stroke="#333"/>
                        </g>
                        
                        <!-- K,V的分块 -->
                        <g transform="translate(100, 0)">
                            <text x="100" y="20" font-size="12">K,V块</text>
                            <rect x="0" y="30" width="200" height="60" fill="#764ba2" opacity="0.3" stroke="#333"/>
                            <!-- 当前处理的块 -->
                            <rect x="0" y="30" width="50" height="60" fill="#764ba2" opacity="0.8" stroke="#333" stroke-width="2"/>
                            <text x="25" y="65" text-anchor="middle" font-size="11" fill="white">K₁V₁</text>
                            <rect x="50" y="30" width="50" height="60" fill="#764ba2" opacity="0.4" stroke="#333"/>
                            <rect x="100" y="30" width="50" height="60" fill="#764ba2" opacity="0.4" stroke="#333"/>
                            <rect x="150" y="30" width="50" height="60" fill="#764ba2" opacity="0.4" stroke="#333"/>
                        </g>
                        
                        <!-- 输出块 -->
                        <g transform="translate(350, 0)">
                            <text x="30" y="20" font-size="12">输出</text>
                            <rect x="0" y="30" width="60" height="50" fill="#00f2fe" opacity="0.8" stroke="#333" stroke-width="2"/>
                            <text x="30" y="60" text-anchor="middle" font-size="11">O₁</text>
                        </g>
                        
                        <!-- 计算流程箭头 -->
                        <g stroke="#333" stroke-width="1.5" marker-end="url(#arrowhead)">
                            <path d="M 65 55 L 95 55"/>
                            <path d="M 300 60 L 345 55"/>
                        </g>
                        
                        <!-- SRAM标注 -->
                        <rect x="0" y="250" width="400" height="40" fill="#e0ffe0" stroke="#00cc00" rx="5"/>
                        <text x="200" y="275" text-anchor="middle" font-size="12" fill="#00cc00">
                            ✓ 当前块完全在SRAM中计算
                        </text>
                    </g>
                    
                    <!-- 算法流程 -->
                    <g transform="translate(100, 380)">
                        <rect x="0" y="0" width="1000" height="100" fill="#f8f9fa" stroke="#667eea" stroke-width="2" rx="10"/>
                        <text x="500" y="25" text-anchor="middle" font-weight="bold" fill="#667eea">Flash Attention算法流程</text>
                        
                        <text x="20" y="50" font-size="12">for i in range(0, N, Br):  <span class="code-comment" fill="#008000"># 遍历Q块</span></text>
                        <text x="40" y="70" font-size="12">for j in range(0, N, Bc):  <span class="code-comment" fill="#008000"># 遍历KV块</span></text>
                        <text x="60" y="90" font-size="12">在SRAM中计算 Q[i]×K[j]^T×V[j]，增量更新O[i]</text>
                    </g>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 代码实现</h4>
                <pre>
<span class="code-comment"># Flash Attention核心循环</span>
def flash_attention_forward(Q, K, V):
    <span class="code-comment"># Q: [batch, heads, N, d]</span>
    <span class="code-comment"># 分块大小（适配SRAM大小）</span>
    Br = 64  <span class="code-comment"># Q块大小</span>
    Bc = 64  <span class="code-comment"># KV块大小</span>
    
    N = Q.shape[2]
    O = torch.zeros_like(Q)  <span class="code-comment"># 输出</span>
    L = torch.zeros(batch, heads, N)  <span class="code-comment"># 归一化因子</span>
    
    <span class="code-comment"># 外循环：遍历Q的块</span>
    for i in range(0, N, Br):
        Qi = Q[:, :, i:i+Br, :]  <span class="code-comment"># 加载Q块到SRAM</span>
        Oi = torch.zeros_like(Qi)
        Li = torch.zeros(batch, heads, Br) - float('inf')
        Mi = torch.full_like(Li, -float('inf'))  <span class="code-comment"># 最大值</span>
        
        <span class="code-comment"># 内循环：遍历KV的块</span>
        for j in range(0, N, Bc):
            Kj = K[:, :, j:j+Bc, :]  <span class="code-comment"># 加载KV块到SRAM</span>
            Vj = V[:, :, j:j+Bc, :]
            
            <span class="code-comment"># 在SRAM中计算注意力</span>
            Sij = torch.matmul(Qi, Kj.transpose(-2, -1)) / sqrt(d)
            
            <span class="code-comment"># 在线softmax（数值稳定）</span>
            Mi_new = torch.max(Mi, Sij.max(dim=-1)[0])
            Pij = torch.exp(Sij - Mi_new.unsqueeze(-1))
            
            <span class="code-comment"># 增量更新输出</span>
            Li = torch.exp(Mi - Mi_new) * Li + Pij.sum(dim=-1)
            Oi = torch.exp(Mi - Mi_new).unsqueeze(-1) * Oi + torch.matmul(Pij, Vj)
            Mi = Mi_new
        
        <span class="code-comment"># 归一化并写回HBM</span>
        O[:, :, i:i+Br, :] = Oi / Li.unsqueeze(-1)
    
    return O

<span class="code-comment"># 关键优化：</span>
<span class="code-comment"># 1. 避免存储n×n注意力矩阵</span>
<span class="code-comment"># 2. 分块计算，每块都在SRAM中</span>
<span class="code-comment"># 3. IO复杂度从O(N²) → O(N²/M)，M是SRAM大小</span></pre>
            </div>
        </div>
        
        <!-- 性能对比表格 -->
        <div class="attention-type">
            <h2><span class="icon">📊</span> 性能对比总结</h2>
            
            <table class="complexity-table">
                <thead>
                    <tr>
                        <th>注意力机制</th>
                        <th>时间复杂度</th>
                        <th>内存复杂度</th>
                        <th>KV缓存</th>
                        <th>最适合场景</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><span class="highlight">Full Attention</span></td>
                        <td>O(n²d)</td>
                        <td>O(n²)</td>
                        <td>O(nhd)</td>
                        <td>短序列，完整建模</td>
                    </tr>
                    <tr>
                        <td><span class="highlight">Local Attention</span></td>
                        <td>O(nwd)</td>
                        <td>O(nw)</td>
                        <td>O(nhd)</td>
                        <td>长序列，局部依赖</td>
                    </tr>
                    <tr>
                        <td><span class="highlight">Linear Attention</span></td>
                        <td>O(nd²)</td>
                        <td>O(nd)</td>
                        <td>O(nhd)</td>
                        <td>超长序列</td>
                    </tr>
                    <tr>
                        <td><span class="highlight">GQA</span></td>
                        <td>O(n²d)</td>
                        <td>O(n²)</td>
                        <td>O(n(h/G)d)</td>
                        <td>推理优化</td>
                    </tr>
                    <tr>
                        <td><span class="highlight">MQA</span></td>
                        <td>O(n²d)</td>
                        <td>O(n²)</td>
                        <td>O(nd)</td>
                        <td>极限内存优化</td>
                    </tr>
                    <tr>
                        <td><span class="highlight">Flash Attention</span></td>
                        <td>O(n²d)</td>
                        <td>O(n)</td>
                        <td>O(nhd)</td>
                        <td>训练加速</td>
                    </tr>
                </tbody>
            </table>
            
            <div style="margin-top: 30px; padding: 20px; background: #f0f8ff; border-radius: 10px;">
                <h3>💡 选择建议</h3>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li><strong>训练阶段：</strong>Flash Attention + Full Attention</li>
                    <li><strong>推理优化：</strong>GQA/MQA 减少KV缓存</li>
                    <li><strong>长文本（8K-32K）：</strong>Local/Sparse Attention</li>
                    <li><strong>超长文本（>32K）：</strong>Linear Attention变体</li>
                    <li><strong>资源受限：</strong>MQA + Flash Attention组合</li>
                </ul>
            </div>
        </div>
    </div>
</body>
</html>
