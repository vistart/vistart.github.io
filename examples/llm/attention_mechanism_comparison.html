<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>注意力机制详解</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script>
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: true
            }
        });
    </script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        }
        
        h1 {
            text-align: center;
            color: #764ba2;
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.1);
        }
        
        .subtitle {
            text-align: center;
            color: #666;
            margin-bottom: 40px;
            font-size: 1.1em;
        }
        
        .attention-type {
            margin-bottom: 60px;
            padding: 30px;
            background: white;
            border-radius: 15px;
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s, box-shadow 0.3s;
        }
        
        .attention-type:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.15);
        }
        
        h2 {
            color: #667eea;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .icon {
            width: 30px;
            height: 30px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            border-radius: 50%;
            display: inline-flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
        }
        
        h3 {
            color: #444;
            margin-top: 25px;
            margin-bottom: 15px;
            font-size: 1.3em;
        }
        
        .math-section {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 4px solid #667eea;
        }
        
        .visualization {
            margin: 30px 0;
            padding: 20px;
            background: #fff;
            border-radius: 10px;
            border: 2px solid #e0e0e0;
        }
        
        svg {
            width: 100%;
            height: auto;
            max-width: 1200px;
            display: block;
            margin: 0 auto;
        }
        
        .code-ref {
            background: #f4f4f4;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #764ba2;
            font-family: 'Courier New', monospace;
        }
        
        .code-ref h4 {
            color: #764ba2;
            margin-bottom: 10px;
        }
        
        .code-comment {
            color: #008000;
            font-style: italic;
        }
        
        .complexity-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
        
        .complexity-table th, .complexity-table td {
            padding: 12px;
            text-align: left;
            border: 1px solid #ddd;
        }
        
        .complexity-table th {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            font-weight: bold;
        }
        
        .complexity-table tr:nth-child(even) {
            background: #f8f9fa;
        }
        
        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .feature-card {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            transition: transform 0.3s;
        }
        
        .feature-card:hover {
            transform: scale(1.05);
        }
        
        .feature-card h4 {
            color: #667eea;
            margin-bottom: 10px;
        }
        
        .nav-menu {
            position: fixed;
            right: 20px;
            top: 50%;
            transform: translateY(-50%);
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.2);
            z-index: 1000;
        }
        
        .nav-menu ul {
            list-style: none;
        }
        
        .nav-menu li {
            margin: 10px 0;
        }
        
        .nav-menu a {
            color: #667eea;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        
        .nav-menu a:hover {
            color: #764ba2;
        }
        
        @media (max-width: 1200px) {
            .nav-menu {
                display: none;
            }
        }
        
        .highlight {
            background: linear-gradient(120deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: bold;
        }
        
        .step-box {
            background: #fff3cd;
            border: 2px solid #ffc107;
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
        }
        
        .matrix-label {
            font-family: 'Courier New', monospace;
            font-weight: bold;
            fill: #333;
        }
    </style>
</head>
<body>
    <div class="nav-menu">
        <ul>
            <li><a href="#full-attention">全注意力</a></li>
            <li><a href="#sparse-attention">稀疏注意力</a></li>
            <li><a href="#linear-attention">线性注意力</a></li>
            <li><a href="#gqa">GQA</a></li>
            <li><a href="#mqa">MQA</a></li>
            <li><a href="#flash-attention">Flash</a></li>
            <li style="margin-top: 15px; padding-top: 15px; border-top: 1px solid #ddd;">
                <a href="#training-inference">训练vs推理</a>
            </li>
            <li><a href="#model-scale">模型规模</a></li>
        </ul>
    </div>
    
    <div class="container">
        <h1>🚀 大语言模型注意力机制详解</h1>
        <p class="subtitle">深入理解Transformer的核心：从数学原理到代码实现</p>
        
        <!-- 训练与推理详解 -->
        <div class="attention-type" id="training-inference">
            <h2><span class="icon">🔄</span> 训练与推理详解</h2>
            
            <h3>📚 QKV的本质含义</h3>
            <div class="math-section">
                <p><b>Query (Q)：查询向量</b></p>
                <ul style="margin-left: 20px;">
                    <li>代表"我在找什么信息"</li>
                    <li>每个位置的token生成一个Query，用于查询其他位置的信息</li>
                </ul>
                
                <p style="margin-top: 15px;"><b>Key (K)：键向量</b></p>
                <ul style="margin-left: 20px;">
                    <li>代表"我能提供什么信息"</li>
                    <li>每个位置的token生成一个Key，供其他位置查询</li>
                </ul>
                
                <p style="margin-top: 15px;"><b>Value (V)：值向量</b></p>
                <ul style="margin-left: 20px;">
                    <li>代表"我的实际信息内容"</li>
                    <li>根据Query-Key的匹配度，Value被加权聚合</li>
                </ul>
                
                <div class="step-box">
                    <b>注意力输出的含义：</b>
                    <p>Attention输出是所有位置Value的加权和，权重由Query与Key的相似度决定。</p>
                    <p>本质上是：<em>"根据当前位置的需求(Q)，从所有位置中按相关性(K)提取信息(V)"</em></p>
                </div>
            </div>
            
            <h3>🚀 训练：并行计算所有位置</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 700">
                    <text x="600" y="30" text-anchor="middle" font-weight="bold" font-size="18">训练时的并行计算（Teacher Forcing）</text>
                    
                    <!-- 左侧：输入序列和处理流程 -->
                    <g transform="translate(50, 60)">
                        <text x="250" y="0" text-anchor="middle" font-weight="bold">输入序列："我爱北京天安门"</text>
                        
                        <!-- Token嵌入 -->
                        <g transform="translate(0, 20)">
                            <rect x="0" y="0" width="60" height="40" fill="#667eea" opacity="0.7" stroke="#333"/>
                            <text x="30" y="25" text-anchor="middle" fill="white">我</text>
                            
                            <rect x="70" y="0" width="60" height="40" fill="#667eea" opacity="0.7" stroke="#333"/>
                            <text x="100" y="25" text-anchor="middle" fill="white">爱</text>
                            
                            <rect x="140" y="0" width="60" height="40" fill="#667eea" opacity="0.7" stroke="#333"/>
                            <text x="170" y="25" text-anchor="middle" fill="white">北京</text>
                            
                            <rect x="210" y="0" width="60" height="40" fill="#667eea" opacity="0.7" stroke="#333"/>
                            <text x="240" y="25" text-anchor="middle" fill="white">天安</text>
                            
                            <rect x="280" y="0" width="60" height="40" fill="#667eea" opacity="0.7" stroke="#333"/>
                            <text x="310" y="25" text-anchor="middle" fill="white">门</text>
                            
                            <rect x="350" y="0" width="60" height="40" fill="#667eea" opacity="0.7" stroke="#333"/>
                            <text x="380" y="25" text-anchor="middle" fill="white">[EOS]</text>
                        </g>
                        
                        <!-- 并行箭头 -->
                        <g stroke="#333" stroke-width="2" marker-end="url(#arrowhead)" transform="translate(0, 20)">
                            <path d="M 30 45 L 30 75"/>
                            <path d="M 100 45 L 100 75"/>
                            <path d="M 170 45 L 170 75"/>
                            <path d="M 240 45 L 240 75"/>
                            <path d="M 310 45 L 310 75"/>
                            <path d="M 380 45 L 380 75"/>
                        </g>
                        
                        <!-- QKV生成（并行） -->
                        <text x="200" y="120" text-anchor="middle" font-size="12">所有位置同时生成Q、K、V</text>
                        
                        <g transform="translate(0, 130)">
                            <rect x="0" y="0" width="420" height="30" fill="#f093fb" opacity="0.5"/>
                            <text x="210" y="20" text-anchor="middle">Q: [6×d_model] 并行计算</text>
                            
                            <rect x="0" y="35" width="420" height="30" fill="#764ba2" opacity="0.5"/>
                            <text x="210" y="55" text-anchor="middle">K: [6×d_model] 并行计算</text>
                            
                            <rect x="0" y="70" width="420" height="30" fill="#4facfe" opacity="0.5"/>
                            <text x="210" y="90" text-anchor="middle">V: [6×d_model] 并行计算</text>
                        </g>
                        
                        <!-- 因果掩码 -->
                        <g transform="translate(0, 260)">
                            <text x="210" y="0" text-anchor="middle" font-weight="bold">因果掩码（Causal Mask）</text>
                            <rect x="50" y="10" width="300" height="300" fill="none" stroke="#333" stroke-width="2"/>
                            
                            <!-- 下三角掩码 -->
                            <g opacity="0.7">
                                <!-- 第1行：只能看到"我" -->
                                <rect x="50" y="10" width="50" height="50" fill="#00cc00"/>
                                
                                <!-- 第2行：能看到"我爱" -->
                                <rect x="50" y="60" width="50" height="50" fill="#00cc00"/>
                                <rect x="100" y="60" width="50" height="50" fill="#00cc00"/>
                                
                                <!-- 第3行：能看到"我爱北京" -->
                                <rect x="50" y="110" width="50" height="50" fill="#00cc00"/>
                                <rect x="100" y="110" width="50" height="50" fill="#00cc00"/>
                                <rect x="150" y="110" width="50" height="50" fill="#00cc00"/>
                                
                                <!-- 第4行：能看到"我爱北京天安" -->
                                <rect x="50" y="160" width="50" height="50" fill="#00cc00"/>
                                <rect x="100" y="160" width="50" height="50" fill="#00cc00"/>
                                <rect x="150" y="160" width="50" height="50" fill="#00cc00"/>
                                <rect x="200" y="160" width="50" height="50" fill="#00cc00"/>
                                
                                <!-- 第5行：能看到"我爱北京天安门" -->
                                <rect x="50" y="210" width="50" height="50" fill="#00cc00"/>
                                <rect x="100" y="210" width="50" height="50" fill="#00cc00"/>
                                <rect x="150" y="210" width="50" height="50" fill="#00cc00"/>
                                <rect x="200" y="210" width="50" height="50" fill="#00cc00"/>
                                <rect x="250" y="210" width="50" height="50" fill="#00cc00"/>
                                
                                <!-- 第6行：能看到所有 -->
                                <rect x="50" y="260" width="50" height="50" fill="#00cc00"/>
                                <rect x="100" y="260" width="50" height="50" fill="#00cc00"/>
                                <rect x="150" y="260" width="50" height="50" fill="#00cc00"/>
                                <rect x="200" y="260" width="50" height="50" fill="#00cc00"/>
                                <rect x="250" y="260" width="50" height="50" fill="#00cc00"/>
                                <rect x="300" y="260" width="50" height="50" fill="#00cc00"/>
                            </g>
                            
                            <text x="200" y="340" text-anchor="middle" font-size="11" fill="#666">
                                绿色=可见，白色=被掩码（防止看到未来）
                            </text>
                        </g>
                    </g>
                    
                    <!-- 右侧：并行优势说明 -->
                    <g transform="translate(650, 80)">
                        <rect x="0" y="0" width="450" height="220" fill="#e8f5e9" stroke="#4caf50" stroke-width="2" rx="10"/>
                        <text x="225" y="30" text-anchor="middle" font-weight="bold" fill="#2e7d32">为什么训练可以并行？</text>
                        
                        <text x="20" y="60" font-size="13">1. Teacher Forcing：训练时知道完整答案</text>
                        <text x="20" y="85" font-size="13">2. 因果掩码：确保每个位置只看到之前的内容</text>
                        <text x="20" y="110" font-size="13">3. 矩阵运算：所有位置的注意力一次计算</text>
                        <text x="20" y="135" font-size="13">4. GPU并行：充分利用GPU的并行计算能力</text>
                        
                        <rect x="20" y="160" width="410" height="45" fill="#fff3cd" stroke="#ffc107" rx="5"/>
                        <text x="30" y="180" font-size="12">💡 一次前向传播处理整个序列，</text>
                        <text x="30" y="195" font-size="12">    而不是逐个token计算</text>
                    </g>
                    
                    <!-- 目标输出 -->
                    <g transform="translate(650, 350)">
                        <text x="225" y="0" text-anchor="middle" font-weight="bold">训练目标（右移一位）</text>
                        
                        <g transform="translate(0, 20)">
                            <rect x="0" y="0" width="60" height="40" fill="#ff6b6b" opacity="0.7" stroke="#333"/>
                            <text x="30" y="25" text-anchor="middle">爱</text>
                            
                            <rect x="70" y="0" width="60" height="40" fill="#ff6b6b" opacity="0.7" stroke="#333"/>
                            <text x="100" y="25" text-anchor="middle">北京</text>
                            
                            <rect x="140" y="0" width="60" height="40" fill="#ff6b6b" opacity="0.7" stroke="#333"/>
                            <text x="170" y="25" text-anchor="middle">天安</text>
                            
                            <rect x="210" y="0" width="60" height="40" fill="#ff6b6b" opacity="0.7" stroke="#333"/>
                            <text x="240" y="25" text-anchor="middle">门</text>
                            
                            <rect x="280" y="0" width="60" height="40" fill="#ff6b6b" opacity="0.7" stroke="#333"/>
                            <text x="310" y="25" text-anchor="middle">[EOS]</text>
                            
                            <rect x="350" y="0" width="60" height="40" fill="#ff6b6b" opacity="0.7" stroke="#333"/>
                            <text x="380" y="25" text-anchor="middle">-</text>
                        </g>
                        
                        <text x="225" y="90" text-anchor="middle" font-size="12" fill="#666">
                            Loss = CrossEntropy(预测输出, 目标输出)
                        </text>
                    </g>
                </svg>
            </div>
            
            <h3>🔮 推理：逐个生成Token</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 750">
                    <text x="600" y="30" text-anchor="middle" font-weight="bold" font-size="18">推理时的自回归生成（Auto-regressive）</text>
                    
                    <!-- Step 1 -->
                    <g transform="translate(50, 80)">
                        <rect x="0" y="0" width="250" height="140" fill="#f0f0f0" stroke="#667eea" stroke-width="2" rx="10"/>
                        <text x="125" y="20" text-anchor="middle" font-weight="bold" fill="#667eea">Step 1: 输入"我"</text>
                        
                        <rect x="20" y="35" width="50" height="30" fill="#667eea" opacity="0.7"/>
                        <text x="45" y="55" text-anchor="middle" fill="white" font-size="12">我</text>
                        
                        <path d="M 45 70 L 45 90" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
                        
                        <text x="45" y="110" text-anchor="middle" font-size="11">生成Q₁,K₁,V₁</text>
                        <text x="45" y="130" text-anchor="middle" font-size="11">Attn → FFN</text>
                        
                        <rect x="150" y="70" width="50" height="30" fill="#00cc00" opacity="0.7"/>
                        <text x="175" y="90" text-anchor="middle" font-size="12">爱</text>
                        
                        <text x="125" y="160" text-anchor="middle" font-size="11">输出: "爱"</text>
                        <text x="125" y="180" text-anchor="middle" font-size="11" fill="#666">KV Cache: [K₁,V₁]</text>
                    </g>
                    
                    <!-- Step 2 -->
                    <g transform="translate(320, 80)">
                        <rect x="0" y="0" width="250" height="140" fill="#f0f0f0" stroke="#667eea" stroke-width="2" rx="10"/>
                        <text x="125" y="20" text-anchor="middle" font-weight="bold" fill="#667eea">Step 2: 输入"我爱"</text>
                        
                        <rect x="20" y="35" width="50" height="30" fill="#667eea" opacity="0.5"/>
                        <text x="45" y="55" text-anchor="middle" fill="white" font-size="12">我</text>
                        <rect x="75" y="35" width="50" height="30" fill="#667eea" opacity="0.7"/>
                        <text x="100" y="55" text-anchor="middle" fill="white" font-size="12">爱</text>
                        
                        <path d="M 100 70 L 100 90" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
                        
                        <text x="75" y="110" text-anchor="middle" font-size="11">复用K₁,V₁</text>
                        <text x="75" y="130" text-anchor="middle" font-size="11">只算Q₂,K₂,V₂</text>
                        
                        <rect x="150" y="70" width="50" height="30" fill="#00cc00" opacity="0.7"/>
                        <text x="175" y="90" text-anchor="middle" font-size="12">北京</text>
                        
                        <text x="125" y="160" text-anchor="middle" font-size="11">输出: "北京"</text>
                        <text x="125" y="180" text-anchor="middle" font-size="11" fill="#666">KV Cache: [K₁,V₁,K₂,V₂]</text>
                    </g>
                    
                    <!-- Step 3 -->
                    <g transform="translate(590, 80)">
                        <rect x="0" y="0" width="280" height="140" fill="#f0f0f0" stroke="#667eea" stroke-width="2" rx="10"/>
                        <text x="140" y="20" text-anchor="middle" font-weight="bold" fill="#667eea">Step 3: 输入"我爱北京"</text>
                        
                        <rect x="20" y="35" width="50" height="30" fill="#667eea" opacity="0.3"/>
                        <text x="45" y="55" text-anchor="middle" fill="white" font-size="12">我</text>
                        <rect x="75" y="35" width="50" height="30" fill="#667eea" opacity="0.5"/>
                        <text x="100" y="55" text-anchor="middle" fill="white" font-size="12">爱</text>
                        <rect x="130" y="35" width="50" height="30" fill="#667eea" opacity="0.7"/>
                        <text x="155" y="55" text-anchor="middle" fill="white" font-size="12">北京</text>
                        
                        <path d="M 155 70 L 155 90" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
                        
                        <text x="100" y="110" text-anchor="middle" font-size="11">复用K₁,V₁,K₂,V₂</text>
                        <text x="100" y="130" text-anchor="middle" font-size="11">只算Q₃,K₃,V₃</text>
                        
                        <rect x="200" y="70" width="50" height="30" fill="#00cc00" opacity="0.7"/>
                        <text x="225" y="90" text-anchor="middle" font-size="12">天安</text>
                        
                        <text x="140" y="160" text-anchor="middle" font-size="11">输出: "天安"</text>
                        <text x="140" y="180" text-anchor="middle" font-size="11" fill="#666">KV Cache持续增长...</text>
                    </g>
                    
                    <!-- KV Cache说明 -->
                    <g transform="translate(100, 250)">
                        <rect x="0" y="0" width="1000" height="180" fill="#fff3cd" stroke="#ffc107" stroke-width="2" rx="10"/>
                        <text x="500" y="30" text-anchor="middle" font-weight="bold" fill="#856404">KV Cache机制</text>
                        
                        <g transform="translate(50, 50)">
                            <text x="0" y="0" font-size="13">• 每步只需计算新位置的Q、K、V</text>
                            <text x="0" y="25" font-size="13">• 之前位置的K、V保存在缓存中复用</text>
                            <text x="0" y="50" font-size="13">• 注意力计算: Q_new × [K_cache; K_new]ᵀ</text>
                            <text x="0" y="75" font-size="13">• 内存需求: 2 × n_layers × seq_len × n_heads × d_head</text>
                            
                            <rect x="400" y="-10" width="500" height="90" fill="white" stroke="#856404" rx="5"/>
                            <text x="410" y="10" font-size="12" font-weight="bold">计算示例（单个token）：</text>
                            <text x="410" y="30" font-size="11">1. 输入embedding: [1 × d_model]</text>
                            <text x="410" y="48" font-size="11">2. 生成Q_new: [1 × n_heads × d_head]</text>
                            <text x="410" y="66" font-size="11">3. Attention: Q_new × K_cache^T → [1 × seq_len]</text>
                        </g>
                    </g>
                    
                    <!-- 推理特点 -->
                    <g transform="translate(100, 460)">
                        <rect x="0" y="0" width="450" height="170" fill="#ffe0e0" stroke="#ff0000" stroke-width="2" rx="10"/>
                        <text x="225" y="30" text-anchor="middle" font-weight="bold" fill="#cc0000">推理瓶颈</text>
                        
                        <text x="20" y="60" font-size="13">❌ 顺序依赖：必须等前一个token生成完</text>
                        <text x="20" y="85" font-size="13">❌ 内存瓶颈：KV Cache随序列长度线性增长</text>
                        <text x="20" y="110" font-size="13">❌ 低GPU利用率：每步只处理1个token</text>
                        
                        <text x="225" y="145" text-anchor="middle" font-size="11" fill="#666">这就是为什么需要GQA/MQA优化！</text>
                    </g>
                    
                    <g transform="translate(580, 460)">
                        <rect x="0" y="0" width="450" height="170" fill="#e0ffe0" stroke="#00cc00" stroke-width="2" rx="10"/>
                        <text x="225" y="30" text-anchor="middle" font-weight="bold" fill="#00aa00">优化方案</text>
                        
                        <text x="20" y="60" font-size="13">✓ 批处理：同时处理多个序列</text>
                        <text x="20" y="85" font-size="13">✓ GQA/MQA：减少KV Cache内存</text>
                        <text x="20" y="110" font-size="13">✓ 投机解码：小模型预测，大模型验证</text>
                        
                        <text x="225" y="145" text-anchor="middle" font-size="11" fill="#666">Flash Attention也能加速推理！</text>
                    </g>
                </svg>
            </div>
        </div>
        
        <!-- 模型规模详解 -->
        <div class="attention-type" id="model-scale">
            <h2><span class="icon">📏</span> 主流模型规模详解</h2>
            
            <h3>🎯 典型模型参数配置</h3>
            <table class="complexity-table">
                <thead>
                    <tr>
                        <th>模型</th>
                        <th>参数量</th>
                        <th>层数</th>
                        <th>隐藏维度</th>
                        <th>注意力头数</th>
                        <th>头维度</th>
                        <th>词表大小</th>
                        <th>最大序列长度</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><b>GPT-3</b></td>
                        <td>175B</td>
                        <td>96</td>
                        <td>12,288</td>
                        <td>96</td>
                        <td>128</td>
                        <td>50,257</td>
                        <td>2,048</td>
                    </tr>
                    <tr>
                        <td><b>Llama 2-7B</b></td>
                        <td>7B</td>
                        <td>32</td>
                        <td>4,096</td>
                        <td>32 (Q) / 8 (KV)</td>
                        <td>128</td>
                        <td>32,000</td>
                        <td>4,096</td>
                    </tr>
                    <tr>
                        <td><b>Llama 2-70B</b></td>
                        <td>70B</td>
                        <td>80</td>
                        <td>8,192</td>
                        <td>64 (Q) / 8 (KV)</td>
                        <td>128</td>
                        <td>32,000</td>
                        <td>4,096</td>
                    </tr>
                    <tr>
                        <td><b>Mistral-7B</b></td>
                        <td>7B</td>
                        <td>32</td>
                        <td>4,096</td>
                        <td>32 (Q) / 8 (KV)</td>
                        <td>128</td>
                        <td>32,000</td>
                        <td>32,768</td>
                    </tr>
                    <tr>
                        <td><b>PaLM</b></td>
                        <td>540B</td>
                        <td>118</td>
                        <td>18,432</td>
                        <td>48 (MQA)</td>
                        <td>256</td>
                        <td>256,000</td>
                        <td>2,048</td>
                    </tr>
                    <tr>
                        <td><b>Claude 3</b></td>
                        <td>~100B</td>
                        <td>~80</td>
                        <td>~12,288</td>
                        <td>~96</td>
                        <td>128</td>
                        <td>~100,000</td>
                        <td>200,000</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>📊 矩阵规模计算示例（以Llama 2-7B为例）</h3>
            <div class="math-section">
                <p><b>单层注意力矩阵规模：</b></p>
                
                <div class="step-box">
                    <p><b>参数设置：</b></p>
                    <ul style="margin-left: 20px;">
                        <li>d_model = 4,096（隐藏维度）</li>
                        <li>n_heads = 32（查询头数）</li>
                        <li>n_kv_heads = 8（KV头数，GQA）</li>
                        <li>d_head = 128（每个头的维度）</li>
                        <li>seq_len = 4,096（序列长度）</li>
                    </ul>
                </div>
                
                <p style="margin-top: 20px;"><b>权重矩阵大小：</b></p>
                <ul style="margin-left: 20px;">
                    <li>W_q: [4096 × 4096] = 16.8M参数</li>
                    <li>W_k: [4096 × 1024] = 4.2M参数（GQA优化）</li>
                    <li>W_v: [4096 × 1024] = 4.2M参数（GQA优化）</li>
                    <li>W_o: [4096 × 4096] = 16.8M参数</li>
                    <li><b>单层注意力总计：42M参数</b></li>
                </ul>
                
                <p style="margin-top: 20px;"><b>运行时矩阵大小（批次=1）：</b></p>
                <ul style="margin-left: 20px;">
                    <li>Q: [4096 × 32 × 128] = 16MB（FP32）</li>
                    <li>K: [4096 × 8 × 128] = 4MB（GQA优化）</li>
                    <li>V: [4096 × 8 × 128] = 4MB（GQA优化）</li>
                    <li>注意力分数: [32 × 4096 × 4096] = 2.1GB（如不用Flash）</li>
                </ul>
            </div>
            
            <h3>💾 内存需求分析</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 500">
                    <text x="600" y="30" text-anchor="middle" font-weight="bold" font-size="18">不同模型的显存需求对比</text>
                    
                    <!-- 训练内存 -->
                    <g transform="translate(100, 80)">
                        <text x="250" y="0" text-anchor="middle" font-weight="bold">训练时显存需求（混合精度）</text>
                        
                        <!-- 7B模型 -->
                        <g transform="translate(0, 20)">
                            <rect x="0" y="0" width="500" height="40" fill="#e0e0e0" stroke="#333"/>
                            <rect x="0" y="0" width="140" height="40" fill="#667eea" opacity="0.7"/>
                            <text x="10" y="25" fill="white" font-weight="bold">7B模型</text>
                            <text x="510" y="25" font-size="12">~56GB（参数14GB + 梯度14GB + 优化器28GB）</text>
                        </g>
                        
                        <!-- 13B模型 -->
                        <g transform="translate(0, 70)">
                            <rect x="0" y="0" width="500" height="40" fill="#e0e0e0" stroke="#333"/>
                            <rect x="0" y="0" width="260" height="40" fill="#764ba2" opacity="0.7"/>
                            <text x="10" y="25" fill="white" font-weight="bold">13B模型</text>
                            <text x="510" y="25" font-size="12">~104GB（参数26GB + 梯度26GB + 优化器52GB）</text>
                        </g>
                        
                        <!-- 70B模型 -->
                        <g transform="translate(0, 120)">
                            <rect x="0" y="0" width="500" height="40" fill="#e0e0e0" stroke="#333"/>
                            <rect x="0" y="0" width="500" height="40" fill="#f093fb" opacity="0.7"/>
                            <text x="10" y="25" fill="white" font-weight="bold">70B模型</text>
                            <text x="510" y="25" font-size="12">~560GB（需要多卡并行）</text>
                        </g>
                        
                        <text x="250" y="200" text-anchor="middle" font-size="11" fill="#666">
                            规则：训练显存 ≈ 模型参数 × 8（混合精度）
                        </text>
                    </g>
                    
                    <!-- 推理内存 -->
                    <g transform="translate(100, 320)">
                        <text x="250" y="0" text-anchor="middle" font-weight="bold">推理时显存需求（FP16）</text>
                        
                        <!-- 基础模型内存 -->
                        <g transform="translate(0, 20)">
                            <rect x="0" y="0" width="200" height="30" fill="#00cc00" opacity="0.7"/>
                            <text x="100" y="20" text-anchor="middle" fill="white">模型权重</text>
                            <text x="210" y="20" font-size="12">7B: 14GB</text>
                        </g>
                        
                        <!-- KV Cache -->
                        <g transform="translate(0, 60)">
                            <rect x="0" y="0" width="300" height="30" fill="#ff6b6b" opacity="0.7"/>
                            <text x="150" y="20" text-anchor="middle" fill="white">KV Cache（seq=4096）</text>
                            <text x="310" y="20" font-size="12">标准: 16GB</text>
                        </g>
                        
                        <g transform="translate(0, 100)">
                            <rect x="0" y="0" width="75" height="30" fill="#ffc107" opacity="0.7"/>
                            <text x="37" y="20" text-anchor="middle">GQA</text>
                            <text x="85" y="20" font-size="12">4GB（节省75%）</text>
                        </g>
                        
                        <g transform="translate(0, 140)">
                            <rect x="0" y="0" width="37" height="30" fill="#4caf50" opacity="0.7"/>
                            <text x="18" y="20" text-anchor="middle" font-size="11">MQA</text>
                            <text x="47" y="20" font-size="12">2GB（节省87.5%）</text>
                        </g>
                    </g>
                    
                    <!-- GPU配置建议 -->
                    <g transform="translate(650, 80)">
                        <rect x="0" y="0" width="450" height="400" fill="#f8f9fa" stroke="#667eea" stroke-width="2" rx="10"/>
                        <text x="225" y="30" text-anchor="middle" font-weight="bold" fill="#667eea">GPU配置建议</text>
                        
                        <g transform="translate(20, 50)">
                            <text x="0" y="20" font-size="14" font-weight="bold">推理部署：</text>
                            <text x="20" y="45" font-size="12">• 7B模型：RTX 4090 (24GB)</text>
                            <text x="20" y="70" font-size="12">• 13B模型：A100 40GB 或 2×RTX 4090</text>
                            <text x="20" y="95" font-size="12">• 70B模型：4×A100 80GB</text>
                            
                            <text x="0" y="140" font-size="14" font-weight="bold">训练微调：</text>
                            <text x="20" y="165" font-size="12">• 7B模型：A100 80GB</text>
                            <text x="20" y="190" font-size="12">• 13B模型：2×A100 80GB</text>
                            <text x="20" y="215" font-size="12">• 70B模型：8×A100 80GB</text>
                            
                            <text x="0" y="260" font-size="14" font-weight="bold">优化技巧：</text>
                            <text x="20" y="285" font-size="12">• 使用GQA/MQA减少KV Cache</text>
                            <text x="20" y="310" font-size="12">• 量化（INT8/INT4）减少权重内存</text>
                            <text x="20" y="335" font-size="12">• Flash Attention减少激活内存</text>
                        </g>
                    </g>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 内存计算公式</h4>
                <pre>
<span class="code-comment"># 训练时内存需求（Adam优化器）</span>
training_memory = (
    model_params * 2 +     <span class="code-comment"># FP16模型权重</span>
    model_params * 2 +     <span class="code-comment"># FP16梯度</span>  
    model_params * 4 * 2   <span class="code-comment"># FP32 Adam状态（m和v）</span>
)

<span class="code-comment"># 推理时内存需求</span>
inference_memory = (
    model_params * 2 +     <span class="code-comment"># FP16模型权重</span>
    kv_cache_memory        <span class="code-comment"># KV缓存</span>
)

<span class="code-comment"># KV Cache计算</span>
<span class="code-comment"># 标准MHA:</span>
kv_cache = 2 * n_layers * seq_len * n_heads * d_head * batch_size * 2  <span class="code-comment"># FP16</span>

<span class="code-comment"># GQA (n_kv_heads < n_heads):</span>
kv_cache_gqa = 2 * n_layers * seq_len * n_kv_heads * d_head * batch_size * 2

<span class="code-comment"># MQA (n_kv_heads = 1):</span>
kv_cache_mqa = 2 * n_layers * seq_len * 1 * d_head * batch_size * 2

<span class="code-comment"># 示例：Llama 2-7B，seq_len=4096, batch=1</span>
<span class="code-comment"># 标准：2 * 32 * 4096 * 32 * 128 * 1 * 2 = 2GB</span>
<span class="code-comment"># GQA： 2 * 32 * 4096 * 8 * 128 * 1 * 2 = 512MB（节省75%）</span></pre>
            </div>
        </div>
        
        <!-- 1. 全注意力 -->
        <div class="attention-type" id="full-attention">
            <h2><span class="icon">1</span> 全注意力 (Full/Dense Attention)</h2>
            
            <h3>📐 数学原理</h3>
            <div class="math-section">
                <p><b>缩放点积注意力 (Scaled Dot-Product Attention):</b></p>
                <p>$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$</p>
                
                <div class="step-box">
                    <b>计算步骤分解：</b>
                    <ol style="margin-left: 20px; line-height: 2;">
                        <li><b>步骤1：</b> 计算注意力分数 $S = QK^T$ （Query与Key的相似度）</li>
                        <li><b>步骤2：</b> 缩放 $S_{scaled} = \frac{S}{\sqrt{d_k}}$ （防止梯度消失）</li>
                        <li><b>步骤3：</b> 归一化 $A = \text{softmax}(S_{scaled})$ （得到注意力权重）</li>
                        <li><b>步骤4：</b> 加权求和 $O = AV$ （根据权重聚合Value）</li>
                    </ol>
                </div>
                
                <p style="margin-top: 20px;">其中：</p>
                <ul style="margin-left: 20px;">
                    <li>$Q \in \mathbb{R}^{n \times d_k}$：查询矩阵 (Queries)</li>
                    <li>$K \in \mathbb{R}^{n \times d_k}$：键矩阵 (Keys)</li>
                    <li>$V \in \mathbb{R}^{n \times d_v}$：值矩阵 (Values)</li>
                    <li>$n$：序列长度，$d_k$：键/查询维度，$d_v$：值维度</li>
                </ul>
            </div>
            
            <h3>🎨 图形化解释 - 分步骤展示</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 600">
                    <!-- 标题 -->
                    <text x="600" y="30" text-anchor="middle" font-weight="bold" font-size="18">注意力计算详细步骤</text>
                    
                    <!-- 步骤1: Q × K^T -->
                    <g transform="translate(50, 60)">
                        <text x="100" y="0" text-anchor="middle" font-weight="bold" fill="#667eea">步骤1: 计算相似度</text>
                        
                        <!-- Q矩阵 -->
                        <rect x="0" y="20" width="60" height="100" fill="#667eea" opacity="0.7" rx="5"/>
                        <text x="30" y="75" text-anchor="middle" class="matrix-label" fill="white">Q</text>
                        <text x="30" y="135" text-anchor="middle" font-size="12">[n×d_k]</text>
                        
                        <!-- × 符号 -->
                        <text x="85" y="75" text-anchor="middle" font-size="20">×</text>
                        
                        <!-- K^T矩阵 (注意维度是转置后的) -->
                        <rect x="110" y="20" width="100" height="60" fill="#764ba2" opacity="0.7" rx="5"/>
                        <text x="160" y="55" text-anchor="middle" class="matrix-label" fill="white">K^T</text>
                        <text x="160" y="95" text-anchor="middle" font-size="12">[d_k×n]</text>
                        
                        <!-- = 符号 -->
                        <text x="235" y="75" text-anchor="middle" font-size="20">=</text>
                        
                        <!-- S矩阵 -->
                        <rect x="260" y="20" width="100" height="100" fill="#f093fb" opacity="0.7" rx="5"/>
                        <text x="310" y="75" text-anchor="middle" class="matrix-label" fill="white">S</text>
                        <text x="310" y="135" text-anchor="middle" font-size="12">[n×n]</text>
                        
                        <!-- 说明文字 -->
                        <text x="180" y="160" text-anchor="middle" font-size="11" fill="#666">
                            每个Query与所有Key的点积
                        </text>
                    </g>
                    
                    <!-- 步骤2: 缩放 -->
                    <g transform="translate(450, 60)">
                        <text x="100" y="0" text-anchor="middle" font-weight="bold" fill="#667eea">步骤2: 缩放</text>
                        
                        <!-- S矩阵 -->
                        <rect x="0" y="20" width="100" height="100" fill="#f093fb" opacity="0.7" rx="5"/>
                        <text x="50" y="75" text-anchor="middle" class="matrix-label" fill="white">S</text>
                        
                        <!-- ÷ 符号 -->
                        <text x="125" y="75" text-anchor="middle" font-size="20">÷</text>
                        
                        <!-- sqrt(d_k) -->
                        <rect x="150" y="55" width="50" height="30" fill="#ffc107" opacity="0.7" rx="5"/>
                        <text x="175" y="75" text-anchor="middle" font-size="12">√d_k</text>
                        
                        <!-- = 符号 -->
                        <text x="225" y="75" text-anchor="middle" font-size="20">=</text>
                        
                        <!-- S_scaled矩阵 -->
                        <rect x="250" y="20" width="100" height="100" fill="#ff6b6b" opacity="0.7" rx="5"/>
                        <text x="300" y="75" text-anchor="middle" class="matrix-label" fill="white">S'</text>
                        <text x="300" y="135" text-anchor="middle" font-size="12">[n×n]</text>
                        
                        <!-- 说明文字 -->
                        <text x="175" y="160" text-anchor="middle" font-size="11" fill="#666">
                            防止softmax梯度消失
                        </text>
                    </g>
                    
                    <!-- 步骤3: Softmax -->
                    <g transform="translate(850, 60)">
                        <text x="100" y="0" text-anchor="middle" font-weight="bold" fill="#667eea">步骤3: 归一化</text>
                        
                        <!-- S_scaled矩阵 -->
                        <rect x="0" y="20" width="100" height="100" fill="#ff6b6b" opacity="0.7" rx="5"/>
                        <text x="50" y="75" text-anchor="middle" class="matrix-label" fill="white">S'</text>
                        
                        <!-- softmax箭头 -->
                        <path d="M 110 70 L 140 70" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
                        <text x="125" y="60" text-anchor="middle" font-size="11">softmax</text>
                        <text x="125" y="90" text-anchor="middle" font-size="10" fill="#666">(按行)</text>
                        
                        <!-- A矩阵 -->
                        <rect x="150" y="20" width="100" height="100" fill="#f5576c" opacity="0.7" rx="5"/>
                        <text x="200" y="75" text-anchor="middle" class="matrix-label" fill="white">A</text>
                        <text x="200" y="135" text-anchor="middle" font-size="12">[n×n]</text>
                        
                        <!-- 说明文字 -->
                        <text x="125" y="160" text-anchor="middle" font-size="11" fill="#666">
                            每行和为1的权重矩阵
                        </text>
                    </g>
                    
                    <!-- 步骤4: A × V -->
                    <g transform="translate(250, 250)">
                        <text x="250" y="0" text-anchor="middle" font-weight="bold" fill="#667eea">步骤4: 加权聚合</text>
                        
                        <!-- A矩阵 -->
                        <rect x="0" y="20" width="100" height="100" fill="#f5576c" opacity="0.7" rx="5"/>
                        <text x="50" y="75" text-anchor="middle" class="matrix-label" fill="white">A</text>
                        <text x="50" y="135" text-anchor="middle" font-size="12">[n×n]</text>
                        
                        <!-- × 符号 -->
                        <text x="125" y="75" text-anchor="middle" font-size="20">×</text>
                        
                        <!-- V矩阵 -->
                        <rect x="150" y="20" width="60" height="100" fill="#4facfe" opacity="0.7" rx="5"/>
                        <text x="180" y="75" text-anchor="middle" class="matrix-label" fill="white">V</text>
                        <text x="180" y="135" text-anchor="middle" font-size="12">[n×d_v]</text>
                        
                        <!-- = 符号 -->
                        <text x="235" y="75" text-anchor="middle" font-size="20">=</text>
                        
                        <!-- Output矩阵 -->
                        <rect x="260" y="20" width="60" height="100" fill="#00f2fe" opacity="0.7" rx="5"/>
                        <text x="290" y="75" text-anchor="middle" class="matrix-label" fill="white">O</text>
                        <text x="290" y="135" text-anchor="middle" font-size="12">[n×d_v]</text>
                        
                        <!-- 说明文字 -->
                        <text x="160" y="160" text-anchor="middle" font-size="11" fill="#666">
                            根据注意力权重组合Value
                        </text>
                    </g>
                    
                    <!-- 连接线展示流程 -->
                    <g stroke="#667eea" stroke-width="2" fill="none" stroke-dasharray="5,5">
                        <path d="M 360 130 Q 400 180, 350 250"/>
                        <path d="M 800 130 Q 850 180, 350 270"/>
                    </g>
                    
                    <!-- 维度示例框 -->
                    <g transform="translate(650, 350)">
                        <rect x="0" y="0" width="400" height="180" fill="#f8f9fa" stroke="#667eea" stroke-width="2" rx="10"/>
                        <text x="200" y="25" text-anchor="middle" font-weight="bold" fill="#667eea">维度示例（n=4, d_k=64, d_v=64）</text>
                        
                        <text x="20" y="55" font-size="13">Q: [4×64] × K^T: [64×4] = S: [4×4]</text>
                        <text x="20" y="80" font-size="13">S: [4×4] ÷ √64 = S': [4×4]</text>
                        <text x="20" y="105" font-size="13">softmax(S'): [4×4] = A: [4×4]</text>
                        <text x="20" y="130" font-size="13">A: [4×4] × V: [4×64] = O: [4×64]</text>
                        
                        <text x="20" y="160" font-size="11" fill="#666">💡 输出维度与输入Q相同，但内容是V的加权组合</text>
                    </g>
                    
                    <!-- 箭头定义 -->
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" fill="#333"/>
                        </marker>
                    </defs>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 代码实现与步骤对应</h4>
                <pre>
<span class="code-comment"># 步骤1: 计算注意力分数（Query与Key的相似度）</span>
scores = torch.matmul(Q, K.transpose(-2, -1))  <span class="code-comment"># [batch, heads, n, d_k] × [batch, heads, d_k, n] = [batch, heads, n, n]</span>

<span class="code-comment"># 步骤2: 缩放（防止softmax的梯度消失）</span>
scores = scores / math.sqrt(d_k)  <span class="code-comment"># 除以√d_k，使方差稳定在1附近</span>

<span class="code-comment"># 步骤3: 应用softmax得到注意力权重（每行归一化）</span>
attn_weights = F.softmax(scores, dim=-1)  <span class="code-comment"># [batch, heads, n, n]，每行和为1</span>

<span class="code-comment"># 步骤4: 使用注意力权重对Value进行加权求和</span>
output = torch.matmul(attn_weights, V)  <span class="code-comment"># [batch, heads, n, n] × [batch, heads, n, d_v] = [batch, heads, n, d_v]</span>

<span class="code-comment"># 关键点解释：</span>
<span class="code-comment"># - Q的每一行代表一个位置的查询向量</span>
<span class="code-comment"># - K的每一行代表一个位置的键向量</span>
<span class="code-comment"># - scores[i,j]表示位置i对位置j的注意力分数</span>
<span class="code-comment"># - softmax使得每个位置的注意力权重和为1</span>
<span class="code-comment"># - 最终输出是V的加权组合，权重由Q和K的相似度决定</span></pre>
            </div>
            
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>✅ 优势</h4>
                    <p>完整的上下文建模能力</p>
                </div>
                <div class="feature-card">
                    <h4>⚠️ 劣势</h4>
                    <p>O(n²)复杂度，长序列计算昂贵</p>
                </div>
                <div class="feature-card">
                    <h4>💡 应用</h4>
                    <p>GPT-3, BERT, 标准Transformer</p>
                </div>
            </div>
            
            <h3 style="margin-top: 40px;">🏆 主流模型采用情况</h3>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin-top: 20px;">
                <div style="background: #e8f5e9; padding: 15px; border-radius: 10px; border-left: 4px solid #4caf50;">
                    <h4 style="color: #2e7d32; margin-bottom: 10px;">GPT系列</h4>
                    <ul style="margin-left: 20px; line-height: 1.8;">
                        <li>GPT-3: Full Attention</li>
                        <li>GPT-4: Flash Attention + Sparse</li>
                    </ul>
                </div>
                
                <div style="background: #e3f2fd; padding: 15px; border-radius: 10px; border-left: 4px solid #2196f3;">
                    <h4 style="color: #1565c0; margin-bottom: 10px;">Llama系列</h4>
                    <ul style="margin-left: 20px; line-height: 1.8;">
                        <li>Llama 1: Full Attention</li>
                        <li>Llama 2/3: GQA + RoPE</li>
                    </ul>
                </div>
                
                <div style="background: #fce4ec; padding: 15px; border-radius: 10px; border-left: 4px solid #e91e63;">
                    <h4 style="color: #880e4f; margin-bottom: 10px;">Google系列</h4>
                    <ul style="margin-left: 20px; line-height: 1.8;">
                        <li>PaLM: MQA</li>
                        <li>Gemini: Flash + GQA混合</li>
                    </ul>
                </div>
                
                <div style="background: #f3e5f5; padding: 15px; border-radius: 10px; border-left: 4px solid #9c27b0;">
                    <h4 style="color: #6a1b9a; margin-bottom: 10px;">其他模型</h4>
                    <ul style="margin-left: 20px; line-height: 1.8;">
                        <li>Mistral/Mixtral: GQA (4头)</li>
                        <li>Falcon: MQA</li>
                        <li>Claude: Flash Attention</li>
                    </ul>
                </div>
            </div>
        </div>
        
        <!-- 2. 稀疏注意力 -->
        <div class="attention-type" id="sparse-attention">
            <h2><span class="icon">2</span> 稀疏注意力 (Sparse Attention)</h2>
            
            <h3>📐 数学原理</h3>
            <div class="math-section">
                <p><b>局部窗口注意力 (Local/Sliding Window):</b></p>
                <p>$\text{LocalAttn}(Q, K, V)_i = \text{softmax}\left(\frac{Q_i K_{[i-w:i+w]}^T}{\sqrt{d_k}}\right)V_{[i-w:i+w]}$</p>
                <p>其中 $w$ 是窗口半径，每个位置只关注邻近的 $2w+1$ 个位置</p>
                
                <p style="margin-top: 20px;"><b>块稀疏注意力 (Block Sparse):</b></p>
                <p>将序列分成大小为 $b$ 的块：</p>
                <p>$\text{BlockAttn}(Q^{(i)}, K^{(i)}, V^{(i)}) = \text{softmax}\left(\frac{Q^{(i)}(K^{(i)})^T}{\sqrt{d_k}}\right)V^{(i)}$</p>
            </div>
            
            <h3>🎨 图形化解释</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 500">
                    <!-- 局部注意力模式 -->
                    <g transform="translate(100, 50)">
                        <text x="200" y="0" text-anchor="middle" font-weight="bold">局部窗口注意力</text>
                        
                        <!-- 注意力矩阵可视化 -->
                        <rect x="0" y="20" width="400" height="400" fill="none" stroke="#333" stroke-width="2"/>
                        
                        <!-- 窗口模式（对角线） -->
                        <g opacity="0.7">
                            <!-- 窗口块 -->
                            <rect x="0" y="20" width="100" height="100" fill="#667eea"/>
                            <rect x="50" y="70" width="100" height="100" fill="#667eea"/>
                            <rect x="100" y="120" width="100" height="100" fill="#667eea"/>
                            <rect x="150" y="170" width="100" height="100" fill="#667eea"/>
                            <rect x="200" y="220" width="100" height="100" fill="#667eea"/>
                            <rect x="250" y="270" width="100" height="100" fill="#667eea"/>
                            <rect x="300" y="320" width="100" height="100" fill="#667eea"/>
                        </g>
                        
                        <!-- 维度标注 -->
                        <text x="-30" y="220" text-anchor="middle" font-size="12">位置i</text>
                        <text x="200" y="450" text-anchor="middle" font-size="12">位置j</text>
                        
                        <!-- 示例说明 -->
                        <g transform="translate(0, 460)">
                            <rect x="0" y="0" width="15" height="15" fill="#667eea" opacity="0.7"/>
                            <text x="25" y="12" font-size="12">有注意力连接</text>
                            <rect x="150" y="0" width="15" height="15" fill="none" stroke="#333"/>
                            <text x="175" y="12" font-size="12">无注意力连接</text>
                        </g>
                    </g>
                    
                    <!-- 块稀疏注意力模式 -->
                    <g transform="translate(650, 50)">
                        <text x="200" y="0" text-anchor="middle" font-weight="bold">块稀疏注意力</text>
                        
                        <!-- 注意力矩阵 -->
                        <rect x="0" y="20" width="400" height="400" fill="none" stroke="#333" stroke-width="2"/>
                        
                        <!-- 块模式 -->
                        <g opacity="0.7">
                            <rect x="0" y="20" width="100" height="100" fill="#764ba2"/>
                            <rect x="100" y="120" width="100" height="100" fill="#764ba2"/>
                            <rect x="200" y="220" width="100" height="100" fill="#764ba2"/>
                            <rect x="300" y="320" width="100" height="100" fill="#764ba2"/>
                        </g>
                        
                        <!-- 块大小标注 -->
                        <path d="M 0 20 L 0 120" stroke="#333" stroke-width="1"/>
                        <text x="-30" y="75" text-anchor="middle" font-size="11">块1</text>
                        <path d="M 100 120 L 100 220" stroke="#333" stroke-width="1"/>
                        <text x="70" y="175" text-anchor="middle" font-size="11">块2</text>
                    </g>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 代码实现</h4>
                <pre>
<span class="code-comment"># 局部窗口注意力：每个位置只关注窗口内的位置</span>
def create_local_mask(seq_len, window_size):
    mask = torch.zeros(seq_len, seq_len)
    for i in range(seq_len):
        start = max(0, i - window_size // 2)
        end = min(seq_len, i + window_size // 2 + 1)
        mask[i, start:end] = 1  <span class="code-comment"># 位置i只关注[start:end]范围</span>
    return mask

<span class="code-comment"># 块稀疏注意力：将序列分块，块内全连接</span>
Q = Q.view(batch, heads, n_blocks, block_size, d_k)  <span class="code-comment"># 重塑为块</span>
K = K.view(batch, heads, n_blocks, block_size, d_k)
scores = torch.matmul(Q, K.transpose(-2, -1))  <span class="code-comment"># 块内计算注意力</span>

<span class="code-comment"># 复杂度分析：</span>
<span class="code-comment"># - 局部注意力: O(n × window_size × d) 而非 O(n² × d)</span>
<span class="code-comment"># - 块稀疏: O(n × block_size × d) 而非 O(n² × d)</span></pre>
            </div>
            
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>✅ 优势</h4>
                    <p>线性复杂度，长序列高效</p>
                </div>
                <div class="feature-card">
                    <h4>⚠️ 劣势</h4>
                    <p>可能丢失长距离依赖</p>
                </div>
                <div class="feature-card">
                    <h4>💡 应用</h4>
                    <p>Longformer, BigBird, Sparse Transformer</p>
                </div>
            </div>
        </div>
        
        <!-- 3. 线性注意力 -->
        <div class="attention-type" id="linear-attention">
            <h2><span class="icon">3</span> 线性注意力 (Linear Attention)</h2>
            
            <h3>📐 数学原理</h3>
            <div class="math-section">
                <p><b>核技巧 (Kernel Trick):</b></p>
                <p>标准注意力：$O = \text{softmax}(QK^T)V = \frac{\exp(QK^T)}{\text{rowsum}(\exp(QK^T))}V$</p>
                <p>线性注意力（改变计算顺序）：$O = \frac{\phi(Q)(\phi(K)^TV)}{\phi(Q)\text{sum}(\phi(K))}$</p>
                
                <div class="step-box">
                    <b>关键优化：改变矩阵乘法顺序</b>
                    <ul style="margin-left: 20px;">
                        <li>标准方式：$(QK^T)V$ - 先算$QK^T$得到$n×n$矩阵，复杂度$O(n^2d)$</li>
                        <li>线性方式：$Q(K^TV)$ - 先算$K^TV$得到$d×d$矩阵，复杂度$O(nd^2)$</li>
                        <li>当$d \ll n$时，显著降低计算复杂度</li>
                    </ul>
                </div>
            </div>
            
            <h3>🎨 图形化解释</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 600">
                    <!-- 标准注意力计算顺序 -->
                    <g transform="translate(100, 50)">
                        <text x="200" y="0" text-anchor="middle" font-weight="bold" fill="#667eea">标准注意力 O(n²d)</text>
                        
                        <!-- 第一步：Q × K^T -->
                        <text x="50" y="40" font-size="12" fill="#666">第1步:</text>
                        <rect x="0" y="50" width="60" height="100" fill="#667eea" opacity="0.7"/>
                        <text x="30" y="105" text-anchor="middle" class="matrix-label" fill="white">Q</text>
                        <text x="30" y="165" text-anchor="middle" font-size="11">[n×d]</text>
                        
                        <text x="75" y="100" text-anchor="middle">×</text>
                        
                        <rect x="90" y="70" width="100" height="60" fill="#764ba2" opacity="0.7"/>
                        <text x="140" y="105" text-anchor="middle" class="matrix-label" fill="white">K^T</text>
                        <text x="140" y="145" text-anchor="middle" font-size="11">[d×n]</text>
                        
                        <text x="205" y="100" text-anchor="middle">=</text>
                        
                        <rect x="220" y="50" width="100" height="100" fill="#f093fb" opacity="0.7"/>
                        <text x="270" y="105" text-anchor="middle" class="matrix-label" fill="white">Attn</text>
                        <text x="270" y="165" text-anchor="middle" font-size="11">[n×n]</text>
                        <text x="270" y="180" text-anchor="middle" font-size="11" fill="red">大矩阵!</text>
                        
                        <!-- 第二步：Attn × V -->
                        <text x="50" y="220" font-size="12" fill="#666">第2步:</text>
                        <rect x="0" y="230" width="100" height="100" fill="#f093fb" opacity="0.7"/>
                        <text x="50" y="285" text-anchor="middle" class="matrix-label" fill="white">Attn</text>
                        
                        <text x="115" y="280" text-anchor="middle">×</text>
                        
                        <rect x="130" y="230" width="60" height="100" fill="#4facfe" opacity="0.7"/>
                        <text x="160" y="285" text-anchor="middle" class="matrix-label" fill="white">V</text>
                        <text x="160" y="345" text-anchor="middle" font-size="11">[n×d]</text>
                        
                        <text x="205" y="280" text-anchor="middle">=</text>
                        
                        <rect x="220" y="230" width="60" height="100" fill="#00f2fe" opacity="0.7"/>
                        <text x="250" y="285" text-anchor="middle" class="matrix-label" fill="white">O</text>
                        <text x="250" y="345" text-anchor="middle" font-size="11">[n×d]</text>
                        
                        <!-- 复杂度标注 -->
                        <rect x="0" y="380" width="320" height="40" fill="#ffe0e0" stroke="#ff0000" rx="5"/>
                        <text x="160" y="405" text-anchor="middle" font-size="12" fill="#ff0000">
                            总复杂度: O(n²d) - 需要存储n×n矩阵
                        </text>
                    </g>
                    
                    <!-- 线性注意力计算顺序 -->
                    <g transform="translate(650, 50)">
                        <text x="200" y="0" text-anchor="middle" font-weight="bold" fill="#667eea">线性注意力 O(nd²)</text>
                        
                        <!-- 第一步：K^T × V -->
                        <text x="50" y="40" font-size="12" fill="#666">第1步:</text>
                        <rect x="0" y="70" width="100" height="60" fill="#764ba2" opacity="0.7"/>
                        <text x="50" y="105" text-anchor="middle" class="matrix-label" fill="white">K^T</text>
                        <text x="50" y="145" text-anchor="middle" font-size="11">[d×n]</text>
                        
                        <text x="115" y="100" text-anchor="middle">×</text>
                        
                        <rect x="130" y="50" width="60" height="100" fill="#4facfe" opacity="0.7"/>
                        <text x="160" y="105" text-anchor="middle" class="matrix-label" fill="white">V</text>
                        <text x="160" y="165" text-anchor="middle" font-size="11">[n×d]</text>
                        
                        <text x="205" y="100" text-anchor="middle">=</text>
                        
                        <rect x="220" y="70" width="60" height="60" fill="#ffc107" opacity="0.7"/>
                        <text x="250" y="105" text-anchor="middle" class="matrix-label">KV</text>
                        <text x="250" y="145" text-anchor="middle" font-size="11">[d×d]</text>
                        <text x="250" y="160" text-anchor="middle" font-size="11" fill="green">小矩阵!</text>
                        
                        <!-- 第二步：Q × KV -->
                        <text x="50" y="220" font-size="12" fill="#666">第2步:</text>
                        <rect x="0" y="230" width="60" height="100" fill="#667eea" opacity="0.7"/>
                        <text x="30" y="285" text-anchor="middle" class="matrix-label" fill="white">Q</text>
                        <text x="30" y="345" text-anchor="middle" font-size="11">[n×d]</text>
                        
                        <text x="75" y="280" text-anchor="middle">×</text>
                        
                        <rect x="90" y="250" width="60" height="60" fill="#ffc107" opacity="0.7"/>
                        <text x="120" y="285" text-anchor="middle" class="matrix-label">KV</text>
                        <text x="120" y="325" text-anchor="middle" font-size="11">[d×d]</text>
                        
                        <text x="165" y="280" text-anchor="middle">=</text>
                        
                        <rect x="180" y="230" width="60" height="100" fill="#00f2fe" opacity="0.7"/>
                        <text x="210" y="285" text-anchor="middle" class="matrix-label" fill="white">O</text>
                        <text x="210" y="345" text-anchor="middle" font-size="11">[n×d]</text>
                        
                        <!-- 复杂度标注 -->
                        <rect x="0" y="380" width="280" height="40" fill="#e0ffe0" stroke="#00cc00" rx="5"/>
                        <text x="140" y="405" text-anchor="middle" font-size="12" fill="#00cc00">
                            总复杂度: O(nd²) - 只需d×d矩阵
                        </text>
                    </g>
                    
                    <!-- 对比说明 -->
                    <g transform="translate(200, 480)">
                        <rect x="0" y="0" width="800" height="80" fill="#f8f9fa" stroke="#667eea" stroke-width="2" rx="10"/>
                        <text x="400" y="25" text-anchor="middle" font-weight="bold" fill="#667eea">效率对比（假设n=2048, d=64）</text>
                        <text x="200" y="50" font-size="12">标准注意力: 2048² × 64 = 268M 操作</text>
                        <text x="200" y="70" font-size="12">线性注意力: 2048 × 64² = 8.4M 操作</text>
                        <text x="600" y="60" font-size="14" fill="#00cc00" font-weight="bold">速度提升 32倍！</text>
                    </g>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 代码实现</h4>
                <pre>
<span class="code-comment"># 线性注意力核心：改变计算顺序</span>

<span class="code-comment"># 标准注意力（低效）</span>
attn = torch.matmul(Q, K.transpose(-2, -1))  <span class="code-comment"># [n, d] × [d, n] = [n, n] 大矩阵！</span>
output = torch.matmul(attn, V)               <span class="code-comment"># [n, n] × [n, d] = [n, d]</span>

<span class="code-comment"># 线性注意力（高效）</span>
<span class="code-comment"># 步骤1: 先计算K^T V，得到小矩阵</span>
KV = torch.matmul(K.transpose(-2, -1), V)    <span class="code-comment"># [d, n] × [n, d] = [d, d] 小矩阵！</span>

<span class="code-comment"># 步骤2: 计算归一化因子</span>
Z = 1 / (torch.einsum('nd,d->n', Q, K.sum(dim=0)) + eps)  <span class="code-comment"># [n]</span>

<span class="code-comment"># 步骤3: Q与KV相乘并归一化</span>
output = torch.matmul(Q, KV) * Z.unsqueeze(-1)  <span class="code-comment"># [n, d] × [d, d] = [n, d]</span>

<span class="code-comment"># 关键点：</span>
<span class="code-comment"># - 避免了n×n的注意力矩阵</span>
<span class="code-comment"># - 当d << n时，效率提升巨大</span>
<span class="code-comment"># - 需要特征映射φ来保证非负性</span></pre>
            </div>
            
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>✅ 优势</h4>
                    <p>O(n)复杂度，适合超长序列(>32K)</p>
                </div>
                <div class="feature-card">
                    <h4>⚠️ 劣势</h4>
                    <p>可能损失表达能力，精度略低</p>
                </div>
                <div class="feature-card">
                    <h4>💡 应用</h4>
                    <p>Performer, Linformer, Linear Transformer, Nyström</p>
                </div>
            </div>
        </div>
        
        <!-- 4. GQA -->
        <div class="attention-type" id="gqa">
            <h2><span class="icon">4</span> 分组查询注意力 (GQA)</h2>
            
            <h3>📐 数学原理</h3>
            <div class="math-section">
                <p><b>核心思想：</b>将查询头分成G组，每组共享键值对</p>
                <p>$$\text{GQA}(Q^{(g)}, K^{(g/G)}, V^{(g/G)}) = \text{softmax}\left(\frac{Q^{(g)}(K^{(g/G)})^T}{\sqrt{d_k}}\right)V^{(g/G)}$$</p>
                
                <div class="step-box">
                    <b>内存节省计算：</b>
                    <ul style="margin-left: 20px;">
                        <li>MHA KV缓存: $2 × n_{heads} × seq\_len × d_k$</li>
                        <li>GQA KV缓存: $2 × n_{kv\_heads} × seq\_len × d_k$</li>
                        <li>节省比例: $1 - \frac{n_{kv\_heads}}{n_{heads}}$</li>
                    </ul>
                </div>
            </div>
            
            <h3>🎨 图形化解释</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 400">
                    <text x="600" y="30" text-anchor="middle" font-weight="bold" font-size="16">GQA: 8个Q头，2个KV头（4:1分组）</text>
                    
                    <!-- Query heads -->
                    <g transform="translate(100, 80)">
                        <text x="100" y="-10" text-anchor="middle" font-weight="bold">Query Heads (8个)</text>
                        
                        <!-- 第一组Q头 -->
                        <g fill="#667eea" opacity="0.8">
                            <rect x="0" y="0" width="40" height="50" stroke="#333"/>
                            <text x="20" y="30" text-anchor="middle" font-size="12" fill="white">Q₁</text>
                            <rect x="45" y="0" width="40" height="50" stroke="#333"/>
                            <text x="65" y="30" text-anchor="middle" font-size="12" fill="white">Q₂</text>
                            <rect x="90" y="0" width="40" height="50" stroke="#333"/>
                            <text x="110" y="30" text-anchor="middle" font-size="12" fill="white">Q₃</text>
                            <rect x="135" y="0" width="40" height="50" stroke="#333"/>
                            <text x="155" y="30" text-anchor="middle" font-size="12" fill="white">Q₄</text>
                        </g>
                        
                        <!-- 第二组Q头 -->
                        <g fill="#667eea" opacity="0.6">
                            <rect x="0" y="60" width="40" height="50" stroke="#333"/>
                            <text x="20" y="90" text-anchor="middle" font-size="12" fill="white">Q₅</text>
                            <rect x="45" y="60" width="40" height="50" stroke="#333"/>
                            <text x="65" y="90" text-anchor="middle" font-size="12" fill="white">Q₆</text>
                            <rect x="90" y="60" width="40" height="50" stroke="#333"/>
                            <text x="110" y="90" text-anchor="middle" font-size="12" fill="white">Q₇</text>
                            <rect x="135" y="60" width="40" height="50" stroke="#333"/>
                            <text x="155" y="90" text-anchor="middle" font-size="12" fill="white">Q₈</text>
                        </g>
                    </g>
                    
                    <!-- 连接线 -->
                    <g stroke="#333" stroke-width="1.5" fill="none">
                        <!-- 第一组连接 -->
                        <path d="M 275 105 Q 400 105, 500 130" stroke="#667eea"/>
                        <path d="M 275 105 Q 400 110, 500 130" stroke="#667eea"/>
                        <path d="M 275 105 Q 400 115, 500 130" stroke="#667eea"/>
                        <path d="M 275 105 Q 400 120, 500 130" stroke="#667eea"/>
                        
                        <!-- 第二组连接 -->
                        <path d="M 275 165 Q 400 165, 500 210" stroke="#667eea" opacity="0.6"/>
                        <path d="M 275 165 Q 400 170, 500 210" stroke="#667eea" opacity="0.6"/>
                        <path d="M 275 165 Q 400 175, 500 210" stroke="#667eea" opacity="0.6"/>
                        <path d="M 275 165 Q 400 180, 500 210" stroke="#667eea" opacity="0.6"/>
                    </g>
                    
                    <!-- KV heads -->
                    <g transform="translate(500, 100)">
                        <text x="50" y="-10" text-anchor="middle" font-weight="bold">KV Pairs (2个)</text>
                        <rect x="0" y="10" width="100" height="60" fill="#764ba2" opacity="0.8" stroke="#333"/>
                        <text x="50" y="45" text-anchor="middle" font-size="14" fill="white">K₁,V₁</text>
                        
                        <rect x="0" y="90" width="100" height="60" fill="#764ba2" opacity="0.6" stroke="#333"/>
                        <text x="50" y="125" text-anchor="middle" font-size="14" fill="white">K₂,V₂</text>
                    </g>
                    
                    <!-- 内存对比 -->
                    <g transform="translate(700, 100)">
                        <text x="200" y="-10" text-anchor="middle" font-weight="bold">KV缓存内存对比</text>
                        
                        <!-- MHA -->
                        <rect x="0" y="10" width="400" height="30" fill="#e0e0e0" stroke="#333"/>
                        <rect x="0" y="10" width="400" height="30" fill="#667eea" opacity="0.5"/>
                        <text x="200" y="30" text-anchor="middle" font-size="12">MHA: 8个KV头 (100%)</text>
                        
                        <!-- GQA -->
                        <rect x="0" y="60" width="400" height="30" fill="#e0e0e0" stroke="#333"/>
                        <rect x="0" y="60" width="100" height="30" fill="#764ba2" opacity="0.8"/>
                        <text x="200" y="80" text-anchor="middle" font-size="12">GQA: 2个KV头 (25%)</text>
                        
                        <text x="200" y="120" text-anchor="middle" font-size="14" fill="green" font-weight="bold">
                            节省75%内存！
                        </text>
                        
                        <!-- 计算示例 -->
                        <rect x="0" y="150" width="400" height="100" fill="#f8f9fa" stroke="#667eea" rx="5"/>
                        <text x="10" y="170" font-size="11">示例（seq_len=2048, d_k=64）：</text>
                        <text x="10" y="190" font-size="11">MHA: 2 × 8 × 2048 × 64 × 4B = 8MB</text>
                        <text x="10" y="210" font-size="11">GQA: 2 × 2 × 2048 × 64 × 4B = 2MB</text>
                        <text x="10" y="230" font-size="11" fill="green">每层节省6MB，12层模型节省72MB！</text>
                    </g>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 代码实现</h4>
                <pre>
<span class="code-comment"># GQA核心实现</span>
class GroupedQueryAttention(nn.Module):
    def __init__(self, d_model, n_heads=8, n_kv_heads=2):
        <span class="code-comment"># n_heads: Query头数量（如8）</span>
        <span class="code-comment"># n_kv_heads: KV头数量（如2）</span>
        <span class="code-comment"># n_groups: 每个KV头服务的Q头数（8/2=4）</span>
        self.n_groups = n_heads // n_kv_heads
        
        <span class="code-comment"># Q使用全部维度，KV使用更少维度</span>
        self.W_q = nn.Linear(d_model, d_model)              <span class="code-comment"># 8个头</span>
        self.W_k = nn.Linear(d_model, n_kv_heads * d_k)     <span class="code-comment"># 2个头</span>
        self.W_v = nn.Linear(d_model, n_kv_heads * d_k)     <span class="code-comment"># 2个头</span>
    
    def forward(self, x):
        <span class="code-comment"># 计算Q, K, V</span>
        Q = self.W_q(x).view(batch, seq, n_heads, d_k)      <span class="code-comment"># [B, N, 8, d]</span>
        K = self.W_k(x).view(batch, seq, n_kv_heads, d_k)   <span class="code-comment"># [B, N, 2, d]</span>
        V = self.W_v(x).view(batch, seq, n_kv_heads, d_k)   <span class="code-comment"># [B, N, 2, d]</span>
        
        <span class="code-comment"># 关键步骤：重复KV以匹配Q的头数</span>
        K = K.repeat_interleave(self.n_groups, dim=2)       <span class="code-comment"># [B, N, 8, d]</span>
        V = V.repeat_interleave(self.n_groups, dim=2)       <span class="code-comment"># [B, N, 8, d]</span>
        
        <span class="code-comment"># 标准注意力计算</span>
        attn = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(d_k)
        attn = F.softmax(attn, dim=-1)
        output = torch.matmul(attn, V)</pre>
            </div>
            
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>✅ 优势</h4>
                    <p>减少KV缓存50-75%，保持模型质量</p>
                </div>
                <div class="feature-card">
                    <h4>⚠️ 劣势</h4>
                    <p>需要调整KV头数量的超参数</p>
                </div>
                <div class="feature-card">
                    <h4>💡 应用</h4>
                    <p>Llama 2, Llama 3, Mistral, Mixtral</p>
                </div>
            </div>
        </div>
        
        <!-- 5. MQA -->
        <div class="attention-type" id="mqa">
            <h2><span class="icon">5</span> 多查询注意力 (MQA)</h2>
            
            <h3>📐 数学原理</h3>
            <div class="math-section">
                <p><b>核心思想：</b>所有查询头共享单一的键值对</p>
                <p>$$\text{MQA}(Q^{(h)}, K, V) = \text{softmax}\left(\frac{Q^{(h)}K^T}{\sqrt{d_k}}\right)V$$</p>
                <p>KV缓存从 $O(n \cdot h \cdot d)$ 降到 $O(n \cdot d)$</p>
            </div>
            
            <h3>🎨 图形化解释</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 350">
                    <text x="600" y="30" text-anchor="middle" font-weight="bold" font-size="16">MQA: 所有查询头共享单个KV对</text>
                    
                    <!-- Query heads -->
                    <g transform="translate(150, 100)">
                        <text x="100" y="-10" text-anchor="middle" font-weight="bold">8个Query Heads</text>
                        <rect x="0" y="0" width="200" height="40" fill="#667eea" opacity="0.8" stroke="#333"/>
                        <text x="100" y="25" text-anchor="middle" fill="white">Q₁ Q₂ Q₃ Q₄ Q₅ Q₆ Q₇ Q₈</text>
                        
                        <!-- 表示8个独立的头 -->
                        <g transform="translate(0, 50)">
                            <circle cx="25" cy="10" r="5" fill="#667eea"/>
                            <circle cx="50" cy="10" r="5" fill="#667eea"/>
                            <circle cx="75" cy="10" r="5" fill="#667eea"/>
                            <circle cx="100" cy="10" r="5" fill="#667eea"/>
                            <circle cx="125" cy="10" r="5" fill="#667eea"/>
                            <circle cx="150" cy="10" r="5" fill="#667eea"/>
                            <circle cx="175" cy="10" r="5" fill="#667eea"/>
                            <circle cx="200" cy="10" r="5" fill="#667eea"/>
                        </g>
                    </g>
                    
                    <!-- 连接线（所有Q头连到同一个KV） -->
                    <g stroke="#333" stroke-width="1.5" fill="none">
                        <path d="M 350 120 Q 450 120, 550 150"/>
                        <path d="M 350 125 Q 450 130, 550 150"/>
                        <path d="M 350 130 Q 450 140, 550 150"/>
                        <path d="M 350 135 Q 450 150, 550 150"/>
                    </g>
                    
                    <!-- Single KV -->
                    <g transform="translate(550, 120)">
                        <text x="60" y="-10" text-anchor="middle" font-weight="bold">单个KV对</text>
                        <rect x="0" y="10" width="120" height="60" fill="#764ba2" opacity="0.8" stroke="#333" stroke-width="2"/>
                        <text x="60" y="45" text-anchor="middle" font-size="16" fill="white">K, V</text>
                        <text x="60" y="85" text-anchor="middle" font-size="11">共享给所有Q头</text>
                    </g>
                    
                    <!-- 内存节省可视化 -->
                    <g transform="translate(750, 100)">
                        <text x="150" y="-10" text-anchor="middle" font-weight="bold">KV缓存对比</text>
                        
                        <!-- MHA -->
                        <rect x="0" y="10" width="300" height="30" fill="#667eea" opacity="0.5" stroke="#333"/>
                        <text x="310" y="30" font-size="12">MHA: 8×n×d</text>
                        
                        <!-- MQA -->
                        <rect x="0" y="60" width="37.5" height="30" fill="#764ba2" opacity="0.8" stroke="#333"/>
                        <text x="310" y="80" font-size="12">MQA: 1×n×d</text>
                        
                        <text x="150" y="120" text-anchor="middle" font-size="14" fill="green" font-weight="bold">
                            节省87.5%内存！
                        </text>
                        
                        <!-- 极限优化说明 -->
                        <rect x="0" y="150" width="300" height="80" fill="#fff3cd" stroke="#ffc107" rx="5"/>
                        <text x="10" y="170" font-size="11">💡 极限内存优化：</text>
                        <text x="10" y="190" font-size="11">• 推理时KV缓存最小化</text>
                        <text x="10" y="210" font-size="11">• 适合边缘设备部署</text>
                    </g>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 代码实现</h4>
                <pre>
<span class="code-comment"># MQA核心实现</span>
class MultiQueryAttention(nn.Module):
    def __init__(self, d_model, n_heads=8):
        <span class="code-comment"># Q有多个头，K和V只有单头</span>
        self.W_q = nn.Linear(d_model, d_model)     <span class="code-comment"># 输出8个头的Q</span>
        self.W_k = nn.Linear(d_model, d_k)         <span class="code-comment"># 输出1个头的K</span>
        self.W_v = nn.Linear(d_model, d_k)         <span class="code-comment"># 输出1个头的V</span>
    
    def forward(self, x):
        <span class="code-comment"># 计算Q（多头）和KV（单头）</span>
        Q = self.W_q(x).view(batch, seq, n_heads, d_k)  <span class="code-comment"># [B, N, 8, d]</span>
        K = self.W_k(x).view(batch, seq, 1, d_k)        <span class="code-comment"># [B, N, 1, d]</span>
        V = self.W_v(x).view(batch, seq, 1, d_k)        <span class="code-comment"># [B, N, 1, d]</span>
        
        <span class="code-comment"># 关键：扩展KV到所有头（广播）</span>
        K = K.expand(-1, -1, n_heads, -1)               <span class="code-comment"># [B, N, 8, d]</span>
        V = V.expand(-1, -1, n_heads, -1)               <span class="code-comment"># [B, N, 8, d]</span>
        
        <span class="code-comment"># 标准注意力计算</span>
        attn = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(d_k)
        
<span class="code-comment"># 内存分析（seq_len=2048, d=64, 8头）：</span>
<span class="code-comment"># MHA KV缓存: 2 × 8 × 2048 × 64 × 4B = 8MB</span>
<span class="code-comment"># MQA KV缓存: 2 × 1 × 2048 × 64 × 4B = 1MB</span>
<span class="code-comment"># 节省: 87.5%！</span></pre>
            </div>
            
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>✅ 优势</h4>
                    <p>最小KV缓存(节省87.5%)，推理最快</p>
                </div>
                <div class="feature-card">
                    <h4>⚠️ 劣势</h4>
                    <p>可能降低模型性能，训练不稳定</p>
                </div>
                <div class="feature-card">
                    <h4>💡 应用</h4>
                    <p>PaLM, Falcon, StarCoder, SantaCoder</p>
                </div>
            </div>
        </div>
        
        <!-- 6. Flash Attention -->
        <div class="attention-type" id="flash-attention">
            <h2><span class="icon">6</span> Flash Attention</h2>
            
            <h3>📐 数学原理</h3>
            <div class="math-section">
                <p><b>在线Softmax (Online Softmax)：</b></p>
                <p>数值稳定的增量计算：</p>
                <p>$$m^{new} = \max(m^{old}, \text{rowmax}(S))$$</p>
                <p>$$l^{new} = e^{m^{old}-m^{new}}l^{old} + \text{rowsum}(e^{S-m^{new}})$$</p>
                <p>$$O^{new} = \frac{e^{m^{old}-m^{new}}l^{old}O^{old} + e^{S-m^{new}}V}{l^{new}}$$</p>
                
                <div class="step-box">
                    <b>分块策略：</b>
                    <ul style="margin-left: 20px;">
                        <li>将Q分成大小为B_r的块</li>
                        <li>将K,V分成大小为B_c的块</li>
                        <li>逐块在SRAM中计算，减少HBM访问</li>
                    </ul>
                </div>
            </div>
            
            <h3>🎨 图形化解释</h3>
            <div class="visualization">
                <svg viewBox="0 0 1200 500">
                    <text x="600" y="30" text-anchor="middle" font-weight="bold" font-size="16">Flash Attention: 分块计算与内存优化</text>
                    
                    <!-- 内存层次 -->
                    <g transform="translate(100, 80)">
                        <text x="150" y="-10" text-anchor="middle" font-weight="bold">GPU内存层次</text>
                        
                        <!-- SRAM (快速小内存) -->
                        <rect x="0" y="0" width="300" height="50" fill="#00f2fe" opacity="0.7" stroke="#333"/>
                        <text x="150" y="30" text-anchor="middle">SRAM (20KB, 19TB/s)</text>
                        
                        <!-- HBM (慢速大内存) -->
                        <rect x="0" y="80" width="300" height="100" fill="#667eea" opacity="0.5" stroke="#333"/>
                        <text x="150" y="135" text-anchor="middle">HBM (40GB, 1.5TB/s)</text>
                        
                        <!-- 数据移动箭头 -->
                        <g stroke-width="2" marker-end="url(#arrowhead)">
                            <path d="M 100 50 L 100 80" stroke="#ff0000"/>
                            <text x="120" y="65" font-size="11" fill="#ff0000">写回</text>
                            <path d="M 200 80 L 200 50" stroke="#00cc00"/>
                            <text x="220" y="65" font-size="11" fill="#00cc00">加载</text>
                        </g>
                        
                        <!-- 瓶颈说明 -->
                        <rect x="0" y="200" width="300" height="60" fill="#ffe0e0" stroke="#ff0000" rx="5"/>
                        <text x="10" y="220" font-size="11">❌ 标准注意力问题：</text>
                        <text x="10" y="240" font-size="11">n×n矩阵太大，无法放入SRAM</text>
                    </g>
                    
                    <!-- 分块计算可视化 -->
                    <g transform="translate(500, 80)">
                        <text x="300" y="-10" text-anchor="middle" font-weight="bold">分块计算策略</text>
                        
                        <!-- Q的分块 -->
                        <g>
                            <text x="30" y="20" font-size="12">Q块</text>
                            <rect x="0" y="30" width="60" height="200" fill="#667eea" opacity="0.3" stroke="#333"/>
                            <!-- 当前处理的块 -->
                            <rect x="0" y="30" width="60" height="50" fill="#667eea" opacity="0.8" stroke="#333" stroke-width="2"/>
                            <text x="30" y="60" text-anchor="middle" font-size="11" fill="white">Q₁</text>
                            <rect x="0" y="80" width="60" height="50" fill="#667eea" opacity="0.4" stroke="#333"/>
                            <rect x="0" y="130" width="60" height="50" fill="#667eea" opacity="0.4" stroke="#333"/>
                            <rect x="0" y="180" width="60" height="50" fill="#667eea" opacity="0.4" stroke="#333"/>
                        </g>
                        
                        <!-- K,V的分块 -->
                        <g transform="translate(100, 0)">
                            <text x="100" y="20" font-size="12">K,V块</text>
                            <rect x="0" y="30" width="200" height="60" fill="#764ba2" opacity="0.3" stroke="#333"/>
                            <!-- 当前处理的块 -->
                            <rect x="0" y="30" width="50" height="60" fill="#764ba2" opacity="0.8" stroke="#333" stroke-width="2"/>
                            <text x="25" y="65" text-anchor="middle" font-size="11" fill="white">K₁V₁</text>
                            <rect x="50" y="30" width="50" height="60" fill="#764ba2" opacity="0.4" stroke="#333"/>
                            <rect x="100" y="30" width="50" height="60" fill="#764ba2" opacity="0.4" stroke="#333"/>
                            <rect x="150" y="30" width="50" height="60" fill="#764ba2" opacity="0.4" stroke="#333"/>
                        </g>
                        
                        <!-- 输出块 -->
                        <g transform="translate(350, 0)">
                            <text x="30" y="20" font-size="12">输出</text>
                            <rect x="0" y="30" width="60" height="50" fill="#00f2fe" opacity="0.8" stroke="#333" stroke-width="2"/>
                            <text x="30" y="60" text-anchor="middle" font-size="11">O₁</text>
                        </g>
                        
                        <!-- 计算流程箭头 -->
                        <g stroke="#333" stroke-width="1.5" marker-end="url(#arrowhead)">
                            <path d="M 65 55 L 95 55"/>
                            <path d="M 300 60 L 345 55"/>
                        </g>
                        
                        <!-- SRAM标注 -->
                        <rect x="0" y="250" width="400" height="40" fill="#e0ffe0" stroke="#00cc00" rx="5"/>
                        <text x="200" y="275" text-anchor="middle" font-size="12" fill="#00cc00">
                            ✓ 当前块完全在SRAM中计算
                        </text>
                    </g>
                    
                    <!-- 算法流程 -->
                    <g transform="translate(100, 380)">
                        <rect x="0" y="0" width="1000" height="100" fill="#f8f9fa" stroke="#667eea" stroke-width="2" rx="10"/>
                        <text x="500" y="25" text-anchor="middle" font-weight="bold" fill="#667eea">Flash Attention算法流程</text>
                        
                        <text x="20" y="50" font-size="12">for i in range(0, N, Br):  <span class="code-comment" fill="#008000"># 遍历Q块</span></text>
                        <text x="40" y="70" font-size="12">for j in range(0, N, Bc):  <span class="code-comment" fill="#008000"># 遍历KV块</span></text>
                        <text x="60" y="90" font-size="12">在SRAM中计算 Q[i]×K[j]^T×V[j]，增量更新O[i]</text>
                    </g>
                </svg>
            </div>
            
            <div class="code-ref">
                <h4>📝 代码实现</h4>
                <pre>
<span class="code-comment"># Flash Attention核心循环</span>
def flash_attention_forward(Q, K, V):
    <span class="code-comment"># Q: [batch, heads, N, d]</span>
    <span class="code-comment"># 分块大小（适配SRAM大小）</span>
    Br = 64  <span class="code-comment"># Q块大小</span>
    Bc = 64  <span class="code-comment"># KV块大小</span>
    
    N = Q.shape[2]
    O = torch.zeros_like(Q)  <span class="code-comment"># 输出</span>
    L = torch.zeros(batch, heads, N)  <span class="code-comment"># 归一化因子</span>
    
    <span class="code-comment"># 外循环：遍历Q的块</span>
    for i in range(0, N, Br):
        Qi = Q[:, :, i:i+Br, :]  <span class="code-comment"># 加载Q块到SRAM</span>
        Oi = torch.zeros_like(Qi)
        Li = torch.zeros(batch, heads, Br) - float('inf')
        Mi = torch.full_like(Li, -float('inf'))  <span class="code-comment"># 最大值</span>
        
        <span class="code-comment"># 内循环：遍历KV的块</span>
        for j in range(0, N, Bc):
            Kj = K[:, :, j:j+Bc, :]  <span class="code-comment"># 加载KV块到SRAM</span>
            Vj = V[:, :, j:j+Bc, :]
            
            <span class="code-comment"># 在SRAM中计算注意力</span>
            Sij = torch.matmul(Qi, Kj.transpose(-2, -1)) / sqrt(d)
            
            <span class="code-comment"># 在线softmax（数值稳定）</span>
            Mi_new = torch.max(Mi, Sij.max(dim=-1)[0])
            Pij = torch.exp(Sij - Mi_new.unsqueeze(-1))
            
            <span class="code-comment"># 增量更新输出</span>
            Li = torch.exp(Mi - Mi_new) * Li + Pij.sum(dim=-1)
            Oi = torch.exp(Mi - Mi_new).unsqueeze(-1) * Oi + torch.matmul(Pij, Vj)
            Mi = Mi_new
        
        <span class="code-comment"># 归一化并写回HBM</span>
        O[:, :, i:i+Br, :] = Oi / Li.unsqueeze(-1)
    
    return O

<span class="code-comment"># 关键优化：</span>
<span class="code-comment"># 1. 避免存储n×n注意力矩阵</span>
<span class="code-comment"># 2. 分块计算，每块都在SRAM中</span>
<span class="code-comment"># 3. IO复杂度从O(N²) → O(N²/M)，M是SRAM大小</span></pre>
            </div>
            
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>✅ 优势</h4>
                    <p>2-4倍速度提升，O(n)内存，数值稳定</p>
                </div>
                <div class="feature-card">
                    <h4>⚠️ 劣势</h4>
                    <p>需要CUDA kernel优化，实现复杂</p>
                </div>
                <div class="feature-card">
                    <h4>💡 应用</h4>
                    <p>GPT-4, Claude, Gemini, 现代大模型标配</p>
                </div>
            </div>
        </div>
        
        <!-- 性能对比表格 -->
        <div class="attention-type">
            <h2><span class="icon">📊</span> 性能对比总结</h2>
            
            <table class="complexity-table">
                <thead>
                    <tr>
                        <th>注意力机制</th>
                        <th>时间复杂度</th>
                        <th>内存复杂度</th>
                        <th>KV缓存</th>
                        <th>最适合场景</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><span class="highlight">Full Attention</span></td>
                        <td>O(n²d)</td>
                        <td>O(n²)</td>
                        <td>O(nhd)</td>
                        <td>短序列，完整建模</td>
                    </tr>
                    <tr>
                        <td><span class="highlight">Local Attention</span></td>
                        <td>O(nwd)</td>
                        <td>O(nw)</td>
                        <td>O(nhd)</td>
                        <td>长序列，局部依赖</td>
                    </tr>
                    <tr>
                        <td><span class="highlight">Linear Attention</span></td>
                        <td>O(nd²)</td>
                        <td>O(nd)</td>
                        <td>O(nhd)</td>
                        <td>超长序列</td>
                    </tr>
                    <tr>
                        <td><span class="highlight">GQA</span></td>
                        <td>O(n²d)</td>
                        <td>O(n²)</td>
                        <td>O(n(h/G)d)</td>
                        <td>推理优化</td>
                    </tr>
                    <tr>
                        <td><span class="highlight">MQA</span></td>
                        <td>O(n²d)</td>
                        <td>O(n²)</td>
                        <td>O(nd)</td>
                        <td>极限内存优化</td>
                    </tr>
                    <tr>
                        <td><span class="highlight">Flash Attention</span></td>
                        <td>O(n²d)</td>
                        <td>O(n)</td>
                        <td>O(nhd)</td>
                        <td>训练加速</td>
                    </tr>
                </tbody>
            </table>
            
            <h3 style="margin-top: 40px;">🔍 特性对比</h3>
            <table class="complexity-table">
                <thead>
                    <tr>
                        <th>特性</th>
                        <th>Full</th>
                        <th>Sparse</th>
                        <th>Linear</th>
                        <th>GQA</th>
                        <th>MQA</th>
                        <th>Flash</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><b>模型质量</b></td>
                        <td>⭐⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐⭐</td>
                    </tr>
                    <tr>
                        <td><b>推理速度</b></td>
                        <td>⭐⭐</td>
                        <td>⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐</td>
                    </tr>
                    <tr>
                        <td><b>内存效率</b></td>
                        <td>⭐</td>
                        <td>⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐⭐</td>
                    </tr>
                    <tr>
                        <td><b>长序列支持</b></td>
                        <td>⭐</td>
                        <td>⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐⭐</td>
                        <td>⭐⭐</td>
                        <td>⭐⭐</td>
                        <td>⭐⭐⭐</td>
                    </tr>
                    <tr>
                        <td><b>实现难度</b></td>
                        <td>⭐⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐⭐</td>
                        <td>⭐</td>
                    </tr>
                    <tr>
                        <td><b>主流采用度</b></td>
                        <td>⭐⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐</td>
                        <td>⭐⭐</td>
                        <td>⭐⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐</td>
                        <td>⭐⭐⭐⭐⭐</td>
                    </tr>
                </tbody>
            </table>
            
            <div style="margin-top: 30px; padding: 20px; background: #f0f8ff; border-radius: 10px;">
                <h3>💡 选择建议</h3>
                <ul style="margin-left: 20px; line-height: 2;">
                    <li><b>训练阶段：</b>Flash Attention + Full Attention（速度与精度并重）</li>
                    <li><b>推理优化：</b>GQA（Llama系列）或MQA（PaLM系列）减少KV缓存</li>
                    <li><b>中等文本（1K-8K）：</b>Local Attention（Longformer）或稀疏模式</li>
                    <li><b>长文本（8K-32K）：</b>块稀疏（BigBird）或混合注意力</li>
                    <li><b>超长文本（>32K）：</b>Linear Attention（Performer）或Linformer</li>
                    <li><b>边缘设备：</b>MQA + 量化 + Flash Attention组合</li>
                    <li><b>实时服务：</b>GQA（4个KV头）+ Flash Attention v2</li>
                </ul>
                
                <div style="margin-top: 20px; padding: 15px; background: #fff3cd; border-radius: 8px;">
                    <h4 style="color: #856404;">🎯 实践经验</h4>
                    <ul style="margin-left: 20px; line-height: 1.8; color: #856404;">
                        <li>不同层可以使用不同注意力：底层Local，中层Sparse，顶层Full</li>
                        <li>Flash Attention已成为训练标配，几乎无需权衡</li>
                        <li>GQA是目前最平衡的选择（Llama 2/3采用）</li>
                        <li>线性注意力虽然快但精度损失明显，谨慎使用</li>
                        <li>根据硬件选择：A100/H100用Flash，消费级GPU用GQA/MQA</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</body>
</html>
