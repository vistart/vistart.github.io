<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer架构演进可视化 - 完整版</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/plotly.js/2.26.0/plotly.min.js"></script>
    <!-- MathJax 3.x 优化配置 -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true,
                processRefs: true
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            startup: {
                typeset: false  // 手动控制渲染时机
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.0/mermaid.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
            line-height: 1.6;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .header {
            text-align: center;
            color: white;
            margin-bottom: 30px;
        }
        
        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        .timeline {
            display: flex;
            justify-content: space-between;
            background: rgba(255,255,255,0.95);
            border-radius: 15px;
            padding: 20px;
            margin-bottom: 30px;
            box-shadow: 0 8px 32px rgba(0,0,0,0.1);
            backdrop-filter: blur(10px);
            flex-wrap: wrap;
            gap: 10px;
        }
        
        .timeline-item {
            text-align: center;
            flex: 1;
            min-width: 120px;
            padding: 10px;
            cursor: pointer;
            border-radius: 10px;
            transition: all 0.3s ease;
            position: relative;
        }
        
        .timeline-item:hover {
            background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
            transform: translateY(-5px);
        }
        
        .timeline-item.active {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }
        
        .year {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 5px;
        }
        
        .tech {
            font-size: 0.9em;
            opacity: 0.8;
        }
        
        .demo-card {
            background: rgba(255,255,255,0.95);
            border-radius: 15px;
            padding: 25px;
            margin-bottom: 30px;
            box-shadow: 0 8px 32px rgba(0,0,0,0.1);
            backdrop-filter: blur(10px);
            text-align: center;
        }
        
        .demo-card h3 {
            color: #667eea;
            margin-bottom: 20px;
            font-size: 1.3em;
            text-align: center;
        }
        
        .controls {
            margin: 20px 0;
            max-width: 400px;
            margin-left: auto;
            margin-right: auto;
        }
        
        .control-group {
            margin-bottom: 15px;
            text-align: left;
        }
        
        .control-group label {
            display: block;
            margin-bottom: 5px;
            font-weight: 500;
        }
        
        .control-group input[type="range"] {
            width: 100%;
            height: 6px;
            border-radius: 3px;
            background: #ddd;
            outline: none;
            -webkit-appearance: none;
        }
        
        .control-group input[type="range"]::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            background: #667eea;
            cursor: pointer;
        }
        
        .control-group select {
            width: 100%;
            padding: 8px;
            border: 1px solid #ddd;
            border-radius: 5px;
            background: white;
        }
        
        .plot-container {
            height: 400px;
            border-radius: 10px;
            overflow: hidden;
            margin: 20px 0;
        }
        
        .comparison-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .metric-card {
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            padding: 15px;
            border-radius: 10px;
            text-align: center;
        }
        
        .metric-value {
            font-size: 1.5em;
            font-weight: bold;
            color: #667eea;
        }
        
        .metric-label {
            font-size: 0.9em;
            opacity: 0.8;
            margin-top: 5px;
        }
        
        .layer-block {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
            text-align: center;
            position: relative;
            transition: all 0.3s ease;
        }
        
        .layer-block:hover {
            transform: scale(1.02);
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }
        
        .improvement-badge {
            position: absolute;
            top: -5px;
            right: -5px;
            background: #ff6b6b;
            color: white;
            border-radius: 50%;
            width: 25px;
            height: 25px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.7em;
            font-weight: bold;
        }
        
        .formula-section {
            background: rgba(255,255,255,0.1);
            border-radius: 10px;
            padding: 15px;
            margin: 15px 0;
            border-left: 4px solid #667eea;
            text-align: center;
        }
        
        .formula-title {
            font-weight: bold;
            margin-bottom: 10px;
            color: #667eea;
        }
        
        .code-section {
            background: #2d3748;
            color: #e2e8f0;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            overflow-x: auto;
            text-align: left;
            white-space: pre-wrap;  /* 保持换行和空白字符 */
            line-height: 1.4;       /* 增加行高提升可读性 */
        }
        
        .toggle-buttons {
            display: flex;
            gap: 10px;
            margin-bottom: 15px;
            justify-content: center;
            flex-wrap: wrap;
        }
        
        .toggle-btn {
            padding: 8px 16px;
            border: none;
            border-radius: 5px;
            background: #e2e8f0;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .toggle-btn.active {
            background: #667eea;
            color: white;
        }
        
        .toggle-content {
            display: none;
        }
        
        .toggle-content.active {
            display: block;
        }
        
        .hidden {
            display: none;
        }
        
        .comparison-container {
            margin: 20px 0;
        }
        
        .comparison-item {
            border-radius: 10px;
            padding: 20px;
            margin: 15px 0;
            border: 2px solid transparent;
            transition: all 0.3s ease;
        }
        
        .comparison-item.before {
            background: linear-gradient(135deg, #ffcccc 0%, #ffe6e6 100%);
            border-color: #ff6b6b;
        }
        
        .comparison-item.after {
            background: linear-gradient(135deg, #ccffcc 0%, #e6ffe6 100%);
            border-color: #4ecdc4;
        }
        
        .comparison-title {
            font-weight: bold;
            margin-bottom: 15px;
            text-align: center;
            font-size: 1.1em;
        }
        
        .mermaid-container {
            background: rgba(255,255,255,0.9);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            text-align: center;
        }
        
        .mermaid {
            max-width: 100%;
            overflow-x: auto;
        }
        
        .explanation-box {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border-left: 4px solid #667eea;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
            font-size: 0.9em;
            text-align: left;
        }
        
        .explanation-box h4 {
            color: #667eea;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .explanation-box ul {
            margin: 10px 0;
            padding-left: 20px;
        }
        
        .explanation-box li {
            margin-bottom: 8px;
            line-height: 1.5;
        }
        
        .complexity-badge {
            display: inline-block;
            background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
            color: #333;
            padding: 3px 8px;
            border-radius: 15px;
            font-size: 0.8em;
            font-weight: bold;
            margin-left: 10px;
        }
        
        .improvement-indicator {
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            border-radius: 10px;
            padding: 10px;
            margin: 10px 0;
            text-align: center;
            font-weight: bold;
        }
        
        .improvement-indicator.positive {
            background: linear-gradient(135deg, #d4f4dd 0%, #b8e6c1 100%);
            border: 2px solid #4caf50;
        }
        
        .improvement-indicator.negative {
            background: linear-gradient(135deg, #ffe6e6 0%, #ffcccc 100%);
            border: 2px solid #f44336;
        }
        
        /* MathJax样式增强 */
        .MathJax {
            font-size: 1.1em !important;
        }
        
        .MathJax_Display {
            margin: 0.5em 0 !important;
        }
        
        .inline-math {
            display: inline !important;
            margin: 0 2px !important;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }
            
            .timeline {
                flex-direction: column;
            }
            
            .timeline-item {
                min-width: auto;
            }
            
            .header h1 {
                font-size: 2em;
            }
            
            .demo-card {
                padding: 15px;
            }
            
            .plot-container {
                height: 300px;
            }
            
            .explanation-box {
                padding: 10px;
                font-size: 0.8em;
            }
            
            .explanation-box h4 {
                font-size: 1em;
            }
            
            .comparison-grid {
                grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
                gap: 10px;
            }
            
            .complexity-badge {
                display: block;
                margin: 5px 0;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>🚀 Transformer架构演进可视化</h1>
            <p>探索从原始Transformer到现代LLM的关键技术突破</p>
        </div>
        
        <div class="timeline">
            <div class="timeline-item active" data-year="2017">
                <div class="year">2017</div>
                <div class="tech">原始Transformer</div>
            </div>
            <div class="timeline-item" data-year="2019">
                <div class="year">2019</div>
                <div class="tech">RMSNorm</div>
            </div>
            <div class="timeline-item" data-year="2020">
                <div class="year">2020</div>
                <div class="tech">SwiGLU</div>
            </div>
            <div class="timeline-item" data-year="2021">
                <div class="year">2021</div>
                <div class="tech">RoPE</div>
            </div>
            <div class="timeline-item" data-year="2023">
                <div class="year">2023</div>
                <div class="tech">现代LLM</div>
            </div>
        </div>
        
        <!-- 2017: 原始Transformer -->
        <div class="demo-section" id="demo-2017">
            <div class="demo-card">
                <h3>🏗️ 原始Transformer架构 - Post-Norm</h3>
                
                <div class="mermaid-container">
                    <div class="mermaid">
                        flowchart TD
                            A[输入序列] --> B[词嵌入 + 位置编码]
                            B --> C[多头注意力]
                            C --> D[残差连接]
                            D --> E[LayerNorm]
                            E --> F[前馈网络 FFN]
                            F --> G[残差连接]
                            G --> H[LayerNorm]
                            H --> I[输出]
                            
                            style C fill:#ff9999
                            style E fill:#ffcc99
                            style F fill:#99ccff
                            style H fill:#ffcc99
                    </div>
                </div>
                
                <div class="formula-section">
                    <div class="formula-title">Post-Norm计算流程：</div>
                    <div id="postnorm-formula">
                        $$\text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))$$
                        $$\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sigma} + \beta$$
                        <p style="margin-top: 10px; font-size: 0.9em; color: #666;">
                        其中 $\mu$ 是均值，$\sigma$ 是标准差
                        </p>
                    </div>
                    <div class="explanation-box">
                        <h4>🔍 Post-Norm架构分析</h4>
                        <p><strong>计算复杂度:</strong> $O(n)$ 注意力 + $O(3d)$ LayerNorm <span class="complexity-badge">额外开销</span></p>
                        <p><strong>设计思路:</strong></p>
                        <ul>
                            <li>🔄 <strong>先计算后规范:</strong> 先进行注意力/FFN计算，再归一化</li>
                            <li>🎯 <strong>输出稳定:</strong> 每层输出都经过归一化，数值范围稳定</li>
                            <li>📊 <strong>原始设计:</strong> 2017年的经典设计，被广泛采用</li>
                        </ul>
                        <p><strong>实际问题:</strong></p>
                        <ul>
                            <li>⚠️ <strong>梯度传播:</strong> 梯度需要通过LayerNorm反向传播，容易不稳定</li>
                            <li>🐌 <strong>训练初期:</strong> 模型参数初始化不当容易导致训练困难</li>
                            <li>📉 <strong>深度限制:</strong> 超过一定深度后训练变得非常困难</li>
                        </ul>
                        <div class="improvement-indicator negative">
                            ⚠️ 梯度不稳定问题在深层网络中尤为突出
                        </div>
                    </div>
                </div>
                
                <div class="layer-block">
                    <strong>Post-Norm结构特点</strong><br>
                    x → Attention → Add → Norm → FFN → Add → Norm
                    <div class="improvement-badge">⚠️</div>
                </div>
            </div>
            
            <div class="demo-card">
                <h3>📍 位置编码：正弦波模式</h3>
                
                <div class="formula-section">
                    <div class="formula-title">正弦位置编码公式：</div>
                    <div id="sinusoidal-formula">
                        $$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
                        $$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
                    </div>
                    <div class="explanation-box">
                        <h4>🔍 深度解析</h4>
                        <p><strong>时间复杂度:</strong> $O(\text{seq\_len} \times d_{\text{model}})$ - 预计算一次即可重复使用</p>
                        <p><strong>为什么用正弦+余弦对？</strong></p>
                        <ul>
                            <li>📐 <strong>唯一性保证:</strong> $(\sin, \cos)$组合可以唯一表示任意角度，避免位置冲突</li>
                            <li>🔄 <strong>周期性特征:</strong> 不同频率的波形组合，捕获多尺度位置关系</li>
                            <li>📏 <strong>相对位置:</strong> 利用三角恒等式 $\sin(a+b) = \sin(a)\cos(b) + \cos(a)\sin(b)$</li>
                            <li>🎯 <strong>平滑过渡:</strong> 连续函数保证相邻位置的编码平滑变化</li>
                        </ul>
                        <p><strong>通俗理解:</strong> 就像给每个位置分配一个"身份证号"，用多个不同频率的波形组合，确保每个位置都有独特的"指纹"</p>
                    </div>
                </div>
                
                <div class="toggle-buttons">
                    <button class="toggle-btn active" onclick="toggleContent('sin-viz', this)">可视化</button>
                    <button class="toggle-btn" onclick="toggleContent('sin-code', this)">代码示例</button>
                </div>
                
                <div class="toggle-content active" id="sin-viz">
                    <div class="controls">
                        <div class="control-group">
                            <label>序列长度: <span id="seq-len-value">64</span></label>
                            <input type="range" id="seq-len" min="16" max="256" value="64">
                        </div>
                        <div class="control-group">
                            <label>维度: <span id="d-model-value">128</span></label>
                            <input type="range" id="d-model" min="64" max="512" step="64" value="128">
                        </div>
                    </div>
                    <div class="plot-container" id="pos-encoding-plot"></div>
                </div>
                
                <div class="toggle-content" id="sin-code">
                    <div class="code-section">
import torch
import math

def sinusoidal_positional_encoding(max_len, d_model):
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len).unsqueeze(1).float()
    
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                        -(math.log(10000.0) / d_model))
    
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    return pe

# 示例使用
pos_encoding = sinusoidal_positional_encoding(512, 256)
print(f"位置编码形状: {pos_encoding.shape}")
                    </div>
                </div>
            </div>
            
            <div class="demo-card">
                <h3>⚡ 激活函数：ReLU特性</h3>
                
                <div class="formula-section">
                    <div class="formula-title">ReLU函数公式：</div>
                    <div id="relu-formula">
                        $$\text{ReLU}(x) = \max(0, x)$$
                    </div>
                    <div class="explanation-box">
                        <h4>🔍 激活函数深度解析</h4>
                        <p><strong>时间复杂度:</strong> $O(n)$ - 线性时间，每个元素一次比较 <span class="complexity-badge">极快</span></p>
                        <p><strong>ReLU工作原理:</strong></p>
                        <ul>
                            <li>🔌 <strong>简单开关:</strong> 输入>0时输出原值，≤0时输出0，像神经元的"开关"</li>
                            <li>⚡ <strong>计算高效:</strong> 只需要一次比较操作，比sigmod/tanh快很多</li>
                            <li>🎯 <strong>稀疏激活:</strong> 约一半神经元被"关闭"，网络更稀疏</li>
                            <li>⚠️ <strong>死神经元问题:</strong> 一旦输出0，梯度也为0，神经元可能永远"死掉"</li>
                        </ul>
                        <div class="improvement-indicator negative">
                            ❌ 主要问题：梯度消失 + 死神经元 + 输出不居中
                        </div>
                        <p><strong>通俗理解:</strong> ReLU就像一个"过滤器"，只让正数通过，负数直接变成0。简单粗暴但有效！</p>
                    </div>
                </div>
                
                <div class="toggle-buttons">
                    <button class="toggle-btn active" onclick="toggleContent('relu-viz', this)">可视化</button>
                    <button class="toggle-btn" onclick="toggleContent('relu-code', this)">代码示例</button>
                </div>
                
                <div class="toggle-content active" id="relu-viz">
                    <div class="controls">
                        <div class="control-group">
                            <label>函数类型:</label>
                            <select id="activation-type">
                                <option value="relu">ReLU</option>
                                <option value="gelu">GELU (改进)</option>
                                <option value="swish">Swish (进一步改进)</option>
                                <option value="swiglu">SwiGLU (现代)</option>
                            </select>
                        </div>
                    </div>
                    <div class="plot-container" id="activation-plot"></div>
                </div>
                
                <div class="toggle-content" id="relu-code">
                    <div class="code-section">
import torch
import torch.nn.functional as F
import math

def relu(x):
    return torch.max(torch.zeros_like(x), x)

def gelu(x):
    # 精确版本 (使用误差函数)
    return 0.5 * x * (1 + torch.erf(x / math.sqrt(2)))
    
def gelu_approximate(x):
    # 近似版本 (计算更快)
    return 0.5 * x * (1 + torch.tanh(
        math.sqrt(2.0 / math.pi) * 
        (x + 0.044715 * torch.pow(x, 3))
    ))

def swish(x):
    return x * torch.sigmoid(x)

def swiglu_gate(x1, x2):
    # SwiGLU的门控逻辑
    return swish(x1) * x2

# 函数特性对比
x = torch.randn(100)
relu_out = relu(x)           # 硬切换，死神经元风险
gelu_out = gelu(x)           # 平滑概率门控  
swish_out = swish(x)         # 自门控机制
                    </div>
                </div>
            </div>
        </div>
        
        <!-- 2019: RMSNorm -->
        <div class="demo-section hidden" id="demo-2019">
            <div class="demo-card">
                <h3>📊 归一化方法演进</h3>
                
                <div class="mermaid-container">
                    <div class="mermaid">
                        graph LR
                            A["LayerNorm 原始"] --> B["RMSNorm 改进"]
                            
                            subgraph LN ["LayerNorm 处理流程"]
                                C["计算均值"]
                                D["计算方差"]
                                E["标准化处理"]
                                F["缩放和偏移"]
                                C --> D
                                D --> E
                                E --> F
                            end
                            
                            subgraph RN ["RMSNorm 处理流程"]
                                G["计算RMS"]
                                H["标准化处理"]
                                I["仅缩放"]
                                G --> H
                                H --> I
                            end
                            
                            style A fill:#ffcccc
                            style B fill:#ccffcc
                            style LN fill:#ffe6e6
                            style RN fill:#e6ffe6
                    </div>
                </div>
                
                <div class="comparison-container">
                    <div class="comparison-item before">
                        <div class="comparison-title">LayerNorm (原始)</div>
                        <div class="formula-section">
                            <div id="layernorm-formula">
                                $$\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sigma} + \beta$$
                                $$\mu = \frac{1}{d}\sum_{i=1}^{d} x_i$$
                                $$\sigma = \sqrt{\frac{1}{d}\sum_{i=1}^{d} (x_i - \mu)^2}$$
                            </div>
                            <div class="explanation-box">
                                <h4>🔍 LayerNorm分析</h4>
                                <p><strong>时间复杂度:</strong> $O(d) + O(d) + O(d) = O(3d)$ <span class="complexity-badge">三次遍历</span></p>
                                <p><strong>计算步骤:</strong></p>
                                <ul>
                                    <li>📊 第一遍扫描：计算均值 $\mu$</li>
                                    <li>📈 第二遍扫描：计算方差 $\sigma^2$</li>
                                    <li>🔄 第三遍扫描：标准化+缩放+偏移</li>
                                    <li>💾 需要存储4个参数：$\mu$, $\sigma$, $\gamma$, $\beta$</li>
                                </ul>
                                <div class="improvement-indicator negative">
                                    ⚠️ 计算量大 + 内存消耗高 + 三次遍历数据
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="comparison-item after">
                        <div class="comparison-title">RMSNorm (改进)</div>
                        <div class="formula-section">
                            <div id="rmsnorm-formula">
                                $$\text{RMSNorm}(x) = \gamma \cdot \frac{x}{\text{RMS}(x)}$$
                                $$\text{RMS}(x) = \sqrt{\frac{1}{d}\sum_{i=1}^{d} x_i^2}$$
                            </div>
                            <div class="explanation-box">
                                <h4>🔍 RMSNorm优化</h4>
                                <p><strong>时间复杂度:</strong> $O(d) + O(d) = O(2d)$ <span class="complexity-badge">仅两次遍历</span></p>
                                <p><strong>关键改进:</strong></p>
                                <ul>
                                    <li>🚀 <strong>去除均值计算:</strong> 不需要计算 $\mu$，直接用RMS</li>
                                    <li>⚡ <strong>减少参数:</strong> 只需 $\gamma$ 缩放参数，去掉 $\beta$ 偏移</li>
                                    <li>🎯 <strong>数值稳定:</strong> RMS计算更稳定，避免均值偏差</li>
                                    <li>💨 <strong>速度提升:</strong> 减少33%的计算量</li>
                                </ul>
                                <div class="improvement-indicator positive">
                                    ✅ 速度快20% + 内存省15% + 数值更稳定
                                </div>
                                <p><strong>为什么有效？</strong> 研究发现，在深度网络中，重新居中（减均值）的重要性不如重新缩放，RMSNorm专注于缩放效果更好。</p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="toggle-buttons">
                    <button class="toggle-btn active" onclick="toggleContent('norm-viz', this)">可视化对比</button>
                    <button class="toggle-btn" onclick="toggleContent('norm-code', this)">代码对比</button>
                </div>
                
                <div class="toggle-content active" id="norm-viz">
                    <div class="controls">
                        <div class="control-group">
                            <label>归一化类型:</label>
                            <select id="norm-type">
                                <option value="layernorm">LayerNorm</option>
                                <option value="rmsnorm">RMSNorm</option>
                                <option value="both">对比显示</option>
                            </select>
                        </div>
                        <div class="control-group">
                            <label>输入噪声强度: <span id="noise-level-value">0.5</span></label>
                            <input type="range" id="noise-level" min="0" max="2" step="0.1" value="0.5">
                        </div>
                    </div>
                    <div class="plot-container" id="norm-comparison-plot"></div>
                </div>
                
                <div class="toggle-content" id="norm-code">
                    <div class="comparison-container">
                        <div class="comparison-item before">
                            <div class="comparison-title">LayerNorm实现</div>
                            <div class="code-section">
class LayerNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(dim))
        self.beta = nn.Parameter(torch.zeros(dim))
        self.eps = eps
    
    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        var = x.var(-1, keepdim=True, unbiased=False)
        std = (var + self.eps).sqrt()
        return self.gamma * (x - mean) / std + self.beta
                            </div>
                        </div>
                        
                        <div class="comparison-item after">
                            <div class="comparison-title">RMSNorm实现</div>
                            <div class="code-section">
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(dim))
        self.eps = eps
    
    def forward(self, x):
        rms = x.norm(dim=-1, keepdim=True) / math.sqrt(x.size(-1))
        return self.gamma * x / (rms + self.eps)
        
# 计算复杂度降低约15-20%
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="demo-card">
                <h3>⚡ 性能提升分析</h3>
                <div class="comparison-grid">
                    <div class="metric-card">
                        <div class="metric-value">100%</div>
                        <div class="metric-label">LayerNorm计算量</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">80%</div>
                        <div class="metric-label">RMSNorm计算量</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">15%</div>
                        <div class="metric-label">内存节省</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">20%</div>
                        <div class="metric-label">速度提升</div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- 2020: SwiGLU -->
        <div class="demo-section hidden" id="demo-2020">
            <div class="demo-card">
                <h3>🔄 激活函数演进历程</h3>
                
                <div class="mermaid-container">
                    <div class="mermaid">
                        graph TD
                            A["ReLU 2010"] --> B["GELU 2016"]
                            B --> C["Swish 2017"]
                            C --> D["SwiGLU 2020"]
                            
                            A1["简单高效<br/>死神经元问题"] --> A
                            B1["平滑可导<br/>更好表达能力"] --> B
                            C1["自门控机制<br/>非单调性"] --> C
                            D1["显式门控<br/>现代LLM标准"] --> D
                            
                            style A fill:#ff9999
                            style B fill:#ffcc99
                            style C fill:#99ccff
                            style D fill:#99ff99
                    </div>
                </div>
                
                <div class="formula-section">
                    <div class="formula-title">激活函数演进公式：</div>
                    <div id="activation-formulas">
                        <div style="margin-bottom: 15px;">
                            <strong>GELU (2016):</strong>
                            $$\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]$$
                            <p style="font-size: 0.8em; color: #666; margin-top: 5px;">近似版本: $$\text{GELU}(x) \approx 0.5x\left(1 + \tanh\left[\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)\right]\right)$$</p>
                        </div>
                        <div style="margin-bottom: 15px;">
                            <strong>Swish (2017):</strong>
                            $$\text{Swish}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}$$
                        </div>
                        <div>
                            <strong>SwiGLU (2020):</strong>
                            $$\text{SwiGLU}(x) = \text{Swish}(xW_1) \odot (xW_2) \cdot W_3$$
                            $$= \frac{xW_1}{1 + e^{-xW_1}} \odot (xW_2) \cdot W_3$$
                        </div>
                    </div>
                    <div class="explanation-box">
                        <h4>🔍 激活函数演进深度解析</h4>
                        <p><strong>时间复杂度对比:</strong></p>
                        <ul>
                            <li>🔴 <strong>ReLU:</strong> $O(n)$ <span class="complexity-badge">最快</span> - 但功能受限</li>
                            <li>🟡 <strong>GELU:</strong> $O(n)$ <span class="complexity-badge">中等</span> - 需要erf函数近似</li>
                            <li>🟢 <strong>Swish:</strong> $O(n)$ <span class="complexity-badge">稍慢</span> - 需要sigmoid计算</li>
                            <li>🔵 <strong>SwiGLU:</strong> $O(2n)$ <span class="complexity-badge">双倍参数</span> - 但效果最佳</li>
                        </ul>
                        
                        <p><strong>函数图像特征描述:</strong></p>
                        <div style="margin: 15px 0;">
                            <div style="background: #fff3cd; padding: 10px; border-radius: 5px; margin: 8px 0;">
                                <strong>🔴 ReLU图像:</strong> 像一个"直角拐弯"，x<0时平坦为0，x>0时直线上升45°
                            </div>
                            <div style="background: #d1ecf1; padding: 10px; border-radius: 5px; margin: 8px 0;">
                                <strong>🟡 GELU图像:</strong> 像一个"平滑的S形弯道"，负值区域有小幅度输出，比ReLU更平滑
                            </div>
                            <div style="background: #d4edda; padding: 10px; border-radius: 5px; margin: 8px 0;">
                                <strong>🟢 Swish图像:</strong> 像一个"带凹陷的上坡路"，负值时有轻微下凹，然后平滑上升
                            </div>
                            <div style="background: #e2e3e5; padding: 10px; border-radius: 5px; margin: 8px 0;">
                                <strong>🔵 SwiGLU图像:</strong> 是两个函数的"智能组合"，Swish作为门控调节另一路的信息流
                            </div>
                        </div>
                        
                        <p><strong>演进逻辑:</strong></p>
                        <div class="improvement-indicator">
                            <strong>ReLU → GELU:</strong> 从硬切换到软切换，解决死神经元问题
                        </div>
                        <div class="improvement-indicator">
                            <strong>GELU → Swish:</strong> 简化计算，保持平滑特性，添加自门控
                        </div>
                        <div class="improvement-indicator positive">
                            <strong>Swish → SwiGLU:</strong> 显式门控机制，类似LSTM门控思想
                        </div>
                        
                        <p><strong>SwiGLU核心创新:</strong></p>
                        <ul>
                            <li>🚪 <strong>门控机制:</strong> Swish(xW₁)作为"门"，控制xW₂的信息流</li>
                            <li>🔄 <strong>信息选择:</strong> 网络自己学会哪些信息重要</li>
                            <li>📈 <strong>表达能力:</strong> 两个线性变换+门控，表达能力大幅提升</li>
                            <li>🎯 <strong>梯度友好:</strong> 门控设计避免梯度消失</li>
                        </ul>
                        
                        <p><strong>网络层配合设计:</strong></p>
                        <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 10px 0;">
                            <h5 style="color: #495057; margin-bottom: 10px;">🔧 SwiGLU的特殊要求</h5>
                            <ul>
                                <li><strong>双路径设计:</strong> 需要两个线性投影层（W₁门控路径 + W₂数值路径）</li>
                                <li><strong>维度调整:</strong> 隐藏层通常为输入维度的2.67倍，而非传统的4倍</li>
                                <li><strong>参数权衡:</strong> 虽然参数量增加，但表达能力和训练效果显著提升</li>
                                <li><strong>前层适配:</strong> 前一层输出无需特殊变换，SwiGLU内部处理所有逻辑</li>
                            </ul>
                            <p style="margin-top: 10px; font-style: italic; color: #6c757d;">
                                💡 为什么是2.67倍？因为SwiGLU需要额外的门控路径，在保持总参数量相近的情况下，
                                2.67倍的隐藏层能达到传统4倍隐藏层的效果，实现了效率与性能的最佳平衡。
                            </p>
                        </div>
                        
                        <p><strong>通俗理解:</strong> SwiGLU就像一个"智能开关"，不仅能开关，还能调节"开启程度"，让信息流动更精确！</p>
                    </div>
                </div>
                
                <div class="toggle-buttons">
                    <button class="toggle-btn active" onclick="toggleContent('swiglu-viz', this)">演进可视化</button>
                    <button class="toggle-btn" onclick="toggleContent('swiglu-code', this)">代码对比</button>
                </div>
                
                <div class="toggle-content active" id="swiglu-viz">
                    <div class="controls">
                        <div class="control-group">
                            <label>对比激活函数:</label>
                            <select id="activation-comparison">
                                <option value="all">全部对比</option>
                                <option value="relu-gelu">ReLU vs GELU</option>
                                <option value="gelu-swish">GELU vs Swish</option>
                                <option value="swish-swiglu">Swish vs SwiGLU</option>
                            </select>
                        </div>
                    </div>
                    <div class="plot-container" id="activation-evolution-plot"></div>
                </div>
                
                <div class="toggle-content" id="swiglu-code">
                    <div class="comparison-container">
                        <div class="comparison-item before">
                            <div class="comparison-title">传统激活函数实现</div>
                            <div class="code-section">
# ReLU: 简单直接的硬切换
def relu(x):
    return torch.max(torch.zeros_like(x), x)

# GELU: 平滑的概率门控
def gelu(x):
    # 精确版本
    return 0.5 * x * (1 + torch.erf(x / math.sqrt(2)))
    
    # 近似版本（更快）
    return 0.5 * x * (1 + torch.tanh(
        math.sqrt(2/math.pi) * (x + 0.044715 * x**3)
    ))

# Swish: 自门控激活
def swish(x):
    return x * torch.sigmoid(x)

# 传统FFN
class TraditionalFFN(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim)
        self.w2 = nn.Linear(hidden_dim, dim)
    
    def forward(self, x):
        return self.w2(swish(self.w1(x)))
                            </div>
                        </div>
                        
                        <div class="comparison-item after">
                            <div class="comparison-title">SwiGLU现代实现</div>
                            <div class="code-section">
class SwiGLU(nn.Module):
    def __init__(self, dim, hidden_dim=None):
        super().__init__()
        # 关键设计：hidden_dim通常是dim的2.67倍
        if hidden_dim is None:
            hidden_dim = int(2.67 * dim)  # 优化的维度比例
            
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)  # 门控投影
        self.w2 = nn.Linear(dim, hidden_dim, bias=False)  # 数值投影  
        self.w3 = nn.Linear(hidden_dim, dim, bias=False)  # 输出投影
    
    def forward(self, x):
        # 双路径处理
        gate = F.silu(self.w1(x))  # Swish门控路径
        value = self.w2(x)         # 线性数值路径
        hidden = gate * value      # 门控调节
        return self.w3(hidden)     # 输出投影

# 现代LLM中的使用
class ModernTransformerBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.attention = MultiHeadAttention(dim)
        self.ffn = SwiGLU(dim)  # 直接替换传统FFN
        self.norm1 = RMSNorm(dim)
        self.norm2 = RMSNorm(dim)
    
    def forward(self, x):
        # 前一层输出无需特殊处理，SwiGLU内部处理所有逻辑
        x = x + self.attention(self.norm1(x))
        x = x + self.ffn(self.norm2(x))  # 无缝集成
        return x
                            </div>
                        </div>
                    </div>
                    
                    <div class="explanation-box">
                        <h4>💡 实现要点解析</h4>
                        <ul>
                            <li><strong>🔧 维度设计:</strong> SwiGLU的hidden_dim=2.67×input_dim，平衡参数量与性能</li>
                            <li><strong>🚪 双路径:</strong> gate路径用Swish激活，value路径保持线性，两者相乘实现门控</li>
                            <li><strong>⚡ 无缝集成:</strong> 可以直接替换传统FFN，无需修改网络其他部分</li>
                            <li><strong>🎯 参数效率:</strong> 虽然多了一个投影层，但总体参数效率更高</li>
                        </ul>
                    </div>
                </div>
            </div>
            
            <div class="demo-card">
                <h3>📈 梯度流分析</h3>
                <div class="controls">
                    <div class="control-group">
                        <label>网络深度: <span id="depth-value">6</span></label>
                        <input type="range" id="depth" min="2" max="12" value="6">
                    </div>
                </div>
                <div class="plot-container" id="gradient-flow-plot"></div>
            </div>
            
            <div class="demo-card">
                <h3>🔧 SwiGLU参数设计优化</h3>
                
                <div class="explanation-box">
                    <h4>🎯 维度设计的数学原理</h4>
                    <p><strong>传统FFN vs SwiGLU参数对比:</strong></p>
                    
                    <div class="comparison-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
                        <div class="metric-card">
                            <div class="metric-value">传统FFN</div>
                            <div class="metric-label">$d \times 4d + 4d \times d = 8d^2$</div>
                            <div style="font-size: 0.8em; margin-top: 5px;">单路径，4倍隐藏层</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-value">SwiGLU</div>
                            <div class="metric-label">$d \times 2.67d \times 3 = 8d^2$</div>
                            <div style="font-size: 0.8em; margin-top: 5px;">双路径，2.67倍隐藏层</div>
                        </div>
                    </div>
                    
                    <p><strong>为什么选择2.67倍？</strong></p>
                    <ul>
                        <li>🧮 <strong>参数等效:</strong> 保持总参数量与传统4倍FFN相同</li>
                        <li>🚪 <strong>门控优势:</strong> 双路径设计比单路径表达能力更强</li>
                        <li>⚡ <strong>计算效率:</strong> 虽然多一次线性变换，但激活效果更好</li>
                        <li>🎯 <strong>经验最优:</strong> 大量实验证明2.67倍是性价比最高的选择</li>
                    </ul>
                    
                    <div class="improvement-indicator positive">
                        💡 关键洞察：SwiGLU用相同的参数量获得了更强的表达能力
                    </div>
                    
                    <p><strong>实际应用中的配置:</strong></p>
                    <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 10px 0;">
                        <ul>
                            <li><strong>LLaMA:</strong> hidden_dim = ⌊2.67 × d⌋，使用SwiGLU作为标准FFN</li>
                            <li><strong>PaLM:</strong> hidden_dim = 4 × d，但改用SwiGLU替代ReLU</li>
                            <li><strong>现代实践:</strong> 大多数新模型采用2.67倍+SwiGLU的组合</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- 2021: RoPE -->
        <div class="demo-section hidden" id="demo-2021">
            <div class="demo-card">
                <h3>🌀 位置编码革命：从绝对到相对</h3>
                
                <div class="mermaid-container">
                    <div class="mermaid">
                        graph TB
                            A["Sinusoidal 绝对位置编码"] --> B["RoPE 相对位置编码"]
                            
                            subgraph SIN ["Sinusoidal特点"]
                                C["固定编码表"]
                                D["绝对位置信息"]
                                E["长序列衰减"]
                            end
                            
                            subgraph ROPE ["RoPE特点"]
                                F["旋转变换"]
                                G["相对位置保持"]
                                H["外推能力强"]
                            end
                            
                            A -.-> SIN
                            B -.-> ROPE
                            
                            style A fill:#ffcccc
                            style B fill:#ccffcc
                    </div>
                </div>
                
                <div class="comparison-container">
                    <div class="comparison-item before">
                        <div class="comparison-title">Sinusoidal (原始)</div>
                        <div class="formula-section">
                            <div id="sin-pos-formula">
                                $$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)$$
                                $$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$$
                            </div>
                            <div class="explanation-box">
                                <h4>🔍 Sinusoidal缺陷分析</h4>
                                <p><strong>时间复杂度:</strong> $O(\text{seq\_len} \times d_{\text{model}})$ <span class="complexity-badge">预计算</span></p>
                                <ul>
                                    <li>📍 <strong>绝对位置:</strong> 只能表示token的绝对位置</li>
                                    <li>📉 <strong>长序列衰减:</strong> 超出训练长度后效果急剧下降</li>
                                    <li>🔒 <strong>固定模式:</strong> 位置编码模式固定，无法适应不同任务</li>
                                    <li>❌ <strong>外推困难:</strong> 训练512长度，测试1024长度效果很差</li>
                                </ul>
                                <div class="improvement-indicator negative">
                                    ⚠️ 长序列处理能力有限，无法很好建模相对位置关系
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="comparison-item after">
                        <div class="comparison-title">RoPE (改进)</div>
                        <div class="formula-section">
                            <div id="rope-pos-formula">
                                $$q_m = R_m \cdot q, \quad k_m = R_m \cdot k$$
                                $$R_m = \begin{pmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{pmatrix}$$
                                $$\theta_i = 10000^{-2i/d}, \quad i = 0, 1, ..., d/2-1$$
                            </div>
                            <div class="explanation-box">
                                <h4>🔍 RoPE革命性创新</h4>
                                <p><strong>时间复杂度:</strong> $O(\text{seq\_len} \times d_{\text{model}})$ <span class="complexity-badge">同样预计算</span></p>
                                <p><strong>旋转编码原理:</strong></p>
                                <ul>
                                    <li>🌀 <strong>旋转变换:</strong> 将q,k向量在复平面上旋转，角度与位置相关</li>
                                    <li>📐 <strong>相对位置保持:</strong> 内积 $q_i^T k_j$ 只依赖于相对位置$(i-j)$</li>
                                    <li>🔄 <strong>数学优雅:</strong> 利用复数旋转的性质，自然编码相对位置</li>
                                    <li>🎯 <strong>外推能力强:</strong> 训练时看到的相对位置模式可以外推到更长序列</li>
                                </ul>
                                
                                <p><strong>为什么用旋转矩阵？</strong></p>
                                <ul>
                                    <li>🧮 <strong>数学基础:</strong> 旋转矩阵保持向量长度，只改变方向</li>
                                    <li>🔗 <strong>相对位置:</strong> 两个向量夹角反映相对位置关系</li>
                                    <li>⚡ <strong>计算高效:</strong> 可以预计算cos/sin值，运行时直接应用</li>
                                    <li>🎨 <strong>优雅实现:</strong> 无需额外参数，纯数学变换</li>
                                </ul>
                                
                                <div class="improvement-indicator positive">
                                    ✅ 完美的相对位置建模 + 强大的外推能力 + 无额外参数
                                </div>
                                
                                <p><strong>通俗理解:</strong> RoPE就像给每个词加上"指南针"，词与词的相对方向表示它们的位置关系，这种关系在长文本中依然有效！</p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="toggle-buttons">
                    <button class="toggle-btn active" onclick="toggleContent('rope-viz', this)">对比可视化</button>
                    <button class="toggle-btn" onclick="toggleContent('rope-code', this)">代码对比</button>
                </div>
                
                <div class="toggle-content active" id="rope-viz">
                    <div class="controls">
                        <div class="control-group">
                            <label>位置编码类型:</label>
                            <select id="pos-encoding-type">
                                <option value="sinusoidal">Sinusoidal</option>
                                <option value="rope">RoPE</option>
                                <option value="both">对比显示</option>
                            </select>
                        </div>
                        <div class="control-group">
                            <label>序列长度: <span id="rope-seq-len-value">128</span></label>
                            <input type="range" id="rope-seq-len" min="32" max="512" value="128">
                        </div>
                    </div>
                    <div class="plot-container" id="rope-plot"></div>
                </div>
                
                <div class="toggle-content" id="rope-code">
                    <div class="comparison-container">
                        <div class="comparison-item before">
                            <div class="comparison-title">Sinusoidal实现</div>
                            <div class="code-section">
def sinusoidal_pos_encoding(seq_len, d_model):
    pe = torch.zeros(seq_len, d_model)
    position = torch.arange(0, seq_len).unsqueeze(1).float()
    
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                        -(math.log(10000.0) / d_model))
    
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    return pe
                            </div>
                        </div>
                        
                        <div class="comparison-item after">
                            <div class="comparison-title">RoPE实现</div>
                            <div class="code-section">
def apply_rope(q, k, seq_len):
    # 计算旋转频率
    dim = q.size(-1)
    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))
    
    # 生成位置
    t = torch.arange(seq_len).float()
    freqs = torch.outer(t, inv_freq)
    
    # 生成旋转矩阵
    cos = freqs.cos()
    sin = freqs.sin()
    
    # 应用旋转
    q_rot = rotate_half(q) * sin + q * cos
    k_rot = rotate_half(k) * sin + k * cos
    
    return q_rot, k_rot
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="demo-card">
                <h3>📏 长序列外推能力</h3>
                <div class="controls">
                    <div class="control-group">
                        <label>训练长度: <span id="train-len-value">512</span></label>
                        <input type="range" id="train-len" min="256" max="1024" step="128" value="512">
                    </div>
                    <div class="control-group">
                        <label>测试长度: <span id="test-len-value">1024</span></label>
                        <input type="range" id="test-len" min="512" max="4096" step="256" value="1024">
                    </div>
                </div>
                <div class="plot-container" id="extrapolation-plot"></div>
            </div>
        </div>
        
        <!-- 2023: 现代LLM -->
        <div class="demo-section hidden" id="demo-2023">
            <div class="demo-card">
                <h3>🏛️ 现代LLM最终形态</h3>
                
                <div class="mermaid-container">
                    <div class="mermaid">
                        flowchart TD
                            A["输入序列"] --> B["词嵌入 + RoPE"]
                            B --> C["RMSNorm"]
                            C --> D["多头注意力 + RoPE"]
                            D --> E["残差连接"]
                            E --> F["RMSNorm"]
                            F --> G["SwiGLU FFN"]
                            G --> H["残差连接"]
                            H --> I["输出"]
                            
                            style B fill:#99ff99
                            style C fill:#99ccff
                            style D fill:#ff9999
                            style F fill:#99ccff
                            style G fill:#ffcc99
                    </div>
                </div>
                
                <div class="formula-section">
                    <div class="formula-title">现代LLM的完整架构：</div>
                    <div id="modern-llm-formula">
                        $$x_{l+1} = x_l + \text{SwiGLU}(\text{RMSNorm}(x_l + \text{RoPE-Attention}(\text{RMSNorm}(x_l))))$$
                    </div>
                    <div class="explanation-box">
                        <h4>🔍 现代架构完整分析</h4>
                        <p><strong>整体时间复杂度:</strong> $O(\text{seq\_len}^2 \times d_{\text{model}} + \text{seq\_len} \times d_{\text{model}}^2)$ <span class="complexity-badge">注意力+FFN</span></p>
                        <p><strong>架构集成优势:</strong></p>
                        <ul>
                            <li>🏗️ <strong>Pre-Norm:</strong> 梯度流更稳定，可训练更深网络</li>
                            <li>⚡ <strong>RMSNorm:</strong> 计算效率提升20%，数值更稳定</li>
                            <li>🌀 <strong>RoPE:</strong> 相对位置编码，长序列外推能力强8倍</li>
                            <li>🚪 <strong>SwiGLU:</strong> 门控激活，表达能力和梯度流双提升</li>
                        </ul>
                        <div class="improvement-indicator positive">
                            🎯 完美融合：每个组件解决特定问题，组合效果超过单独改进的总和
                        </div>
                    </div>
                </div>

                <div class="comparison-container">
                    <div class="comparison-item before">
                        <div class="comparison-title">Post-Norm (原始)</div>
                        <div class="formula-section">
                            <div id="postnorm-arch-formula">
                                $$\text{Output} = \text{Norm}(x + \text{Sublayer}(x))$$
                            </div>
                            <div class="explanation-box">
                                <h4>🔍 Post-Norm问题分析</h4>
                                <p><strong>梯度传播路径:</strong> 梯度需要先经过Norm层再到残差连接</p>
                                <ul>
                                    <li>📉 <strong>梯度不稳定:</strong> 深层网络梯度容易爆炸或消失</li>
                                    <li>🐌 <strong>训练初期慢:</strong> 需要学习率预热，否则不收敛</li>
                                    <li>⚠️ <strong>深度限制:</strong> 很难训练超过12层的网络</li>
                                    <li>🎚️ <strong>超参敏感:</strong> 学习率调节困难</li>
                                </ul>
                                <div class="improvement-indicator negative">
                                    ❌ 训练不稳定 + 深度受限 + 超参调节困难
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="comparison-item after">
                        <div class="comparison-title">Pre-Norm (现代)</div>
                        <div class="formula-section">
                            <div id="prenorm-arch-formula">
                                $$\text{Output} = x + \text{Sublayer}(\text{Norm}(x))$$
                            </div>
                            <div class="explanation-box">
                                <h4>🔍 Pre-Norm优势解析</h4>
                                <p><strong>梯度传播路径:</strong> 梯度直接通过残差连接传播，更加顺畅</p>
                                <ul>
                                    <li>🎯 <strong>梯度稳定:</strong> 残差连接提供直接的梯度通道</li>
                                    <li>🚀 <strong>训练快速:</strong> 无需学习率预热，直接高效训练</li>
                                    <li>🏗️ <strong>深度无限:</strong> 可以轻松训练100+层网络</li>
                                    <li>⚙️ <strong>超参鲁棒:</strong> 对学习率等超参数不敏感</li>
                                    <li>📈 <strong>收敛更快:</strong> 通常能提前达到更好效果</li>
                                </ul>
                                <div class="improvement-indicator positive">
                                    ✅ 训练超稳定 + 深度无限制 + 超参数鲁棒 + 收敛更快
                                </div>
                                
                                <p><strong>为什么Pre-Norm更好？</strong></p>
                                <p>关键在于梯度流：Pre-Norm让梯度有一条"高速公路"（残差连接）直达每一层，而Post-Norm的梯度必须"绕路"经过归一化层，容易在路上"迷失"。</p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="toggle-buttons">
                    <button class="toggle-btn active" onclick="toggleContent('arch-viz', this)">训练稳定性</button>
                    <button class="toggle-btn" onclick="toggleContent('arch-code', this)">架构代码</button>
                </div>
                
                <div class="toggle-content active" id="arch-viz">
                    <div class="controls">
                        <div class="control-group">
                            <label>架构类型:</label>
                            <select id="norm-position">
                                <option value="post">Post-Norm (原始)</option>
                                <option value="pre">Pre-Norm (现代)</option>
                                <option value="both">对比显示</option>
                            </select>
                        </div>
                        <div class="control-group">
                            <label>网络层数: <span id="num-layers-value">12</span></label>
                            <input type="range" id="num-layers" min="6" max="24" value="12">
                        </div>
                    </div>
                    <div class="plot-container" id="norm-position-plot"></div>
                </div>
                
                <div class="toggle-content" id="arch-code">
                    <div class="comparison-container">
                        <div class="comparison-item before">
                            <div class="comparison-title">Post-Norm块</div>
                            <div class="code-section">
class PostNormBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.attention = MultiHeadAttention(dim)
        self.ffn = FeedForward(dim)
        self.norm1 = LayerNorm(dim)
        self.norm2 = LayerNorm(dim)
    
    def forward(self, x):
        # Post-Norm: 先操作再归一化
        x = self.norm1(x + self.attention(x))
        x = self.norm2(x + self.ffn(x))
        return x
                            </div>
                        </div>
                        
                        <div class="comparison-item after">
                            <div class="comparison-title">Pre-Norm块</div>
                            <div class="code-section">
class PreNormBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.attention = MultiHeadAttention(dim)
        self.ffn = SwiGLU(dim)
        self.norm1 = RMSNorm(dim)
        self.norm2 = RMSNorm(dim)
    
    def forward(self, x):
        # Pre-Norm: 先归一化再操作
        x = x + self.attention(self.norm1(x))
        x = x + self.ffn(self.norm2(x))
        return x
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="demo-card">
                <h3>📊 综合性能提升</h3>
                <div class="comparison-grid">
                    <div class="metric-card">
                        <div class="metric-value">40%</div>
                        <div class="metric-label">训练速度提升</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">25%</div>
                        <div class="metric-label">计算效率提升</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">8x</div>
                        <div class="metric-label">长序列处理能力</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">15%</div>
                        <div class="metric-label">模型性能提升</div>
                    </div>
                </div>
                
                <div class="explanation-box">
                    <h4>🎯 技术协同效应分析</h4>
                    <p><strong>1 + 1 + 1 + 1 > 4 的协同效应：</strong></p>
                    <ul>
                        <li>🏗️ <strong>Pre-Norm + RMSNorm:</strong> 训练稳定性×计算效率 = 40%速度提升</li>
                        <li>🌀 <strong>RoPE + SwiGLU:</strong> 位置建模×表达能力 = 长序列能力8倍提升</li>
                        <li>⚡ <strong>所有技术组合:</strong> 内存使用↓15%，推理速度↑25%，效果↑15%</li>
                    </ul>
                    
                    <p><strong>为什么现代架构如此成功？</strong></p>
                    <div class="improvement-indicator positive">
                        🎯 每个技术都解决了原始Transformer的核心痛点，组合使用产生质的飞跃
                    </div>
                    
                    <p><strong>实际应用影响:</strong></p>
                    <ul>
                        <li>📚 <strong>GPT系列:</strong> 使用这些技术训练出ChatGPT等强大模型</li>
                        <li>🦙 <strong>LLaMA系列:</strong> 采用RMSNorm+RoPE+SwiGLU的标准现代架构</li>
                        <li>🌍 <strong>开源模型:</strong> Qwen、Baichuan等都采用这套"黄金组合"</li>
                    </ul>
                </div>
                
                <div class="layer-block">
                    <strong>Pre-Norm + RMSNorm</strong><br>
                    更稳定的训练，更快的收敛
                    <div class="improvement-badge">✅</div>
                </div>
                <div class="layer-block">
                    <strong>RoPE位置编码</strong><br>
                    相对位置编码，优秀外推能力
                    <div class="improvement-badge">✅</div>
                </div>
                <div class="layer-block">
                    <strong>SwiGLU激活</strong><br>
                    门控机制，更强表达能力
                    <div class="improvement-badge">✅</div>
                </div>
                
                <div class="explanation-box">
                    <h4>🚀 未来展望</h4>
                    <p>这套现代架构已成为大语言模型的"标准配置"，未来的改进可能集中在：</p>
                    <ul>
                        <li>🔧 <strong>注意力机制优化:</strong> 从 $O(n^2)$ 到 $O(n)$ 复杂度</li>
                        <li>💾 <strong>内存效率提升:</strong> 更高效的激活重计算</li>
                        <li>🎯 <strong>专门化设计:</strong> 针对特定任务的架构优化</li>
                        <li>🧠 <strong>混合专家系统:</strong> 更大规模的稀疏模型</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    
    <script>
        // 全局变量
        let currentYear = "2017";
        
        // 初始化Mermaid (更新到10.6.0的API)
        mermaid.initialize({
            startOnLoad: false,
            theme: 'default',
            themeVariables: {
                primaryColor: '#667eea',
                primaryTextColor: '#333',
                primaryBorderColor: '#764ba2',
                lineColor: '#666',
                secondaryColor: '#e2e8f0',
                tertiaryColor: '#f7fafc'
            },
            flowchart: {
                useMaxWidth: true,
                htmlLabels: true
            },
            graph: {
                useMaxWidth: true,
                htmlLabels: true
            }
        });
        
        // ====== MathJax 渲染函数 ======
        
        function renderMathJaxForElement(element) {
            return new Promise((resolve, reject) => {
                if (typeof MathJax !== 'undefined' && MathJax.typesetPromise) {
                    MathJax.typesetPromise([element]).then(() => {
                        // 修复行内公式样式
                        element.querySelectorAll('.MathJax').forEach(mathElement => {
                            const parent = mathElement.parentElement;
                            if (parent) {
                                const text = parent.textContent || parent.innerText || '';
                                // 检查是否为行内公式（不是$$包围，但包含$）
                                if (!text.match(/\$\$[\s\S]*?\$\$/g) && text.includes('$') && !mathElement.parentElement.closest('.MathJax_Display')) {
                                    mathElement.style.display = 'inline';
                                    mathElement.style.margin = '0 2px';
                                    mathElement.classList.add('inline-math');
                                }
                            }
                        });
                        resolve();
                    }).catch(reject);
                } else {
                    console.warn('MathJax未完全加载');
                    resolve();
                }
            });
        }
        
        function renderVisibleMathJax() {
            const visibleSections = document.querySelectorAll('.demo-section:not(.hidden)');
            const promises = Array.from(visibleSections).map(section => renderMathJaxForElement(section));
            return Promise.all(promises);
        }
        
        function forceRenderAllMathJax() {
            return new Promise((resolve, reject) => {
                if (typeof MathJax !== 'undefined' && MathJax.typesetPromise) {
                    // 清除已处理标记
                    document.querySelectorAll('[data-mathml], [data-mathprocessed]').forEach(el => {
                        el.removeAttribute('data-mathml');
                        el.removeAttribute('data-mathprocessed');
                    });
                    
                    MathJax.typesetPromise().then(() => {
                        // 修复所有行内公式样式
                        document.querySelectorAll('.MathJax').forEach(mathElement => {
                            const parent = mathElement.parentElement;
                            if (parent) {
                                const text = parent.textContent || parent.innerText || '';
                                if (!text.match(/\$\$[\s\S]*?\$\$/g) && text.includes('$') && !mathElement.parentElement.closest('.MathJax_Display')) {
                                    mathElement.style.display = 'inline';
                                    mathElement.style.margin = '0 2px';
                                    mathElement.classList.add('inline-math');
                                }
                            }
                        });
                        resolve();
                    }).catch(reject);
                } else {
                    console.warn('MathJax未加载，跳过渲染');
                    resolve();
                }
            });
        }
        
        // ====== Mermaid 渲染函数 ======
        
        async function renderMermaid() {
            const mermaidElements = document.querySelectorAll('.mermaid');
            
            for (let i = 0; i < mermaidElements.length; i++) {
                const element = mermaidElements[i];
                if (!element.dataset.processed) {
                    try {
                        const graphDefinition = element.textContent.trim();
                        const { svg } = await mermaid.render(`mermaid-${i}-${Date.now()}`, graphDefinition);
                        element.innerHTML = svg;
                        element.dataset.processed = 'true';
                    } catch (error) {
                        console.warn('Mermaid渲染失败:', error);
                    }
                }
            }
        }
        
        // ====== 切换内容函数 ======
        
        function toggleContent(contentId, button) {
            const parent = button.parentElement.parentElement;
            const contents = parent.querySelectorAll('.toggle-content');
            const buttons = parent.querySelectorAll('.toggle-btn');
            
            contents.forEach(content => content.classList.remove('active'));
            buttons.forEach(btn => btn.classList.remove('active'));
            
            document.getElementById(contentId).classList.add('active');
            button.classList.add('active');
        }
        
        // ====== 时间线切换 ======
        
        document.querySelectorAll('.timeline-item').forEach(item => {
            item.addEventListener('click', () => {
                document.querySelectorAll('.timeline-item').forEach(i => i.classList.remove('active'));
                item.classList.add('active');
                
                document.querySelectorAll('.demo-section').forEach(section => {
                    section.classList.add('hidden');
                });
                
                currentYear = item.dataset.year;
                document.getElementById(`demo-${currentYear}`).classList.remove('hidden');
                
                setTimeout(() => {
                    renderMermaid();
                    renderVisibleMathJax().then(() => {
                        initVisualization(currentYear);
                    }).catch(err => {
                        console.warn('MathJax渲染失败:', err);
                        initVisualization(currentYear);
                    });
                }, 200);
            });
        });
        
        // ====== 可视化初始化 ======
        
        function initVisualization(year) {
            switch(year) {
                case "2017":
                    initOriginalTransformer();
                    break;
                case "2019":
                    initRMSNorm();
                    break;
                case "2020":
                    initSwiGLU();
                    break;
                case "2021":
                    initRoPE();
                    break;
                case "2023":
                    initModernLLM();
                    break;
            }
        }
        
        // 2017: 原始Transformer
        function initOriginalTransformer() {
            function updatePositionalEncoding() {
                const seqLen = parseInt(document.getElementById('seq-len').value);
                const dModel = parseInt(document.getElementById('d-model').value);
                
                document.getElementById('seq-len-value').textContent = seqLen;
                document.getElementById('d-model-value').textContent = dModel;
                
                const positions = Array.from({length: seqLen}, (_, i) => i);
                const dimensions = Array.from({length: Math.min(8, dModel)}, (_, i) => i);
                
                const data = dimensions.map(dim => ({
                    x: positions,
                    y: positions.map(pos => {
                        if (dim % 2 === 0) {
                            return Math.sin(pos / Math.pow(10000, 2 * dim / dModel));
                        } else {
                            return Math.cos(pos / Math.pow(10000, 2 * (dim-1) / dModel));
                        }
                    }),
                    name: `维度 ${dim}`,
                    type: 'scatter',
                    mode: 'lines'
                }));
                
                Plotly.newPlot('pos-encoding-plot', data, {
                    title: '正弦位置编码模式',
                    xaxis: { title: '位置' },
                    yaxis: { title: '编码值' },
                    margin: { t: 40, r: 20, b: 40, l: 40 }
                });
            }
            
            function updateActivation() {
                const activationType = document.getElementById('activation-type').value;
                const x = Array.from({length: 200}, (_, i) => (i - 100) / 20);
                
                let y, name, color;
                switch(activationType) {
                    case 'relu':
                        y = x.map(val => Math.max(0, val));
                        name = 'ReLU';
                        color = '#ff6b6b';
                        break;
                    case 'gelu':
                        y = x.map(val => val * 0.5 * (1 + Math.tanh(Math.sqrt(2/Math.PI) * (val + 0.044715 * Math.pow(val, 3)))));
                        name = 'GELU';
                        color = '#4ecdc4';
                        break;
                    case 'swish':
                        y = x.map(val => val / (1 + Math.exp(-val)));
                        name = 'Swish';
                        color = '#45b7d1';
                        break;
                    case 'swiglu':
                        y = x.map(val => (val / (1 + Math.exp(-val))) * val);
                        name = 'SwiGLU (简化)';
                        color = '#96ceb4';
                        break;
                }
                
                const data = [{
                    x: x,
                    y: y,
                    type: 'scatter',
                    mode: 'lines',
                    name: name,
                    line: { color: color, width: 3 }
                }];
                
                Plotly.newPlot('activation-plot', data, {
                    title: `${name} 激活函数`,
                    xaxis: { title: '输入值' },
                    yaxis: { title: '输出值' },
                    margin: { t: 40, r: 20, b: 40, l: 40 }
                });
            }
            
            document.getElementById('seq-len').addEventListener('input', updatePositionalEncoding);
            document.getElementById('d-model').addEventListener('input', updatePositionalEncoding);
            document.getElementById('activation-type').addEventListener('change', updateActivation);
            
            updatePositionalEncoding();
            updateActivation();
        }
        
        // 2019: RMSNorm
        function initRMSNorm() {
            function updateNormComparison() {
                const normType = document.getElementById('norm-type').value;
                const noiseLevel = parseFloat(document.getElementById('noise-level').value);
                
                document.getElementById('noise-level-value').textContent = noiseLevel;
                
                const x = Array.from({length: 100}, (_, i) => i);
                const cleanSignal = x.map(i => Math.sin(i * 0.1));
                const noisySignal = cleanSignal.map(val => val + (Math.random() - 0.5) * noiseLevel);
                
                let data = [];
                
                if (normType === 'both') {
                    const mean = noisySignal.reduce((a, b) => a + b) / noisySignal.length;
                    const variance = noisySignal.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / noisySignal.length;
                    const std = Math.sqrt(variance);
                    const layernormSignal = noisySignal.map(val => (val - mean) / (std + 1e-6));
                    
                    const rms = Math.sqrt(noisySignal.reduce((sum, val) => sum + val * val, 0) / noisySignal.length);
                    const rmsnormSignal = noisySignal.map(val => val / (rms + 1e-6));
                    
                    data = [
                        {
                            x: x,
                            y: noisySignal,
                            name: '原始信号',
                            type: 'scatter',
                            mode: 'lines',
                            line: { color: '#666', width: 2 }
                        },
                        {
                            x: x,
                            y: layernormSignal,
                            name: 'LayerNorm',
                            type: 'scatter',
                            mode: 'lines',
                            line: { color: '#ff6b6b', width: 2 }
                        },
                        {
                            x: x,
                            y: rmsnormSignal,
                            name: 'RMSNorm',
                            type: 'scatter',
                            mode: 'lines',
                            line: { color: '#4ecdc4', width: 2 }
                        }
                    ];
                } else {
                    let normalizedSignal;
                    let color, normName;
                    
                    if (normType === 'layernorm') {
                        const mean = noisySignal.reduce((a, b) => a + b) / noisySignal.length;
                        const variance = noisySignal.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / noisySignal.length;
                        const std = Math.sqrt(variance);
                        normalizedSignal = noisySignal.map(val => (val - mean) / (std + 1e-6));
                        color = '#ff6b6b';
                        normName = 'LayerNorm';
                    } else {
                        const rms = Math.sqrt(noisySignal.reduce((sum, val) => sum + val * val, 0) / noisySignal.length);
                        normalizedSignal = noisySignal.map(val => val / (rms + 1e-6));
                        color = '#4ecdc4';
                        normName = 'RMSNorm';
                    }
                    
                    data = [
                        {
                            x: x,
                            y: noisySignal,
                            name: '原始信号',
                            type: 'scatter',
                            mode: 'lines',
                            line: { color: '#666', width: 2 }
                        },
                        {
                            x: x,
                            y: normalizedSignal,
                            name: `${normName} 处理后`,
                            type: 'scatter',
                            mode: 'lines',
                            line: { color: color, width: 2 }
                        }
                    ];
                }
                
                Plotly.newPlot('norm-comparison-plot', data, {
                    title: '归一化效果对比',
                    xaxis: { title: '时间步' },
                    yaxis: { title: '数值' },
                    margin: { t: 40, r: 20, b: 40, l: 40 }
                });
            }
            
            document.getElementById('norm-type').addEventListener('change', updateNormComparison);
            document.getElementById('noise-level').addEventListener('input', updateNormComparison);
            
            updateNormComparison();
        }
        
        // 2020: SwiGLU
        function initSwiGLU() {
            function updateActivationEvolution() {
                const comparison = document.getElementById('activation-comparison').value;
                const x = Array.from({length: 200}, (_, i) => (i - 100) / 20);
                
                const activations = {
                    relu: { 
                        y: x.map(val => Math.max(0, val)),
                        color: '#ff6b6b',
                        name: 'ReLU'
                    },
                    gelu: {
                        y: x.map(val => {
                            const tanh_part = Math.tanh(Math.sqrt(2/Math.PI) * (val + 0.044715 * Math.pow(val, 3)));
                            return 0.5 * val * (1 + tanh_part);
                        }),
                        color: '#4ecdc4',
                        name: 'GELU'
                    },
                    swish: {
                        y: x.map(val => val / (1 + Math.exp(-val))),
                        color: '#45b7d1',
                        name: 'Swish'
                    },
                    swiglu: {
                        y: x.map(val => {
                            const gate = val / (1 + Math.exp(-val));
                            const value = val;
                            return gate * value * 0.7;
                        }),
                        color: '#96ceb4',
                        name: 'SwiGLU'
                    }
                };
                
                let data = [];
                if (comparison === 'all') {
                    data = Object.values(activations).map(act => ({
                        x: x,
                        y: act.y,
                        type: 'scatter',
                        mode: 'lines',
                        name: act.name,
                        line: { color: act.color, width: 3 }
                    }));
                } else {
                    const [first, second] = comparison.split('-');
                    data = [activations[first], activations[second]].map(act => ({
                        x: x,
                        y: act.y,
                        type: 'scatter',
                        mode: 'lines',
                        name: act.name,
                        line: { color: act.color, width: 3 }
                    }));
                }
                
                Plotly.newPlot('activation-evolution-plot', data, {
                    title: '激活函数演进对比',
                    xaxis: { title: '输入值' },
                    yaxis: { title: '输出值' },
                    margin: { t: 40, r: 20, b: 40, l: 40 },
                    annotations: [{
                        x: 0,
                        y: 0.5,
                        xref: 'x',
                        yref: 'paper',
                        text: '注意各函数在负值区域的不同行为',
                        showarrow: false,
                        font: { size: 10, color: '#666' }
                    }]
                });
            }
            
            function updateGradientFlow() {
                const depth = parseInt(document.getElementById('depth').value);
                document.getElementById('depth-value').textContent = depth;
                
                const layers = Array.from({length: depth}, (_, i) => i + 1);
                
                const gradientFlows = {
                    relu: layers.map(layer => Math.pow(0.8, layer - 1)),
                    gelu: layers.map(layer => Math.pow(0.87, layer - 1)),
                    swish: layers.map(layer => Math.pow(0.92, layer - 1)),
                    swiglu: layers.map(layer => Math.pow(0.96, layer - 1))
                };
                
                const data = Object.entries(gradientFlows).map(([name, flow]) => ({
                    x: layers,
                    y: flow,
                    type: 'scatter',
                    mode: 'lines+markers',
                    name: name.toUpperCase(),
                    line: { width: 3 }
                }));
                
                Plotly.newPlot('gradient-flow-plot', data, {
                    title: '梯度流衰减对比 - SwiGLU显著改善深层网络训练',
                    xaxis: { title: '网络层数' },
                    yaxis: { title: '梯度强度', type: 'log' },
                    margin: { t: 40, r: 20, b: 40, l: 40 },
                    annotations: [{
                        x: depth * 0.7,
                        y: Math.pow(0.96, depth * 0.7),
                        text: 'SwiGLU门控机制<br>保持梯度流动',
                        showarrow: true,
                        arrowhead: 2,
                        ax: 20,
                        ay: -40,
                        font: { size: 10 }
                    }]
                });
            }
            
            document.getElementById('activation-comparison').addEventListener('change', updateActivationEvolution);
            document.getElementById('depth').addEventListener('input', updateGradientFlow);
            
            updateActivationEvolution();
            updateGradientFlow();
        }
        
        // 2021: RoPE
        function initRoPE() {
            function updateRoPE() {
                const posEncodingType = document.getElementById('pos-encoding-type').value;
                const seqLen = parseInt(document.getElementById('rope-seq-len').value);
                
                document.getElementById('rope-seq-len-value').textContent = seqLen;
                
                const positions = Array.from({length: seqLen}, (_, i) => i);
                
                if (posEncodingType === 'both') {
                    const sinCorrelation = positions.map(i => 
                        positions.map(j => Math.cos((i - j) * Math.PI / 64))
                    );
                    
                    const ropeCorrelation = positions.map(i => 
                        positions.map(j => Math.cos(Math.abs(i - j) * Math.PI / 128) * Math.exp(-Math.abs(i - j) / 100))
                    );
                    
                    const data = [
                        {
                            z: sinCorrelation,
                            x: positions,
                            y: positions,
                            type: 'heatmap',
                            colorscale: 'Viridis',
                            name: 'Sinusoidal',
                            visible: true,
                            showscale: false
                        },
                        {
                            z: ropeCorrelation,
                            x: positions,
                            y: positions,
                            type: 'heatmap',
                            colorscale: 'Plasma',
                            name: 'RoPE',
                            visible: false,
                            showscale: false
                        }
                    ];
                    
                    const layout = {
                        title: 'Sinusoidal vs RoPE 位置相关性对比',
                        xaxis: { title: '位置i' },
                        yaxis: { title: '位置j' },
                        margin: { t: 60, r: 60, b: 40, l: 40 },
                        updatemenus: [{
                            type: 'buttons',
                            direction: 'left',
                            buttons: [{
                                args: [{'visible': [true, false]}],
                                label: 'Sinusoidal',
                                method: 'restyle'
                            }, {
                                args: [{'visible': [false, true]}],
                                label: 'RoPE',
                                method: 'restyle'
                            }],
                            x: 0.1,
                            y: 1.02,
                            xanchor: 'left',
                            yanchor: 'top'
                        }]
                    };
                    
                    Plotly.newPlot('rope-plot', data, layout);
                } else {
                    let correlation, title, colorscale;
                    
                    if (posEncodingType === 'sinusoidal') {
                        correlation = positions.map(i => 
                            positions.map(j => Math.cos((i - j) * Math.PI / 64))
                        );
                        title = 'Sinusoidal 位置相关性矩阵';
                        colorscale = 'Viridis';
                    } else {
                        correlation = positions.map(i => 
                            positions.map(j => Math.cos(Math.abs(i - j) * Math.PI / 128) * Math.exp(-Math.abs(i - j) / 100))
                        );
                        title = 'RoPE 位置相关性矩阵';
                        colorscale = 'Plasma';
                    }
                    
                    const data = [{
                        z: correlation,
                        x: positions,
                        y: positions,
                        type: 'heatmap',
                        colorscale: colorscale
                    }];
                    
                    Plotly.newPlot('rope-plot', data, {
                        title: title,
                        xaxis: { title: '位置i' },
                        yaxis: { title: '位置j' },
                        margin: { t: 40, r: 60, b: 40, l: 40 }
                    });
                }
            }
            
            function updateExtrapolation() {
                const trainLen = parseInt(document.getElementById('train-len').value);
                const testLen = parseInt(document.getElementById('test-len').value);
                
                document.getElementById('train-len-value').textContent = trainLen;
                document.getElementById('test-len-value').textContent = testLen;
                
                const positions = Array.from({length: testLen}, (_, i) => i);
                
                const sinusoidalPerf = positions.map(pos => {
                    if (pos <= trainLen) return 1.0;
                    return Math.max(0.3, 1.0 - (pos - trainLen) / trainLen * 0.8);
                });
                
                const ropePerf = positions.map(pos => {
                    if (pos <= trainLen) return 1.0;
                    return Math.max(0.7, 1.0 - (pos - trainLen) / trainLen * 0.3);
                });
                
                const data = [
                    {
                        x: positions,
                        y: sinusoidalPerf,
                        name: 'Sinusoidal 位置编码',
                        type: 'scatter',
                        mode: 'lines',
                        line: { color: '#ff6b6b', width: 3 }
                    },
                    {
                        x: positions,
                        y: ropePerf,
                        name: 'RoPE',
                        type: 'scatter',
                        mode: 'lines',
                        line: { color: '#4ecdc4', width: 3 }
                    },
                    {
                        x: [trainLen, trainLen],
                        y: [0, 1],
                        name: '训练长度边界',
                        type: 'scatter',
                        mode: 'lines',
                        line: { color: '#666', dash: 'dash' }
                    }
                ];
                
                Plotly.newPlot('extrapolation-plot', data, {
                    title: '长序列外推能力对比',
                    xaxis: { title: '序列长度' },
                    yaxis: { title: '性能保持率' },
                    margin: { t: 40, r: 20, b: 40, l: 40 }
                });
            }
            
            document.getElementById('pos-encoding-type').addEventListener('change', updateRoPE);
            document.getElementById('rope-seq-len').addEventListener('input', updateRoPE);
            document.getElementById('train-len').addEventListener('input', updateExtrapolation);
            document.getElementById('test-len').addEventListener('input', updateExtrapolation);
            
            updateRoPE();
            updateExtrapolation();
        }
        
        // 2023: 现代LLM
        function initModernLLM() {
            function updateNormPosition() {
                const normPosition = document.getElementById('norm-position').value;
                const numLayers = parseInt(document.getElementById('num-layers').value);
                
                document.getElementById('num-layers-value').textContent = numLayers;
                
                const epochs = Array.from({length: 100}, (_, i) => i + 1);
                
                const layerFactor = Math.log(numLayers) / Math.log(12);
                
                const postNormLoss = epochs.map(epoch => {
                    const base = 2.0 * Math.exp(-epoch / (30 + layerFactor * 10));
                    const noise = Math.sin(epoch * 0.3) * 0.2 * Math.exp(-epoch / 20) * (1 + layerFactor * 0.5);
                    return Math.max(0.1, base + noise);
                });
                
                const preNormLoss = epochs.map(epoch => {
                    const base = 2.0 * Math.exp(-epoch / (20 - layerFactor * 2));
                    const noise = Math.sin(epoch * 0.1) * 0.1 * Math.exp(-epoch / 30) * (1 + layerFactor * 0.2);
                    return Math.max(0.05, base + noise);
                });
                
                let data = [];
                
                if (normPosition === 'both') {
                    data = [
                        {
                            x: epochs,
                            y: postNormLoss,
                            name: 'Post-Norm',
                            type: 'scatter',
                            mode: 'lines',
                            line: { color: '#ff6b6b', width: 3 }
                        },
                        {
                            x: epochs,
                            y: preNormLoss,
                            name: 'Pre-Norm',
                            type: 'scatter',
                            mode: 'lines',
                            line: { color: '#4ecdc4', width: 3 }
                        }
                    ];
                } else {
                    const isPostNorm = normPosition === 'post';
                    data = [{
                        x: epochs,
                        y: isPostNorm ? postNormLoss : preNormLoss,
                        name: isPostNorm ? 'Post-Norm' : 'Pre-Norm',
                        type: 'scatter',
                        mode: 'lines',
                        line: { 
                            color: isPostNorm ? '#ff6b6b' : '#4ecdc4',
                            width: 3 
                        }
                    }];
                }
                
                Plotly.newPlot('norm-position-plot', data, {
                    title: `训练稳定性对比 (${numLayers}层网络)`,
                    xaxis: { title: '训练轮次' },
                    yaxis: { title: '损失值', type: 'log' },
                    margin: { t: 40, r: 20, b: 40, l: 40 }
                });
            }
            
            document.getElementById('norm-position').addEventListener('change', updateNormPosition);
            document.getElementById('num-layers').addEventListener('input', updateNormPosition);
            
            updateNormPosition();
        }
        
        // 页面加载时初始化
        function initializePage() {
            setTimeout(() => {
                renderMermaid();
                forceRenderAllMathJax().then(() => {
                    console.log('MathJax渲染完成');
                    initVisualization(currentYear);
                }).catch(err => {
                    console.warn('MathJax渲染失败:', err);
                    initVisualization(currentYear);
                });
            }, 500);
        }
        
        if (document.readyState === 'loading') {
            document.addEventListener('DOMContentLoaded', initializePage);
        } else {
            initializePage();
        }
        
        window.addEventListener('load', () => {
            setTimeout(() => {
                if (typeof MathJax !== 'undefined') {
                    forceRenderAllMathJax().catch(console.warn);
                }
            }, 1000);
        });
    </script>
</body>
</html>
